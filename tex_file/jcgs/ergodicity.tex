%\vspace{-.1in}
\section{Geometric ergodicity}
%\vspace{-.05in}
Finally, we derive conditions under which our symmetrized 
MH algorithm
inherits mixing properties of an `ideal' sampler that operates without
computational constraints. The latter proposes a new parameter $\vartheta$
from distribution $q(\vartheta|\theta)$, and accepts with probability
$\alpha_I(\theta,\vartheta; X) = 1 \wedge \frac{P(X , \vartheta)q(\theta| \vartheta)}
{P(X , \theta)q(\vartheta|\theta)}$.  The resulting ideal (but intractable)
Markov chain has transition probability
$P_I(\theta'|\theta) = q(\theta'|\theta)\alpha_I(\theta,\theta';X) + \left[1-\int d\vartheta
q(\vartheta|\theta)\alpha_I(\theta,\vartheta;X)\right]\delta_\theta(\theta')$, the first
term corresponding to acceptance, and the second, rejection.

Our main result is Theorem~\ref{thm:geom_erg}, which shows that if the ideal MCMC
sampler is geometrically ergodic, then so is our tractable auxiliary
variable sampler (Algorithm~\ref{alg:MH_improved}). 
%Assumption~\ref{asmp:cond_num} is the only non-trivial
%one we have to make, requiring that the rate-matrix $A(\theta)$ not be
%arbitrarily ill-conditioned over all
%settings of the parameter $\theta$. This assumption is reasonable from
%both theoretical and practical viewpoints.
%%\boqian{Write $X$ as the observations, which is fixed.}
We first state all our definitions and assumptions, before diving into the 
proofs.
%For clarity, we focus on the case where the uniformization rate 
%is set as 
%$\Omega(\theta, \vartheta) \doteq \Omega(\theta) + \Omega(\vartheta)$,
%where $A$ is the rate matrix of the Markov jump process and $\Omega(\theta) \doteq k \max_s |A_{ss}(\theta)|$,
%for some $k > 1$.
\begin{assumption}
The uniformization rate is set as $\Omega(\theta, \vartheta) = \Omega(\theta) + 
\Omega(\vartheta)$, where %$A$ is the rate matrix of the Markov jump process and 
$\Omega(\theta) = k_1 \max_s A_{s}(\theta) + k_0$, for some 
$k_1 > 1, k_0 > 0$.
\label{asmp:unif_rate}
\end{assumption}
%If $\inf_\theta \Mx{\theta} > 0$ (as is often the case),
% we can just set $\Omega(\theta) = \Mx{\theta} + \mx{\theta}$ so that
% $m_2 = \frac{1}{1+\mu}$.}
\noindent Although it is possible to specify broader conditions under which our 
result holds, for clarity we focus on this case. 
We can drop the $k_0$ if $\inf_\theta \max_s A_s(\theta) > 0$
%This is also how we setup our sampler in our experiments.

\begin{assumption}
 % $\Omega(\theta)$ is not bounded, and 
 There exists a positive constant $\theta_0$ such 
  that for any $\theta_x, \theta_y$ satisfying
  $\| \theta_x \| \ge \| \theta_y \| > \theta_0$, we have $\Omega(\theta_x) \ge \Omega(\theta_y)$.  
%  \begin{align*}
%   \lim_{\|\theta\| \rightarrow +\infty} \Omega(\theta) = + \infty.
%  \end{align*}
  \label{asmp:mono_tail}
\end{assumption}
\noindent %We will see that proving our result when $\Omega(\theta)$ is bounded is 
%easy. %This assumption usually holds under suitable reparametrization,
%and is not critical: we impose 
%When this is not so, 
This assumption avoids book-keeping by making 
$\Omega(\theta)$ increase monotonically with $\theta$.

\begin{definition}
Let $\pi_\theta$ be the stationary distribution of the MJP with rate-matrix 
$A(\theta)$, and define $D_\theta = \text{diag}(\pi_\theta)$. Define 
$\tilde{A}(\theta) = D_\theta^{-1}A(\theta)D_\theta$, and the 
{\em reversibilization} of $A(\theta)$ as $R_A(\theta) = 
(A(\theta)+\tilde{A}(\theta))/2$. 
\label{def:mjp_symm}
\end{definition}
\noindent This definition is from~\cite{fill1991}, who show that the matrix 
$R_A(\theta)$ is reversible with real eigenvalues, the smallest being $0$. 
The larger its second smallest eigenvalue, the faster the MJP converges to its 
stationary distribution $\pi_\theta$.
Note that if the original MJP is reversible, then $R_A(\theta) = A(\theta)$.

%{
%\begin{definition}
%Let $P_{st}(X | \theta)$ be the stationary distribution of the observations 
%$X$, given the parameter $\theta$. Write $V_X$ for the states of the markov chain corresponding to the observations.
%\begin{align*}
%P_{st}(X | \theta) \doteq \sum_{V_X} P(X | V_X, \theta) \pi_\theta(V_X).
%\end{align*}
%\label{def:stationary_pst}
%\end{definition}
%}
%\begin{assumption}
%For the stationary distribution $\pi_\theta$, there exists $\pi_\lb > 0$, such that $\inf_{\theta, s} \pi_{\theta}(s) > \pi_\lb$.
%\label{asmp:stationary_dist_lower_bound}
%\end{assumption}
\begin{assumption}
%  The rate-matrix $A(\theta)$ is ergodic for all $\theta$ and satisfies
 % $\inf_\theta \frac{\mx{\theta}}{\Mx{\theta}} = \mu > 0$.
%  $\forall \theta$ satisfying
%  $|\theta| > |\hat{\theta}|$, we have $\Mx{\theta} \asymp
%  \mx{\theta}$.
  Write $\lambda^{R_A}_2(\theta)$ for the second smallest eigenvalue of
    $R_A(\theta)$. There exist $\mu > 0, \theta_1 > 0$
    such that for all $\theta$ satisfying $ \| \theta \|> \theta_1$, 
    we have $ \lambda^{R_A}_2(\theta) \geq \mu \max_s A_s(\theta)$
    (or equivalently from Assumption~\ref{asmp:unif_rate}, 
    $ \lambda^{R_A}_2(\theta) \geq \mu \Omega(\theta)$),
    and $\min_s \pi_\theta(s) > 0$. 
  \label{asmp:cond_num}
\end{assumption} 
\noindent %This is the strongest assumption needed to prove our result;
%essentially requiring the rate matrix to be well-conditioned for all 
%large $\theta$. 
%Assumption~\ref{asmp:mono_tail} allows $\max |A_{ss}(\theta)|$ 
%(and therefore $\Omega(\theta)$) to grow with $\|\theta\|$. Here, we 
The assumption on $\lambda^{R_A}_2$ is the strongest we need, requiring that 
$\lambda^{R_A}_2(\theta)$ (which sets the MJP mixing rate) grows 
at least as fast as $\max A_s(\theta)$. 
%This controls the relative stability of different MJP states, and 
This is satisfied when, for example, all elements of $A(\theta)$ grow 
with $\theta$ at similar rates, controlling the relative stability of 
the least and most stable states.
%rules 
%out any state $s'$ such that 
%$\frac{|A_{s's'}(\theta)}{\max_s A_{ss}(\theta)}| \rightarrow 0$ as $\theta$ increases.
While not trivial, this is a reasonable assumption: the MCMC chain over MJP paths 
will mix well if we can control the mixing of the MJP itself.
%it holds for the examples we considered. %One open question is whether this assumption is necessary.
%We suspect that assumption~\ref{asmp:ideal_geom}, which requires 
%the ideal sampler to be geometrically ergodic, implies this condition,
%however we do not try to prove it here.
%is not sufficient to ensure that $ $ is a 
%stronger requirement, ensures
%parameters of interest\{?: the most
%unstable state cannot have a leaving rate that is larger than that of
%the most stable state by an arbitrary factor.} This condition controls the
%mixing behavior of the MJP (and the embedded Markov chain) for any
%starting state.
%We note that we only need this
%condition to hold in the tails of $\theta$, i.e.\ we only need
%  $\inf_\theta \frac{\mx{\theta}}{\Mx{\theta}} > 0$ for all $\theta$
%  satisfying $|\theta| > h$ for some $h$. For clarity, we restrict
%  ourselves to $h=0$, extending our proof to the general case is
%  straightforward.
%for any interval $\Delta$, we can always find a $\theta_0$ such that the
%MJP has mixed at the end of the interval. The earlier assumption ensures
%this holds for all $\theta \ge \theta_0$.
To better understand this, recall $B(\theta, \theta') = I+\frac{A(\theta)}{\Omega(\theta, \theta')}$
is the transition matrix of the embedded Markov chain, and note 
it has the same stationary distribution $\pi_\theta$ as $A(\theta)$.
Define the reversibilization $R_B(\theta,\theta')$ from $B(\theta,\theta')$ 
just as we did $R_A(\theta)$ from $A(\theta)$. 
\begin{lemma}
  Let $\|\theta\| > \max(\theta_0, \theta_1)$ and $\theta'$ satisfy 
$\frac{1}{K_0} \le \frac{\Omega(\theta')}{\Omega(\theta)} \le K_0 $ 
with $K_0$ %> 1$, which 
satisfying $(1 + \frac{1}{K_0})k_1 \ge 2$. 
For all such $(\theta,\theta')$, the Markov chain with transition matrix 
$B(\theta,\theta')$ converges geometrically to stationarity at a rate 
uniformly bounded away from $0$.
%The second largest eigenvalue $\lambda^2_B(\theta,\theta')$ of $R_B$ and  
%second smallest eigenvalue $\lambda^2_B(\theta,\theta')$ of $R_B$ satisfy  
%$\lambda^2_B(\theta,\theta') = 1 - \frac{\lambda^2_A(\theta)}{\Omega(\theta, \theta')}$.
  \label{lem:eig_lemma}
\end{lemma}
\begin{proof}
%it is easy to see that both $A(\theta)$ and $B(\theta,\theta')$ have
%the same stationary distribution $\pi_\theta(s)$. For the matrix $B(\theta,\theta')$, 
A little algebra gives $R_B(\theta,\theta') = I + R_A(\theta)/\Omega(\theta,\theta')$. It 
follows that both $R_A$ and $R_B$ share the same eigenvectors, with 
eigenvalues satisfying 
%and that 
%the eigenvalues of $A(\theta)$ and $B(\theta,\theta')$ satisfy
$\lambda_{R_B}(\theta, \theta') = 1 - \frac{\lambda_{R_A}(\theta)}{\Omega(\theta,
\theta')}$. In particular, the second largest eigenvalue 
$\lambda_2^{R_B}(\theta,\theta')$ of $R_B$ and  
second smallest eigenvalue $\lambda^{R_A}_2(\theta,\theta')$ of $R_A$ satisfy  
$\lambda^{R_B}_2(\theta,\theta') = 1 - \frac{\lambda^{R_A}_2(\theta)}{\Omega(\theta, \theta')}$.
Then, from assumptions~\ref{asmp:unif_rate} and~\ref{asmp:cond_num}, and 
the lemma's assumptions, 
$1 - \lambda^{R_B}_2(\theta,\theta') = \frac{\lambda^{R_A}_2(\theta)}{\Omega(\theta, \theta')} 
\ge \frac{\lambda^{R_A}_2(\theta)}{(K_0+1)\Omega(\theta)} 
\ge \frac{\mu}{K_0+1} $. 
Also, 
\begin{align*}
\Omega(\theta, \theta') &= \Omega(\theta) + \Omega(\theta') \ge (1 + \frac{1}{K_0})\Omega(\theta)
 > (1 + \frac{1}{K_0})k_1\max_s A_{s}(\theta) \ge 2\max_s A_{s}(\theta). %\ge -2 A(\theta)_{s,s}.
\end{align*}
So for any state $s$, the diagonal element $B_s(\theta, \theta') = 1 - 
\frac{A_s(\theta)}{\Omega(\theta, \theta')}> \frac{1}{2}$.
From~\cite{fill1991}, this diagonal property and the bound 
on $1-\lambda_2^{R_B}(\theta,\theta')$ give the result.
%\qed
\end{proof}

Our overall proof strategy is to show that for {$\| \theta \|$} and $W$ large 
enough, the conditions of Lemma~\ref{lem:eig_lemma} hold with 
high probability. We show that Lemma~\ref{lem:eig_lemma} then allows the 
distribution over latent states for the continuous-time MJP and its 
discrete-time counterpart embedded in $W$ to be brought arbitrarily 
close to $\pi_\theta$ (and thus to each other), allowing our sampler 
to inherit mixing properties of the ideal sampler. For the remaining 
$\theta$ and $W$, we will exploit their boundedness to establish a 
`small-set condition' where the MCMC algorithm forgets its state with 
some probability. These two conditions will be sufficient for 
geometric ergodicity. The next assumption states these small-set conditions 
for the ideal sampler.
% This combined with the
%likelihood $p(X|s, \theta)$ being bounded  gives the result.
%\begin{definition}
%Let $\pi_\theta$ be the stationary distribution of the MJP with rate-matrix 
%$A(\theta)$, and define $D_\theta = \text{diag}(\pi_\theta)$. Define 
%$\tilde{A}(\theta) = D_\theta^{-1}A(\theta)D_\theta$, and the 
%{\em reversibilization} of $A(\theta)$ as $R_A(\theta) = 
%(A(\theta)+\tilde{A}(\theta))/2$. 
%\label{def:mjp_symm}
%\end{definition}

\begin{assumption}
For the ideal sampler with transition probability $p_I(\theta'|\theta)$: \\
i) for each $M$, for the set $\SM_M=\{\theta:\Omega(\theta)\le M \}$,
there exists a probability measure $\phi$ and a constant
$\kappa_1 > 0$ s.t.\ % $q(\ptheta | \theta) \alpha_I(\theta, \ptheta)
%$p_I(\theta'|\theta) \geq \kappa_1 \phi(\theta')$ for $\theta \in \SM_M$, \\
$\alpha_I(\theta, \theta'; X) q(\theta' | \theta) \ge \kappa_1 \phi(\theta')$
 for $\theta \in \SM_M$. 
Thus $B_M$ is a $1$-small set. \\
ii) for $M$ large enough, $\exists \rho < 1$ s.\ t.\
$\int \Omega(\ptheta) p_I(\ptheta|\theta) d\ptheta
\leq (1-\rho) \Omega(\theta)+L_I$, $\forall \theta \not\in \SM_M$.
  \label{asmp:ideal_geom}
\end{assumption}
\noindent %Here we specify the properties of the ideal sampler that we
%want our proposed sampler to inherit. 
These two conditions are standard
small-set and drift conditions necessary for the ideal sampler to satisfy
geometric ergodicity. The first implies that for $\theta$ in
$\SM_M$, the ideal sampler forgets its current
location with probability $\kappa_1$. The second condition ensures that
for $\theta$ outside this set, the ideal sampler drifts towards
$\SM_M$. These two conditions together imply geometric
mixing with rate equal or faster than $\kappa_1$~\cite{meyn2009}.
Observe that we have used $\Omega(\theta)$ as the so-called 
Lyapunov-Foster function to define the drift condition for the ideal 
sampler. %and whose sub-level sets form the small sets. 
This is the most natural choice,
though our proof can be tailored to different choices. Similarly, we
could easily allow $\SM_M$ to be an $n$-small set for any $n\ge 1$ (so
the ideal sampler needs $n$ steps before it can forget its current
value in $\SM_M$); we restrict ourselves to the $1$-small case for
clarity.


%\begin{assumption}
%  $\inf_{\ptheta} \left(\Omega(\theta,\ptheta) - \Mx{\theta}\right)
%  \ge m\Mx{\theta} \forall \theta$.
 % There exist constants $m_1$ and $m_2$ such that
  %$\inf_\theta \Omega(\theta) = m_1 > 0$ and $\sup_{s,\theta}
  %\frac{A_s(\theta)}{\Omega(\theta)} = m_2 < 1$.
  %\label{asmp:low_bnd}
%\end{assumption}
% \noindent
% \{
% Assumption~\ref{asmp:cond_num} allows us to easily ensure this is
% satisfied by setting $\Omega(\theta) = \Mx{\theta} + \mx{\theta}+m_1$.
% Another option is to set $\Omega(\theta) = \kappa\Mx{\theta} + m_1$ some
% $\kappa > 1$.  If $\inf_\theta \Mx{\theta} > 0$ (as is often the case),
% we can just set $\Omega(\theta) = \Mx{\theta} + \mx{\theta}$ so that
% $m_2 = \frac{1}{1+\mu}$.}


\begin{assumption}
$\exists$ $ \ub > \lb > 0$ s.t.
$\prod P(X | s_o, \theta) \in [\lb, \ub]$ for any state $s_o$ and $\theta$.%\vinayak{Is this for all $s_o$?}
  \label{asmp:obs_bnd}
\end{assumption}
\noindent This assumption follows~\cite{miasojedow2017}, and holds if
%the observation process involves no hard constraints over the latent state, and
$\theta$ does not include parameters of the observation process (or if so,
the likelihood is finite and nonzero for all settings of $\theta$). We can relax this assumption,
though this will introduce technicalities unrelated to our focus, which 
is on complications in parameter inference arising from the continuous-time
dynamics, rather than the observation process. 
%Extensions to the more
%general case should be clear from our proof.

\begin{assumption}
Given the proposal density $q(\ptheta | \theta)$, $\exists \eta_0 > 0, \theta_2 > 0$ 
such that for $\theta$ satisfying $\| \theta \|  > \theta_2$, 
$ \int_\Theta \Omega(\ptheta)^2 q(\ptheta | \theta)d\ptheta \leq \eta_0 \Omega(\theta)^2.$
\label{asmp:integ_bound}
\end{assumption}
\noindent This mild requirement can be satisfied by choosing a proposal 
distribution $q$ that does not attempt to explore large $\theta$'s too 
aggressively.
\begin{corollary}
Given the proposal density $q(\ptheta | \theta)$, $\exists \eta_1 > 0, \theta_2 > 0$ such that for $\theta$ 
satisfying $\| \theta \|  > \theta_2$, 
$ \int_\Theta \Omega(\ptheta) q(\ptheta | \theta)d\ptheta \leq \eta_1 \Omega(\theta).$
\label{corol:integ_bound}
\end{corollary}
\begin{proof}
From assumption \ref{asmp:integ_bound},  we have $ \int_\Theta \Omega(\ptheta)^2 q(\ptheta | \theta)d\ptheta \leq \eta_0 \Omega(\theta)^2$ for $\theta$ satisfying $\| \theta \|  > \theta_2$.
For such $\theta$, by the Cauchy-Schwarz inequality, we have
\begin{align*}
\left[ \int_\Theta \Omega(\ptheta) q(\ptheta | \theta) d\ptheta \right]^2 &\le \int_\Theta \Omega(\ptheta)^2 q(\ptheta | \theta) d\ptheta \cdot \int_\Theta q(\ptheta | \theta) d\ptheta \le \eta_0 \Omega(\theta)^2.
\end{align*}
So for $\theta$ satisfying $\| \theta \|  > \theta_2$, we have $\int_\Theta \Omega(\ptheta) q(\ptheta | \theta) d\ptheta \le \sqrt{\eta_0} \Omega(\theta).$
%\qed
\end{proof}
%\begin{assumption}
%For any positive $\epsilon > 0$ and $K_0 > 1$, there exists $\theta_2$, such that for $\theta > \theta_2$, $P(\mid \frac{q(\theta | \theta')p(\theta')}{q(\theta | \theta')p(\theta)}\mid \leq K_0 | \theta) \geq 1 - \epsilon$, where $\theta' \sim q(\cdot | \theta)$.
%\end{assumption}

%\begin{assumption}
%For any positive $\epsilon$, there exists $h > 0$ and $\theta_2 > 0$, such that $P(\theta' \in B_{\theta, h} | \theta) > 1 - \epsilon$ for $\theta \in B_{0, \theta_2}^c$ .
%\end{assumption}

%\begin{assumption}
%For any positive $\epsilon > 0$ and $K_1 > 1$, there exists $\theta_3$, such that for $\theta > \theta_3$, $P(\frac{1}{K_1}%\frac{\Omega(\theta')}{\Omega(\theta)} \leq K_1 | \theta) \geq 1 - \epsilon$, where $\theta' \sim q(\cdot | \theta)$.
%\end{assumption}
%\begin{assumption}
%There exists $M_1 > 0$ and $\theta_3 > 0$ such that $\frac{-p'(\theta)}{p(\theta)} \le M_1$ for $\theta \in B_{0, \theta_3}^c$.
%\end{assumption}

We need two further assumptions on the proposal distribution $q(\theta'|\theta)$.
\begin{assumption}
For any $\epsilon>0$,  there exist finite $M_\epsilon$, $\theta_{3,\epsilon}$
such that for $\theta$ satisfying $\| \theta \|  > \theta_{3,\epsilon}$,
the condition $q(\{\theta': \frac{p(\theta')q(\theta|\theta')}{p(\theta)q(\theta'|\theta)} \le M_\epsilon\}|\theta) 
> 1 - \epsilon$ holds.
  \label{asmp:prior}
\end{assumption}
\noindent This holds, when e.g.\ $p(\theta)$ is a gamma distribution,
and $q(\theta'|\theta)$ is Gaussian.
%\begin{proof}
%From the assumptoin, there exists $\theta_5 > 0$ such that $p(\theta)$ is decreasing with respect to $\Vert \theta \Vert$ for $\theta \in B_{0, \theta_5}^c$. For $\theta > \max(\theta_2, \theta_5)$, $\theta' \in B_{\theta, h}$ with probability greater than $1 - \epsilon$.
%\begin{align*}
%\log \frac{p(\theta')}{p(\theta)} &\le \log\frac{p(\theta - h)}{p(\theta)}
%= \log p(\theta - h) - \log p(\theta)\\
%& \le -h \frac{p'(\chi)}{p(\chi)} \le M_1h
%\end{align*}
%,where $\chi \in B_{\theta, h}$.
%So $p(\theta')/p(\theta) \le M_1 h$ with probability greater than $1 - \epsilon$.
%\end{proof}

%\begin{assumption}
%There exists $M_2 > 0$ and $\theta_4 > 0$ such that $\frac{\Omega'(\theta)}{\Omega(\theta)} \le M_2$ for $\theta \in B_{0, \theta_4}^c$.
%\end{assumption}

%\begin{assumption}
%There exists $\theta_5 > 0$ such that $p(\theta)$ is decreasing with respect to $\Vert \theta \Vert$ for $\theta \in B_{0, \theta_5}^c$.
%\end{assumption}

\begin{assumption}
For any $\epsilon > 0$ and  $K > 1$, there exists $\theta_{4,\epsilon}^K$ such that 
for $\theta$ satisfying $\| \theta \|  > \theta_{4,\epsilon}^K$, the 
condition
$q(\{\theta':\frac{\Omega(\theta')}{\Omega(\theta)} \in 
  \left[\frac{1}{K}, K\right]\} | \theta) > 1 - \epsilon$ holds.
  \label{asmp:omega}
\end{assumption}
\noindent This holds when e.g.\ $q(\theta'|\theta)$ is a centered on $\theta$ and has 
finite variance.

%\begin{proof}
%From the assumptoin, there exists $\theta_5 > 0$ such that $p(\theta)$ is decreasing with respect to $\Vert \theta \Vert$ for $\theta \in B_{0, \theta_5}^c$. For $\theta > \max(\theta_2, \theta_5)$, $\theta' \in B_{\theta, h}$ with probability greater than $1 - \epsilon$.
%\begin{align*}
%\log(\frac{\Omega(\theta')}{\Omega(\theta)}) & \le \log(\frac{\Omega(\theta + h)}{\Omega(\theta)})\\
%& = h \frac{\Omega(\chi')}{\Omega(\chi')} \le h M_2
%\end{align*}
%Similarily, we have
%\begin{align*}
%\log(\frac{\Omega(\theta')}{\Omega(\theta)}) & \ge \log(\frac{\Omega(\theta - h)}{\Omega(\theta)})\\
%& = -h \frac{\Omega(\chi'')}{\Omega(\chi'')} \ge -h M_2
%\end{align*}
%So $\Omega(\theta') / \Omega(\theta) \in [\exp(-hM_2), \exp(hM_2)]$ with probability greater than $1 - \epsilon$.
%\end{proof}

%\begin{assumption}
%For any positive $\epsilon > 0$ and $0 < K_2 < 1$, there exists $\theta_4$, such that for $\theta > \theta_4$, $P(\theta' > \theta K_2 | \theta) %\geq 1 - \epsilon$, where $\theta' \sim q(\cdot | \theta)$.
%\end{assumption}

%\begin{assumption}
%For the above $C \subseteq \Theta$, $\exists$ $\bar{\Omega} > 0$ s.t.
%$\Omega(\theta)  \leq \bar{\Omega}$ for $\forall \theta \in C$.
%\end{assumption}

%\begin{definition}
%Given $K > 1$, $\epsilon > 0$, define $\theta_\epsilon^K$ as $\max\{ \theta_0, \theta_1, \theta_2, \theta_3, \theta_4^\epsilon, \theta_{5, K}^\epsilon\}$.
%\label{def:constant}
%\end{definition}


\begin{theorem}
Under the above assumptions, our auxiliary variable MCMC sampler is
geometrically ergodic.  \label{thm:geom_erg}
\end{theorem}
\begin{proof}
\noindent This theorem follows from two lemmas we will prove.
Lemma~\ref{lem:small_set} shows there exist small sets 
$\{(W,\theta,\vartheta): \lambda_1|W| + \Omega(\theta) < M \}$ for 
$\lambda_1, M > 0$, within which our sampler forgets its current state with 
some positive probability. Lemma~\ref{lem:drift}
shows that for appropriate $(\lambda_1,M)$, our sampler drifts towards this set whenever
outside. Together, these two results imply geometric
ergodicity~\citep[Theorems 15.0.1 and Lemma 15.2.8]{meyn2009}.
If $\sup_\theta \Omega(\theta) < \infty$, we just need 
the small set $\{(W,\theta,\vartheta: |W| < M \}$ for some $M$.
%\qed
\end{proof}
For easier comparison with the ideal sampler, we begin an MCMC 
iteration from step 5 in
Algorithm~\ref{alg:MH_improved}. Thus, our sampler operates on 
$(\theta,\vartheta,W)$,  with $\theta$ the current 
parameter, $\vartheta$ the auxiliary variable, and $W$ the Poisson grid. 
An MCMC iteration updates this to $(\theta',\vartheta',W')$ by 
(a) sampling states $V$ (or $(S,T)$)  with a backward pass, 
(b) discarding $\vartheta$, (c) sampling $\ptheta$ from $q(\ptheta|\theta)$, 
(d) sampling $U'$ given $(\theta,\ptheta)$ and setting $W' = T \cup U'$, (e) proposing to swap 
$(\theta,\ptheta)$ and then (f) accepting or rejecting with a forward pass. 
On acceptance, $\theta' = \ptheta$ and
$\vartheta' = \theta$, and on rejection, $\theta'=\theta$ and 
$\vartheta'=\ptheta$. We write $(\theta'',\vartheta'',W'')$ for the 
MCMC state after two iterations.
Recall that step (a) actually assigns states $V$ to $W$. $T$ are 
the elements of $W$ where $V$ changes value, and $S$ are the 
corresponding elements of $V$. The remaining elements $U$ are the elements 
of $W$ corresponding to self-transitions. For reference, we repeat some of 
our notation in the appendix.
% \{?
% We establish the drift condition via the Lyapunov-Foster function
% $\cV(W,\theta,\vartheta) = \left. \lambda_1|W| + \lambda_2 \Omega(\theta) +
% \Omega(\vartheta) +L \right. := \cV_W(W) + \cV_{\theta}(\theta) +
% \cV_{\theta^*}(\theta^*)$, for settings of $\lambda_1,
%   \lambda_2$ and $L$ we will define later.}
% We point out that if $\Mx{\theta}$ is bounded, then we can replace
% the two $\Omega$ terms by their supremum and absorb them into the
% constant $L$, so that the Lyapunov function only involves $W$.
% The small set will be a level set of this function, i.e.\
% $B_\alpha := \{(\theta,\theta^*,W): \cV(\theta,\vartheta,W) \le \alpha\}$.
% We show that this is a 2-small set (i.e.\ the distribution over states
% after 2-steps of our sampler satisfies $P^2(\cdot|\theta,\vartheta,W)
% \ge \epsilon \ptheta(\cdot)$ for all $(\theta,\vartheta,W) \in \SM_\alpha$.
% Unlike the ideal sampler, we need a 2-step transition because of the
% exchange move we make,
% where on acceptance $\vartheta'=\theta$, while on rejection, $\theta'=\theta$.

We first bound self-transition probabilities
of the embedded Markov chain from $0$: 
%Proofs, if not included, can be found in the appendix.
%\begin{proposition}
%The a priori probability the embedded Markov chain makes a self-transition,
%$P(V_{i + 1}=s|V_i=s,\theta,\theta^*)$ is uniformly bounded away
%from $0$ for all $s,\theta,\theta^*$.
%\end{proposition}
%\begin{proof}
%    & \ge 1-\frac{A_{s_0}(\theta)}{\Mx{\theta}} \ge 1 - \mu
% \intertext{For $\Omega(\theta) \ge k\Omega(\theta^*)$}
% P(V_{i + 1}=s|V_i=s,\theta,\theta^*) &=
%   1 - \frac{A_{s_0}(\theta)}{\Omega(\theta) + \Omega(\theta^*)} \ge
%   1 - \frac{\Omega(\theta)}{m + \Omega(\theta^*)}
% \intertext{For $\Omega(\theta) < k\Omega(\theta^*)$}
% P(V_{i + 1}=s|V_i=s,\theta,\theta^*) &\ge
%   1 - \frac{A_{s_0}(\theta)}{\Omega(\theta) + \Omega(\theta)/k}
%   \ge 1 - \frac{1}{1 + 1/k}
%\end{align*}
%\end{proof}
\begin{proposition}
The posterior probability that the embedded Markov chain makes a
self-transition,
$P(V_i = V_{i + 1} | W, X, \theta, \vartheta) \ge \delta_1 > 0$,
for %$i = 0, 1, 2, ..., |W|$ and
any $\theta,\vartheta, W$.
\label{prop:self_tr}
\end{proposition}
\noindent The proof (in the appendix) exploits the bounded likelihood from 
assumption~\ref{asmp:obs_bnd}. A simple by-product of the proof is
the following corollary:
\begin{corollary}
%The posterior probability that the embedded Markov chain makes a
%self-transition,
$P(V_{i + 1} = s|V_s=s, W, X, \theta, \vartheta) \ge \delta_1 > 0$,
for %$i = 0, 1, 2, ..., |W|$ and
any $\theta,\vartheta, W,s$.
\label{corol:self_tr}
\end{corollary}

\begin{lemma}
  For all $M,h > 0$, the set $B_{h,M} =
\left\lbrace (W, \theta, \vartheta) : |W| \leq h, \theta \in \SM_M
\right\rbrace$ is a 2-small set under our proposed sampler. 
Thus, for all $(W,\theta,\vartheta)$ {in $B_{h, M}$},
the two-step transition probability satisfies 
$P(W'', \theta'',\vartheta'' | W, \theta, \vartheta) \ge \rho_1 
\phi_1(W{''}, \theta'',\vartheta'') $ for a constant $\rho_1$ and a 
probability measure $\phi_1$ independent of the initial state.
\label{lem:small_set}
\end{lemma}
\begin{proof} Recall the definition of $B_M$, and of an $n$-small set from 
  Assumption~\ref{asmp:ideal_geom}. The $1$-step transition probability of our MCMC algorithm
  consists of two terms, corresponding to the proposed parameter being
  accepted and rejected. Discarding the latter, we have %the bound
\begin{align*}
  P(W',\theta',\vartheta'|W,\theta,\vartheta,X)&\geq
  \delta_\theta(\vartheta') q(\theta'|\theta)
 \sum_{S,T}  P(S,T | W, \theta, \vartheta, X) 
             P(W'| S, T, \theta, \theta')
             \alpha(\theta, \theta', W';X)
\end{align*}
%Suppose $(W, \theta, \vartheta) \in B_{h, M}$.
Here we use the fact that given $(S,T)$,
$P(W'|S,T,\theta,\theta',X)$ is independent of  $X$.
We further bound the summation over $(S,T)$ by considering only the terms
with $S$ a constant. When this constant is  state $s^*$, we write this as 
$(S=[s^*], T= \emptyset)$. This corresponds to $|W|$ self-transitions 
after the starting state $S_0=s^*$, so that
%\vspace{-.1in}
\begin{align*}
  P(S=[s^*], T = \emptyset | W, \theta, &\vartheta, X) =
P(S_0=s^*|X,W, \theta, \vartheta)\prod_{i = 0}^{|W| - 1} 
P(V_{i + 1} = s^* | V_i = s^*,X,W,\theta,\vartheta) \\ 
%&\prod P(X_{[w_i, w_{i + 1})} | v_i = s_0, \theta)\\
& \geq P(S_0=s^*|X,W, \theta, \vartheta)\delta_1^{|W|} %\numberthis %\eta_1
\end{align*}
Here $\delta_1$ is the lower bound from Corollary~\ref{corol:self_tr}.
With $S(t)$ fixed at $s^*$, $W'$ is a Poisson process with rate
%$r(\theta', \theta, s_0) = 
$\Omega(\theta') + \Omega(\theta) - A_{s^*}(\theta)$.
%$> \epsilon_1 > 0$.
From the %fact that $\Omega(\theta',\theta) = \Omega(\theta') + \Omega(\theta)$,
%we apply the 
Poisson superposition theorem,
%$\PP(r(\theta, \ptheta, s_0)) = \PP(\Omega(\ptheta)) \cup \PP(r(\theta, \ptheta, s_0) - \Omega(\ptheta))$.
\begin{align*}
  P(W' |  S=&[s^*], T = \emptyset, \theta', \theta)  \geq P(W' \from
\PP(\Omega(\theta')))
P(\emptyset \from \PP(\Omega(\theta)-A_{s^*}(\theta) ))\\
  & \geq P(W' \from \PP(\Omega(\theta'))) P(\emptyset \from \PP(\Omega(\theta) ))\\
%& = P(W' \from \PP(\Omega(\theta'))) \exp(-\Omega(\theta)t_{end})\\
  & \geq P(W' \from \PP(\Omega(\theta'))) \exp(-M t_{end}) 
\quad \text{(since for $\theta \in B_M$, $\Omega(\theta) \le M$)}.
%\numberthis %\quad (\text{since }\theta\in B_M)
\end{align*}
%\vspace{-.05in}
Thus we have
\begin{align*}
  \sum_{S,T} P(S,T,W' | W, \theta, \vartheta, X) & 
  \geq \sum_{{s^*}} P(S{=[s^*]}, T = \emptyset | W, \theta, \vartheta, X)
  P(W' | S{=[s^*]}, T=\emptyset,\theta', \theta)\\
               &\geq \delta_1^{|W|} \exp(-Mt_{end})
P(W' \from \PP(\Omega(\theta'))) \numberthis
\label{eq:marg}
\end{align*}
Finally we relate the acceptance rate to that of the ideal sampler:
\begin{align*}
\alpha(\theta, \theta', W'; X) &
%= 1 \wedge \frac{P(X | W', \theta', \theta)
%q(\theta|\theta')p(\theta')}{P(X | W', \theta, \theta')q(\theta'|\theta)p(\theta)}
= 1 \wedge \frac{P(X|W', \theta', \theta) / P(X|\theta')}{P(X|W', \theta,
\theta') / P(X|\theta)} \cdot \frac{P(X | \theta')
q(\theta|\theta')p(\theta')}{P(X | \theta)q(\theta'|\theta)p(\theta)}\\
& \geq 1 \wedge \frac{\lb^2}{\ub^2} \cdot 	\frac{P(X | \theta')
q(\theta|\theta')p(\theta')}{P(X | \theta)q(\theta'|\theta)p(\theta)}
 \geq \alpha_I(\theta, \theta';X)\frac{\lb^2}{\ub^2}.
\numberthis
\label{eq:acc}
\end{align*}
Since by assumption $|W| \le h$, %from equations~\eqref{eq:marg} and~\eqref{eq:acc}, 
and $q(\theta'|\theta)\alpha_I(\theta,\theta';X) \ge \kappa_1 \phi(\theta')$ 
(from assumption \ref{asmp:ideal_geom}),
\begin{align*}
P(W', \theta',\vartheta' | W, \theta, \vartheta)  
%&\ge \frac{\lb^2 }{\ub^2}\delta_1^{h}
%\exp(-M t_{end})\delta_\theta(\vartheta')P(W' \from \PP(\Omega(\theta')) 
% \alpha_I(\theta', \theta;X)q(\theta'|\theta)\\
  & \geq \frac{\lb^2 }{\ub^2}\delta_1^{h}
\exp(-M t_{end})\delta_\theta(\vartheta')\kappa_1P(W' \from \PP(\Omega(\theta'))\phi(\theta') \\
  & \assign \rho_1 \delta_\theta(\vartheta')P(W' \from \PP(\Omega(\theta'))\phi(\theta')
\end{align*}
{Write $F_{Poiss(a)}$ for the CDF of a rate-$a$ Poisson.
The two-step transition satisfies}
\begin{align*}
  P(W'', \theta''&,\vartheta'' | W, \theta, \vartheta)  
  %\int P(W'', \theta'',\vartheta'' | W', \theta', \vartheta')
  %    P(W', \theta',\vartheta' | W, \theta, \vartheta)
  %    dW' d\theta' d\vartheta' \\
       \ge \int_{\SM_{h,M}} P(W'', \theta'',\vartheta'' | W', \theta', \vartheta')
       P(W', \theta',\vartheta' | W, \theta, \vartheta)
       dW' d\theta' d\vartheta' \\
       &\ge \int_{\SM_{h,M}}  \rho_1 \delta_{\theta'}(\vartheta'')P(W'' \from \PP(\Omega(\theta''))\phi(\theta'') \\
         &\qquad \qquad \rho_1 \delta_\theta(\vartheta')P(W' \from \PP(\Omega(\theta'))\phi(\theta')
       dW' d\theta' d\vartheta' \\
       &\ge \rho_1^2 \phi(\theta'')P(W'' \from \PP(\Omega(\theta''))
       \int_{\SM_{h,M}} \!\!\!\! \delta_{\theta'}(\vartheta'')
       F_{Poiss(\Omega(\theta'))}(h)\phi(\theta')
       d\theta'  \\
       & \ge \rho_1^2 P(W'' \from
       \PP(\Omega(\theta''))\phi(\theta'')\phi(\vartheta'')F_{Poiss(\Omega(\vartheta''))}(h)\delta_{\SM_{h,M}}(\vartheta'') \\
       & \ge \rho_1^2 P(W'' \from
       \PP(\Omega(\theta''))\phi(\theta'')
       \phi(\vartheta'')\delta_{\SM_{h,M}}(\vartheta'')\exp(-\Omega(\vartheta''))  \numberthis
       \label{eq:density_lowbound}
\end{align*}
The last line uses $F_{Poiss(a)}(h) \ge F_{Poiss(a)}(0) = \exp(-a)\ \forall 
a$, 
%Then
%\begin{align*}
%\int \phi(\vartheta'')\delta_{\SM_{h,M}}(\vartheta'')\exp(-\Omega(\vartheta'')) d\vartheta'' \le \int_{\SM_{h,M}}\phi(\vartheta'')d\vartheta'' < 1
%\end{align*}
%So, $\phi(\vartheta'')\delta_{\SM_{h,M}}(\vartheta'')\exp(-\Omega(\vartheta''))$ is proportional to a probability density.
and gives our result, 
with $\phi_1(W'',\theta'',\vartheta'') \propto P(W'' \from
  \PP(\Omega(\theta''))\phi(\theta'') \phi(\vartheta'')
  \delta_{\SM_{h,M}}(\vartheta'')\exp(-\Omega(\vartheta''))  $.
%\qed
\end{proof}
\noindent We have established the small set condition: for any point 
inside $B_{h,M}$ our sampler forgets its state with nonzero 
probability. We next establish a drift condition, showing that outside 
this small set, the algorithm drifts back towards it 
(Lemma~\ref{lem:drift}).
%Observe that if $\Omega(\theta)$ is bounded uniformly, then
%the drift condition need not include $\theta$. To prove the drift
%condition for the general case when $\Omega(\theta)$ is unbounded,
We first establish a result needed when $\Mx{\theta}$ is unbounded 
as $\theta$ increases.
This states that the acceptance probabilities of our 
sampler and the ideal sampler can be brought arbitrarily close
outside a small set, so long as $\Omega(\theta)$ and
$\Omega(\theta')$ are sufficiently close.
  \begin{lemma}
  %Consider two successive observations, separated by time $t_{diff}$, and
  Suppose %$\theta$ and $\theta'$ satisfy 
  $\frac{1}{K_0} \le \frac{\Omega(\theta)}{\Omega(\theta')} \leq K_0
  $, for $K_0$ satisfying $(1 + \frac{1}{K_0})k_1 \ge 2$  
  ($k_1$ is from Assumption~\ref{asmp:unif_rate}). Write $\mW$ for the
  minimum number of elements of grid $W$ between any successive pairs of observations.
  %$\| \theta'  - \theta \| \leq h$
  For any $\epsilon > 0$, there exist  $w^{K_0}_\epsilon,  \theta_{5, \epsilon}^{K_0} > 0$ such that
  $|P(X| W, \theta, \theta') - P(X | \theta)| < \epsilon$
  for any $(W, \theta)$ with $\mW > w^{K_0}_\epsilon$ and $\| \theta \| > \theta_{5, \epsilon}^{K_0}$.
  \label{lem:eigenvalue_lemma}
  \end{lemma}
  \begin{proof}
% Since $B(\theta, \theta') = I+\frac{A(\theta)}{\Omega(\theta, \theta')}$,
% %there is a one to one mapping between the eigen values of B and the
% it is easy to see that both $A(\theta)$ and $B(\theta,\theta')$ have
% the same stationary distribution, call this $P_{st}(s|\theta)$. Further,
% the eigenvalues of $A(\theta)$ and $B(\theta,\theta')$ satisfy
% $\lambda_B(\theta, \theta') = 1 - \frac{\lambda_A(\theta)}{\Omega(\theta,
% \theta')}$, in particular, the second eigenvalue
% $\lambda^2_B(\theta,\theta')$ of $B$ (which determines its mixing
% properties) equals $1 - \frac{\lambda^2_A(\theta)}{\Omega(\theta, \theta')}$.
%
{From lemma \ref{lem:eig_lemma}, for all $\theta, \theta'$ satisfying 
 the lemma's assumptions, 
 %$\frac{1}{K_0} \le \frac{\Omega(\theta)}{\Omega(\theta')} \leq K_0$ for some $K_0 > 1$ which satisfies $(1 + \frac{1}{K_0})k_1 \ge 2$, 
 the Markov chain with transition matrix $B(\theta, \theta')$ converges 
 geometrically to stationarity distribution $\pi_\theta$  
 at a rate uniformly bounded away from 0.
}
By setting $\mW$ large enough, for all such $(\theta,\theta')$ and for 
any initial state, the Markov chain would have mixed beween each pair of
observations, with distribution over states returning arbitrarily 
close to $\pi_\theta$.


%Our proof strategy is to show that %$\theta$ and $W$ large enough 
%{for $\| \theta \|$ and $\mW$ large enough}, the
%distribution over latent states for the continuous-time MJP and
%its discrete-time counterpart embedded in $W$ can be brought arbitrarily
%close to %$P_{st}$ (and thus 
%to each other. This combined with the
%likelihood $p(X|\theta)$ being bounded  gives the result.
%\do we need to define $P_{st}$?
%We start with the discrete-time system.
%Write $P^{w}(\cdot | W, \theta, \theta' )$ for the $w$-step transition
%probability (equal to $B^w(\theta, \theta')$).
%\boqian{Assumption~\ref{asmp:cond_num} requires
%%that $\lambda^A_2(\theta)$ satisfies
%$\lambda^A_2(\theta) \geq \mu \Omega(\theta)$.
%Together with the assumption $\Omega(\theta') \leq K_0 \Omega(\theta)$,
%this gives
%$\frac{\lambda^A_2(\theta)}{\Omega(\theta) + \Omega(\theta')} \geq
%\mu / (1 + K_0) $, implying that the second eigenvalue of
%$B(\theta, \theta')$ is bounded away from $1$.}\boqian{remove?}
%any $\epsilon' > 0$, there exists a $w_0$ such that for all $W$
%with $|W| > w_0$, we have $\| P^{|W|}(\cdot | W, \theta, \theta')
%- P_{st}(\cdot | \theta)\|{\text{TV}} \leq
%{\epsilon'}$.


%From the assumption \ref{asmp:obs_bnd}, we have %$\exists \ \xi_1 > \eta_1 > 0$, such that,
%$0 < \eta_1 \leq P(X | V=v, \theta) \leq \xi_1$, so that
%the following. Assume the state space has $N$ elements.
Write $W_X$ for the indices of the grid $W$ containing observations, 
and $V_X$ for the Markov chain state at these times (illustrated 
in Section~\ref{sec:notation} in the appendix).
Let $P_B(V_X | W, \theta, \theta')$ be the probability distribution over
$V_X$ under the Markov chain with transition matrix $B$ given 
$W$ and $P_{st}(V_X|\theta)$ be the probability of $V_X$ sampled
independently under the stationary distribution. 
Let $P(X | W, \theta, \theta')$ be the marginal probability of the 
observations $X$ under that Markov chain $B(\theta,\theta')$ given $W$. Dropping $W$ and 
$\theta'$ from notation, $P(X|\theta)$
is the probability of the observations under the rate-$A(\theta)$ MJP.

From the first paragraph, for $\mW > w_{0}$ for 
large enough $w_0$,
$P_B(V_X | W, \theta, \theta')$ and  $P_{st}(V_X | W, \theta)$ can be
brought $\epsilon'$ close.
% Then,
% \begin{align*}
%   P(X|W , \theta, \theta') &= \sum_{V_X} P(X | V_X, W, \theta, \theta') P_B(V_X | W, \theta, \theta')\\
% &= \sum_{V_X} P(X | V_{X}, \theta) P_{B}(V_{X} | W, \theta, \theta')\\
% \end{align*}
%  We also define the stationary likelihood as
%  $$P_{st}(X | \theta) = \sum_{V_X} P(X | V_X, \theta) P_{st}(V_X | \theta).$$
Then for any $W$ with $\mW > w_0$, we have
\begin{align*}
  |P(X|W , \theta, \theta') - & P_{st}(X | \theta)| = | \sum_{V_X} P(X|V_X, \theta) [P_B(V_X | W, \theta, \theta') -  P_{st}(V_X | \theta) ]|\\
& \leq \sum_{V_X} P(X | V_X, \theta)|P_B(V_X | W, \theta, \theta') -  P_{st}(V_X | \theta)|\le \epsilon'',
%& \leq \sum_{V_X} P(X | V_X, \theta){\epsilon'} \le \epsilon'' \quad (\text{from Assumption}~\ref{asmp:obs_bnd}).
%& \leq N \| X \| \ub \epsilon' := \epsilon''
\end{align*}
using $P(X|V_X,\theta) \le \ub$ 
(Assumption~\ref{asmp:obs_bnd}), and 
$\sum_{V_X} |P_B(V_X | W, \theta, \theta') -  P_{st}(V_X | \theta)| < \epsilon$.
%So for $W$ with $|W| > W_0$, we have  $|P(X| W, \theta, \theta') - P_{st}(X | \theta)| < \epsilon / 4$. \\
For large $\theta$, we prove a similar result in the continuous 
case by uniformization. For any $\theta'$,
\begin{align*}
P(X | \theta) %&= \int_W dW P(X , W | \theta, \theta')\\
= \int dW P(X | W, \theta, \theta') P(W | \theta, \theta') %+ \int_{\mW \leq w_0}dW P(X | W, \theta, \theta') P(W | \theta, \theta').
\quad 
(\text{$P(W|\theta,\theta')$ is a rate-$\Omega(\theta) + 
\Omega(\theta')$ Poisson process}).
\end{align*}
%\sum_{v} P(X | V_{|W|} = v, \theta) P^{(|W|)}(V_{|W|} = v | W, \theta, \theta') P(W|\theta, \theta')
%&= \sum_W \sum_{V_{|W|}} P(X | V_{|W|}, \theta) P^{(|W|)}(V_{|W|} | W, \theta, \theta') P(W|\theta, \theta')\\
%&= \sum_{W s.t. |W| \leq W_0'} \sum_{V_{|W|}} P(X | V_{|W|}, \theta) P^{(|W|)}(V_{|W|} | W, \theta, \theta') P(W|\theta, \theta') + \\
%&\sum_{W s.t. |W| > W_0'} \sum_{V_{|W|}} P(X | V_{|W|}, \theta) P^{(|W|)}(V_{|W|} | W, \theta, \theta') P(W|\theta, \theta')
%\end{align*}
% Consider the difference between the first term $\sum_{W s.t. |W| > W_0} P(X | W, \theta, \theta') P(W | \theta, \theta')$ and $P_{st}(X | \theta)P(|W| > W_0 | \theta, \theta')$. Also from the previous derivations, $|P(X| W, \theta, \theta') - P_{st}(X | \theta)| < \epsilon / 4$ for $W$ with $|W| > W_0$.\\
% \begin{align*}
% &|\sum_{W s.t. |W| > W_0} P(X | W, \theta, \theta') P(W | \theta, \theta')  - P_{st}(X | \theta)P(|W| > W_0 | \theta, \theta')|  \leq  \\
% &\sum_{W s.t. |W| > W_0} |P(X | W, \theta, \theta') - P_{st}(X | \theta)|  P(W | \theta, \theta') \leq P(|W| > W_0 | \theta, \theta') \epsilon / 4
% \end{align*}
% For the $W_0$, there exists $\theta_0 > 0$ such that for any $\theta > \theta_0$,and any $\theta'$, $P(|W| \leq W_0 | \theta, \theta') < \frac{\epsilon}{4\xi_1}.$
% For the second term, we have
% \begin{align*}
% &|\sum_{W s.t. |W| \leq W_0} \sum_{v} P(X | V_{|W|} = v, \theta) P^{(|W|)}(V_{|W|} = v | W, \theta, \theta') P(W|\theta, \theta')|  \\
% &\leq \xi_1 P(|W| \leq W_0 | \theta, \theta') \leq \epsilon / 4
% \end{align*}
% Consider the difference between $P(X|\theta)$ and $P_{st}(X|\theta)$ for $\theta > \theta_0$, and any $\theta'$ with $\Omega(\theta') \le K_0\Omega(\theta)$.
% \begin{align*}
% |P(X|\theta) - P_{st}(X|\theta)| &\leq |\sum_{W s.t. |W| > W_0} P(X | W, \theta, \theta') P(W | \theta, \theta')  - P_{st}(X | \theta)P(|W| > W_0 |\theta, \theta')| \\
% &+ |\sum_{W s.t. |W| \leq W_0} \sum_{v} P(X | V_{|W|} = v, \theta) P^{(|W|)}(V_{|W|} = v | W, \theta, \theta') P(W|\theta, \theta')|\\
% &+ |P_{st}(X | \theta)P(|W| \leq W_0 | \theta, \theta')|\\
% &\leq \epsilon / 4 + \epsilon / 4 + \epsilon / 4 = 3\epsilon/4
% \end{align*}
We split this integral into two parts, one over the set $\{\mW > w_0\}$, and 
the second over its complement. On the former, for $w_0$ large enough, 
$|P(X |W, \theta, \theta')-P_{st}(X|\theta)|
\le \epsilon''$. %Since, under uniformization, $W$ comes from a 
For $\theta$ large enough, $\{\mW > w_0\}$ occurs
with arbitrarily high probability for any $\theta'$. Since the likelihood is bounded, the
integral over the second set can be made arbitrarily small (say, $\epsilon''$ again). 
Finally, from the triangle
inequality,
%So for $\epsilon > 0$, and $K_0 > 0$, if $|W| > W_0$ and $\theta > \theta_0$ and $\Omega(\theta') \leq K_0 \Omega(\theta)$, we have
\begin{align*}
|P(X | \theta) - P(X | W, \theta, \theta')| &\leq |P(X | \theta) -P_{st}(X | \theta) | + | P_{st}(X | \theta) -  P(X | W, \theta, \theta')|\\
       & \leq (\epsilon'' + \epsilon'') + \epsilon'' \assign \epsilon
\end{align*}
\end{proof}

\begin{proposition}
  Let $(W, \theta, \vartheta)$ be the current state of the sampler.
Then, for any $\epsilon$, there exists $\theta_\epsilon > 0$ as well as a set $\E_\epsilon \subseteq \{(W', \theta'): |\alpha_I(\theta,\theta';X) - \alpha(\theta,\theta';W',X)| \le \epsilon\}$, such that for $\theta$ satisfying $\| \theta \| > \theta_\epsilon$ and any $\vartheta$, we have
$P(E_\epsilon|W,\theta,\vartheta) > 1-\epsilon$.
\label{prop:mix0}
\end{proposition}
\noindent The previous lemma bounded the difference in probability of observations 
under the discrete-time and continuous-time processes. This result 
uses this to bound the acceptance probabilities of the ideal sampler, 
and our proposed sampler (where acceptace probabilities are calculated 
conditioned on the grid $W$). See the appendix for the proof.
%   \begin{proposition}
%     Write $P_{\theta}(W',\theta^*)$ for the conditional distribution
%     over $(W',\theta^*)$ given the current state of the MCMC sampler and the
%     observations, and let
%     $\lim_{\theta\rightarrow\infty} \Mx{\theta} = \infty$. Then for any positive
%     $\epsilon$, there exist $B_\theta$ such that for all
%     $\theta \ge B_\theta$, % with $\Omega(\theta) \ge k\Omega(\vartheta)$,
%     we have
%     $P_{\theta}(\{W'\ s.t.\ |\alpha_I(\theta,\vartheta,X) - \alpha(\theta,\vartheta,W',X)|
%     \le \epsilon)\} \ge 1-\epsilon$.%$(\theta,\theta^*)$ satisfying
%   \label{prop:mix}
%   \end{proposition}
%   \begin{proof}
%   Consider the nearest pair of observations, occuring at times $t$ and
%   $t + \Delta$. %let them be separated by a time interval $\Delta$.
%   By setting $\theta$ high enough, assumption~\ref{asmp:cond_num} ensures
%   that the distribution over waiting times of each state can be concentrated
%   arbitrarily close to $0$. Thus, the distribution over states after an
%   interval $\Delta$ can be brought arbitrarily close to the equilibrium
%   distribution of $A(\theta)$  (call this $p_{\theta}$).

%   Next, recall that the embedded Markov chain has a transition matrix
%   given by $B(\theta^*,\theta) = (I + A(\theta^*)/\Omega(\theta,\theta^*))$.
%   For any setting of $\Omega$, this has the same stationary distribution
%   $p_{\theta}$: this can be verified by multiplying $B(\theta,\theta^*)$ with
%   $p_\theta$. For the embedded Markov chain to be brought close to
%   equilibrium over $[t,t+\Delta]$, we need two conditions, 1) the number of
%   Poisson events must be large enough (to allow sufficient transitions), and 2) the
%   transition matrix $B$ must mix well enough (otherwise even a
%   large number of transition opportunities will not forget the initial state).
%   We show that for $\theta$ large enough, this holds with high probability.
%   Assumption~\ref{asmp:mono_tail} will ensure that this continues to hold
%   for all larger $\theta$ as well.

%   Recall first that the proposal distribution $q(\theta^*|\theta)$ is
%   centered at the current value $\theta$. For any $\epsilon$, we can find
%   an interval $[\theta-h,\theta+h]$ such that for all $\theta$,
%   $q(\theta^* \in [\theta-h,\theta+h]|\theta)
%   \ge 1-\epsilon$. By choosing $\theta$ large
%   enough, we can ensure that %$\Omega(\theta) \approx \Omega(\theta^*)$ (i.e.\
%   $\frac{\Omega(\theta)}{ \Omega(\theta^*)} \in [1-\epsilon,1+\epsilon]$)
%   with probability greater than $1-\epsilon$.
%   %The condition $\Omega(\theta) \ge k\Omega(\vartheta)$,
%   This, together with assumption~\ref{asmp:cond_num}, ensures that with
%   probability greater than $1-\epsilon$, the self-transition probability of $B(\theta,\vartheta)$
%   is bounded away from one, and limits how poorly $B$ can mix.
%   Second, since $W$ comes from a Poisson process with intensity
%   $\Omega(\theta')+ \Omega(\theta) - A_{S(t)}(\theta)$, a large $\theta'$
%   ensures a large $|W'|$ with high probability: by setting $B_\theta$ large
%   enough we can ensure $|W'|$ is large enough with arbitrary probability.

%   Thus, for large enough $B_\theta$ we can ensure that for all $\theta >
%   B_\theta$, the distributions $p(x_{t+\Delta}|s_t)$ and
%   $p(x_{t+\Delta}|W,s_t)$ can be brought arbitrarily close with arbitrarily
%   high probability.
%   By a simple chaining argument, this holds for $p(X|\theta)$ and
%   $p(X|\theta,W)$ as well and so too
%     \vinayak{expand}
%   $\alpha_I(\theta',\theta,X)$ and $\alpha(\theta',\theta,W,X)$
%   \end{proof}
%\begin{lemma}
%Write $P(W' | W, \theta, \vartheta, \theta')$ as the transition density with respect to the grids $W'$, there exists $\psi > 1$, such that $P(W' | W, \theta, \vartheta, \theta') \leq \psi ^{|W'|} \tilde{\Omega}(\theta,\theta')^{|W'|}\exp(-\tilde{\Omega}(\theta,\theta')\tau)$, where $\tilde{\Omega}(\theta, \theta') = (\Omega(\theta) + \Omega(\theta')) / \psi$ and $\tau$ is the length of the time interval.
%\end{lemma}
%\begin{proof}
%As we defined,  there exists $k > 1$, such that $\Omega(\theta) = k \max\{ A_s(\theta)\}$. Recall that given the current state $(W, \theta, \vartheta)$, we first sample $(S, T)$, which is a discrete time Markov chain with rate matrix $B(\theta, \vartheta) = I + \frac{A(\theta)}{\Omega(\theta) + \Omega(\vartheta)}$. Then we sample $W'$ which is a Poisson process with rate $\Omega(\theta) + \Omega(\theta') - A_{S_t}(\theta)$. Integrating out $S, T$, we can get the transition density with respect to the grids $W'$. We have
%\begin{align*}
%P(W' | W, \theta, \vartheta, \theta') &= \sum_{S,T} P(S, T | W, \theta, \vartheta) P(W' | S, T, \theta, \theta') .
%\end{align*}
%Given any $S, T$, the Poisson rate has a lower bound $\frac{k - 1}{k} (\Omega(\theta) +\Omega(\theta'))$. So
%\begin{align*}
%P(W' | S, T, \theta, \theta') &= \prod_{i = 0}^{|T|} (\Omega(\theta) + \Omega(\theta') - A_{S_i}(\theta))^{|W_i'|} \exp(-(\Omega(\theta) + \Omega(\theta') - A_{S_i}(\theta))(T_{i + 1} - T_i))\\
%& \leq \prod_{i = 0}^{|T|} (\frac{k - 1}{k}(\Omega(\theta) + \Omega(\theta')))^{|W_i'|} \exp(- \frac{k - 1}{k} (\Omega(\theta) + \Omega(\theta'))(T_{i + 1} - T_i))(\frac{k}{k - 1})^{|W'|} \\
%&\leq (\frac{k - 1}{k}(\Omega(\theta) + \Omega(\theta')))^{|W'|} \exp(- \frac{k - 1}{k} (\Omega(\theta) + \Omega(\theta'))(T_{|T| + 1} - T_0))(\frac{k}{k - 1})^{|W'|}
%\end{align*}
%So we can set $\psi = \frac{k}{k - 1}$ and $\tilde{\Omega}(\theta, \theta') = (\Omega(\theta) + \Omega(\theta')) / \psi$, and then sum up $S, T$.
%\begin{align*}
%P(W' | W, \theta, \vartheta, \theta') &= \sum_{S,T} P(S, T | W, \theta, \vartheta) P(W' | S, T, \theta, \theta') \\
%& \leq  \psi ^{|W'|} \tilde{\Omega}(\theta,\theta')^{|W'|}\exp(-\tilde{\Omega}(\theta,\theta')\tau)
%\end{align*}
%\end{proof}

\begin{lemma}(drift condition) There exist $\delta_2 \in (0, 1), \lambda_1 > 0$ and $L > 0$
  such that \\
  $\mathbb{E}\left[\lambda_1|W'| + \Omega(\theta')  | W, \theta, \vartheta, X\right]
  \leq (1 - \delta_2)\left(\lambda_1|W| + \Omega(\theta)   \right) + L$.
  %where $\lambda = \lceil \frac{(t_{end})k_2(\eta_0 + 1)}{(\eta_1^2 \kappa_1 \mathbb{P}_\phi(C)/\xi_1^2 - \eta_0)} \rceil.$
\label{lem:drift}
\end{lemma}
\begin{proof}
Since $W'=T\cup U'$, we consider $\mathbb{E}[|T| |W,\theta,\vartheta,X]$
and $\mathbb{E}[|U'| | W, \theta, \vartheta, X]$ separately.
An upper bound of $\mathbb{E}[|T| | W,\theta,\vartheta, X]$ can be derived
directly from proposition~\ref{prop:self_tr}:
\begin{align*}
\mathbb{E}[|T| |W,\theta,\vartheta,X] &= \mathbb{E}[\sum_{i = 0}^{|W|-1}
  \mathbb{I}_{\{ V_{i + 1} \neq V_i \}}| W, \theta, \vartheta, X]
\leq \sum_{i = 0}^{|W| - 1} (1 - \delta_1) = |W|(1 - \delta_1).
\end{align*}
By corollary \ref{corol:integ_bound}, there exist $\eta_1 , \theta_2$ 
such that for 
%$\theta$  satisfying 
$ \| \theta \| > \theta_2$, 
 $ \int \Omega(\ptheta) q(\ptheta | \theta)d\ptheta \leq \eta_1 \Omega(\theta) 
 $. Then,
\begin{align*}
%So for $\theta$ satisfying $ \| \theta \| > \theta_3$, we have
\mathbb{E}[|U'| |W, \theta, \vartheta, X] &= 
\mathbb{E}_{S,T, \ptheta}\mathbb{E}[|U'| | S, T, W, \theta, \vartheta, \ptheta, X] = \mathbb{E}_{S,T, \ptheta}\mathbb{E}[|U'| | S, T, W, \theta, \ptheta] \\
& \leq \mathbb{E}_{S,T, \ptheta} \left[t_{end}\Omega(\theta, \ptheta)\right] = t_{end}\int \Omega(\theta, \ptheta) q(\ptheta | \theta) d\ptheta\\
& = t_{end} \left[ \left(  \Omega(\theta) +
\int_\Theta \Omega(\ptheta) q(\ptheta | \theta)d\ptheta \right) \right] 
 \leq t_{end} (\eta_1 + 1) \Omega(\theta) %\right]
%\assign a \Omega(\theta) + b.
\end{align*}
%Next, we note that $\vartheta'$ takes on value $\theta$ with
%probability of acceptance, else it takes the value $\ptheta$ proposed  from
%$q(\ptheta|\theta)$. We bound the acceptance
%probability by $1$, so that
%\begin{align}
%\mathbb{E}[\Omega(\vartheta')|\theta,\vartheta,W,X)] &\le \Omega(\theta)
%+ \int d\ptheta (1-\alpha(\ptheta,\theta)) q(\ptheta|\theta) \Omega(\ptheta)\nonumber \\
 % & \le (1+\eta_0) \Omega(\theta)
%\end{align}
To bound $\mathbb{E}\left[\Omega(\theta')  | W, \theta, \vartheta, X\right]$,
consider the transition probability over $(W',\theta')$:
\begin{align*}
  P(dW', d\theta'&| W, \theta, \vartheta)
=d\theta' dW' \left[q(\theta' | \theta)
  \sum_{S,T} P(S, T | W, \theta, \vartheta, X)P(W' | S, T, \theta, \theta')
\alpha(\theta, \theta' ; W', X)\right. \\
&\left.+ \int q(\ptheta | \theta) \sum_{S,T} P(S, T|W,\theta,\vartheta,
    X)P(W' | S, T, \theta, \ptheta) ( 1 - {\alpha(\theta, \ptheta ; W', X)})d\ptheta
    \delta_\theta(\theta')\right].
\end{align*}
With $P(W' | W, \theta, \vartheta, \theta', X) =
\sum_{S,T} P(S, T | W, \theta, \vartheta, X)P(W' | S, T, \theta, \theta')$,
integrate out $W'$:
\begin{align*}
  P(d\theta'| W, \theta, \vartheta) &=d\theta' \int_{W'}dW'
  \bigg[q(\theta' | \theta)
     P(W' | W, \theta, \vartheta, \theta', X) \alpha(\theta, \theta' ; W', X) + \\
  &\left.  \int q(\ptheta | \theta)  P(W' |  W, \theta, \vartheta, \ptheta,
X) ( 1 - \alpha(\theta, \ptheta ; W', X))d\ptheta
\delta_\theta(\theta')\right] %\\
%&\assign d\theta' I_1(W, \theta', \theta, \vartheta) + d\theta'\delta_\theta(\theta')I_2(W, \theta, \vartheta).
\end{align*}
%Then let  $\int \Omega(\theta') P(d\theta'| W, \theta, \vartheta)
%  = \int d\theta' \Omega(\theta') I_1(W, \theta', \theta, \vartheta) + \Omega(\theta) I_2(W, \theta, \vartheta) 
%$, with
Then let  $\int \Omega(\theta') P(d\theta'| W, \theta, \vartheta)
  = I_1(W, \theta, \vartheta) + \Omega(\theta) I_2(W, \theta, \vartheta) 
$, with
\begin{align*}
  &I_1(W, \theta, \vartheta) = \int d\theta' \Omega(\theta') q(\theta' | \theta)\int dW'P(W' | W, \theta, \vartheta, \theta', X)\alpha(\theta, \theta' ; W', X), \\
&I_2(W, \theta, \vartheta) =\int d\ptheta  dW'q(\ptheta | \theta)P(W' | W, \theta, \vartheta, \ptheta, X)(1 - \alpha(\theta, \ptheta ; W', X)).
\end{align*}
{Consider the second term $I_2$.
  From Proposition~\ref{prop:mix0}, for any positive $\epsilon$, there
  exists $\theta_\epsilon > 0$ such that the set $E_{\epsilon}$
  (where $|\alpha(\theta, \ptheta ; X,W') - \alpha_I(\theta, \ptheta ; X)| \le
  \epsilon$) has probability greater than $1-\epsilon$.
  Write $I_{2,\E_{\epsilon}}$ for the integral restricted to this set, 
  and $I_{2,\E_{\epsilon}^c}$ for that over the complement, so that
 $I_{2}= I_{2,\E_{\epsilon}}+I_{2,\E_{\epsilon}^c}$.
 Then for $\theta > \theta_\epsilon$,}
% such that for $\theta > \theta_0$, we have $P(\theta' > \theta_0, |W'| >
% W_0 |\theta, \vartheta, W) > 1 - \epsilon$. Defining $\E_{\epsilon} =
% \{ (\theta', W') | \theta' > \theta_0, |W'| > W_0\}$, we can divide the
% area of integration into two complementary parts: $\E_{\epsilon}$, the
% part of $(\ptheta,W')$ where
%  $|\alpha(\theta, \ptheta | X,W') - \alpha_I(\theta, \ptheta | X)| \le \epsilon$,
%and its complement $\E^c_{\epsilon}$. Call these $I_{2,\E_{\epsilon}}$ and
%$I_{2,\E_{\epsilon}^c}$ respectively.
%For $\theta > \theta_0$, the second term has probability less than $\epsilon$, and using the bound
%$1-\alpha \le 1$, we have
%}
%use $\alpha(\theta, \theta' | W', X)\le 1$ and $q(\theta'|\theta) < \epsilon$
%to get}
%%and equation~\eqref{eq:acc} to get}
%  P(d\theta'| &W, \theta, \vartheta)
%\leq d\theta' \epsilon
% &\left.\left(1 -  \int q(\ptheta | \theta) \sum_S P(S, T|W, \theta,
% \vartheta, X)P(W'|S, T, \theta, \ptheta)\alpha_I(\theta, \ptheta)
% \frac{\eta_1^2}{\xi_1^2}d\ptheta dW' dT\right) \delta_\theta(\theta')\right]\\
%          & \leq d\ptheta \left[q(\ptheta | \theta) + \left(1 -
%          \frac{\eta_1^2}{\xi_1^2} \int_\Theta q(\ptheta |
%      \theta)\alpha_I(\theta, \ptheta) d\ptheta\right)\delta_\theta(\theta')\right]
%      \numberthis \label{eq:nu1}
\begin{align*}
I_{2,\E_{\epsilon}}(W, \theta, \vartheta) &= \int_{\E_{\epsilon}} d\ptheta  dW'q(\ptheta | \theta)P(W' | W, \theta, \vartheta, \ptheta, X)(1 - \alpha(\theta, \ptheta ; W', X)) \\
&\le \int_{\E_\epsilon}d\ptheta dW' q(\ptheta | \theta)
  P(W' | W, \theta, \vartheta, \ptheta, X)  [ 1 - (\alpha_I(\theta, \ptheta ; X)-\epsilon)] \\
&\le   \int d\ptheta dW'  q(\ptheta | \theta)
  P(W' | W, \theta, \vartheta, \ptheta, X)
  [ 1 - (\alpha_I(\theta, \ptheta ; X)-\epsilon)] \\
  &\le (1+\epsilon)  - \int  q(\ptheta | \theta) \alpha_I(\theta, \ptheta ; X) d\ptheta, \quad \text{and}  \\
  I_{2,\E_{\epsilon}^c}(W, \theta, \vartheta)  &= \int_{\E^c_{\epsilon}} d\ptheta  dW'q(\ptheta | \theta)P(W' | W, \theta, \vartheta, \ptheta, X)(1 - \alpha(\theta, \ptheta ; W', X)) \\
  &\le \int_{\E^c_{\epsilon}} d\ptheta dW'
  q(\ptheta | \theta) P(W' | W, \theta, \vartheta, \ptheta, X) \le \epsilon.
% &= \int_{\E^c_{\epsilon}} d\ptheta dW'
% q(\ptheta | \theta) P(W' |  W, \theta, \ptheta, \vartheta, X)
\end{align*}
%\int_{W_1}dW' \int_S q(\ptheta | \theta) &\int \sum_S P(S, T | W, \theta, \vartheta,
%X)P(W' | S, T, \theta, \ptheta)dT ( 1 - [\alpha_I(\theta, \ptheta |
%X,W')])d\ptheta \\
%I_{2,\E_{\epsilon}} \le \int_{\E_\epsilon}dW' d\ptheta q(\ptheta | \theta) &
%  \sum_{S,T} P(S, T | W, \theta, \vartheta, X)P(W' | S, T, \theta, \ptheta)
%  [ 1 - (\alpha_I(\theta, \ptheta | X)-\epsilon)] \\
%\le \int dW' d\ptheta q(\ptheta | \theta) &
%  P(W' | W, \theta, \vartheta, \ptheta, X)
 % [ 1 - (\alpha_I(\theta, \ptheta | X)-\epsilon)] \\
 %\le (1+\epsilon) & - \int  q(\ptheta | \theta) \alpha_I(\theta, \ptheta | X) d\ptheta, \text{while} \\
%   \intertext{Then}
%      P(d\theta'| &W, \theta, \vartheta)   \le d\theta' \int_{W'}dW'
%      \left[q(\theta' | \theta) \int \sum_S P(S, T
%   | W, \theta, \vartheta, X)P(W' | S, T, \theta, \theta')dT
%   \left[\alpha_I(\theta', \theta | X)+\epsilon_{}\right]\right.\\
%   &\left. + \int q(\ptheta | \theta) \int \sum_S P(S, T | W, \theta, \vartheta,
%   X)P(W' | S, T, \theta, \ptheta)dT ( 1 - [\alpha_I(\theta, \ptheta |
%   X)-\epsilon_\ptheta])d\ptheta \delta_\theta(\theta')\right]  \\
%   & \leq d\theta'\left[q(\theta' | \theta)\left[\alpha_I(\theta', \theta |
%   X)+\epsilon_{\theta'}\right] +
%   \left(1 -  \int q(\ptheta | \theta) [\alpha_I(\ptheta, \theta|X)-\epsilon_\ptheta]
%   d\ptheta \right) \delta_\theta(\theta')\right]\\
%   & \leq d\theta'\left[q(\theta' | \theta) \alpha_I(\theta', \theta | X) +
%   \left(1 -  \int q(\ptheta | \theta) \alpha_I(\theta, \ptheta|X) d\ptheta \right)
%   \delta_\theta(\theta')\right]+\epsilon_{\theta'}q(\theta'|\theta)+
%   \delta_\theta(\theta')\int \epsilon_{\ptheta} q(\ptheta|\theta) d\ptheta \\
%   & = p_I(\theta'|\theta,X) + \epsilon_{\theta'}q(\theta'|\theta)+
%   \delta_\theta(\theta')\int \epsilon_{\ptheta} q(\ptheta|\theta) d\ptheta
%   \numberthis \label{eq:nu2}
%, and bounding
%$\alpha$ by one on the $W$-set with probability $\epsilon$, we have the
%bound}
%the first term involves an integral over $(\theta',W')$ which 
We similarly divide the integral $I_1$ into two parts, $I_{1,E_\epsilon}$ 
(over $\E_\epsilon$) and $I_{1,E_\epsilon^c}$ (over its complement 
$\E^c_\epsilon$).
%From lemma 8, we have
%$P(W' | W, \theta, \vartheta, \theta') \le \psi ^{|W'|} \tilde{\Omega}(\theta,\theta')^{|W'|}\exp(-\tilde{\Omega}(\theta,\theta')\tau)$, where $\tilde{\Omega} = (\Omega(\theta) + \Omega(\theta')) / \psi$ and $0 < \psi < 1$ and $\tau$ is the length of the time interval.\\
%\begin{align*}
%  \tilde{I}_{1,\E^c_\epsilon} &\le  \int_{\E^c_\epsilon}  \Omega(\theta') q(\theta' | \theta)P(W' | W, \theta, \vartheta, \theta')d\theta'dW'\\
%  &=  \int_{\theta' \leq \theta_0}  \Omega(\theta') q(\theta' | \theta)P(W' | W, \theta, \vartheta, \theta')d\theta'dW' +  \int_{\theta' > \theta_0, |W'| < W_0}  \Omega(\theta') q(\theta' | \theta)P(W' | W, \theta, \vartheta, \theta')d\theta'dW'\\
%  & \leq  \Omega(\theta_0) \int_{\theta' > \theta_0}q(\theta' | \theta) d\theta' + \int_{\theta' > \theta_0} \Omega(\theta') q(\theta' | \theta) [ \sum_{w \le W_0}\frac{\tilde{\Omega}(\theta,\theta')^{w}}{w!}\exp(-\tilde{\Omega}(\theta,\theta')\tau)]d\theta'\\
%  & \leq  \Omega(\theta) \epsilon + \int_{\Theta} \Omega(\theta') q(\theta' | \theta)\epsilon d\theta'\\
%  &= \Omega(\theta) \epsilon (1 + \eta_0)
%  \{?}
%  \end{align*}
%  For $\epsilon > 0$, there exists $\theta_1 > 0$, such that for $\theta > \theta_1$ $\int_{\theta' > \theta_0}q(\theta' | \theta) d\theta' \le \epsilon$. There also exists $\theta_2 > 0$, such that for $\theta > \theta_2$ and any $\theta'$,  $\sum_{w \le W_0}\frac{\tilde{\Omega}(\theta,\theta')^{w}}{w!}\exp(-\tilde{\Omega}(\theta,\theta')\tau) < \epsilon$. For $\theta > \max(\theta_0, \theta_1, \theta_2)$, we have
%\begin{align*}
%\tilde{I}_{1,\E^c_\epsilon} &\le \Omega(\theta_0) \int_{\theta' > \theta_0}q(\theta' | \theta) d\theta' + \int_{\theta' > \theta_0} \Omega(\theta') q(\theta' | \theta) [ \sum_{w \le W_0}\frac{\tilde{\Omega}(\theta,\theta')^{w}}{w!}\exp(-\tilde{\Omega}(\theta,\theta')\tau)]d\theta'\\
%& \le  \Omega(\theta_0) \epsilon + \int_{\Theta} \Omega(\theta') q(\theta' | \theta)\epsilon d\theta' \\
%& \le \Omega(\theta_0) \epsilon + \eta_0 \Omega(\theta)\epsilon \\
%& \le (1 + \eta_0) \Omega(\theta)\epsilon
%\end{align*}
%$$\int d\theta' \Omega(\theta') I_1(W, \theta', \theta, \vartheta) =
%$ I_1(W, \theta, \vartheta) = \int_{\E_\epsilon \cup \E_\epsilon^c} 
%\hspace{-.0in} d\theta' \Omega(\theta')I_1(W,\theta',\theta,\vartheta) \assign I_{1,\epsilon} + I_{1,\epsilon}^c. \\
%$
%
%\int_{\E_\epsilon} d\theta' \Omega(\theta') I_1(W, \theta', \theta, \vartheta) +
%\int_{\E_\epsilon^c} d\theta' \Omega(\theta') I_1(W, \theta', \theta, \vartheta).
For $\|\theta\|$ large enough, we can bound the acceptance probability 
by $\alpha_I(\theta,\theta'; X) + \epsilon$ on the set $E_\epsilon$, and by corollary 
\ref{corol:integ_bound}, we get 
\begin{align*}
%\int_{\E_\epsilon} d\theta' \Omega(\theta') I_1(W, \theta', \theta, \vartheta)  &
  I_{1,E_\epsilon} & \leq \int_{\E_\epsilon} \Omega(\theta')q(\theta' | \theta) (\alpha_I(\theta, \theta'; X) + \epsilon) d\theta' 
 \leq \int \Omega(\theta')q(\theta' | \theta) \alpha_I(\theta, \theta'; X) d\theta' + \eta_1 \epsilon \Omega(\theta).
\end{align*}
For $I_{1,E_\epsilon^c}$, 
from assumption \ref{asmp:integ_bound}, %there exists $\eta_0 , \theta_2 > 0$  
%such that for $\theta$ satisfying $ \| \theta \| > \theta_2$, 
we have $\int_\Theta \Omega(\ptheta)^2 q(\ptheta | \theta)d\ptheta \leq \eta_0 \Omega(\theta)^2$
for $\|\theta\| > \theta_2$.
So, by Cauchy-Schwarz inequality and bounding the acceptance probability by one, we have
\begin{align*}
%\left[  \int_{\E_\epsilon^c} I_1(W, \theta', \theta, \vartheta) \Omega(\theta') d\theta'  \right]^2 
  \left( I_{1,E_\epsilon^c}\right)^2 %\le \left[  \int_{\E_\epsilon^c}  \Omega(\theta') q(\theta' | \theta) P(W' | W, \theta, \vartheta, \theta', X)d\theta' \right]^2\\
& \le \int_{\E_\epsilon^c} q(\theta' | \theta) P(W' |W, \theta, \vartheta, \theta', X)d\theta'dW'  \int_{\E_\epsilon^c}  \Omega(\theta')^2 q(\theta' | \theta) P(W' |W, \theta, \vartheta, \theta', X)d\theta'dW'   \\
& \le \epsilon \int  \Omega(\theta')^2 q(\theta' | \theta)d\theta' 
 \le \epsilon \eta_0 \Omega(\theta)^2 ,
\end{align*}
giving
$
I_{1,E_\epsilon^c}  \le \sqrt{\epsilon \eta_0 }\Omega(\theta) .
$
Putting these four results together, for $\theta$ satisfying $ \| \theta \| >\max(\theta_2, 
\theta_\epsilon, M) $ 
(where $M$ is from Assumption~\ref{asmp:ideal_geom} on 
the ideal sampler), we have 
\begin{align*}
  \int \Omega(\theta') P(d\theta'| W, \theta, \vartheta)
%  &= \int_{\E_\epsilon} I_1(W, \theta', \theta, \vartheta) \Omega(\theta') d\theta' + \int_{\E_\epsilon^c} I_1(W, \theta', \theta, \vartheta) \Omega(\theta') d\theta' \\ &+ I_{2,\E_{\epsilon}}(W, \theta, \vartheta) + I_{2,\E_{\epsilon}^c}(W, \theta, \vartheta) \\
  & \leq \int \Omega(\theta') q(\theta' | \theta)\alpha_I(\theta, \theta'| X) d\theta'  + \Omega(\theta)\int  q(\ptheta | \theta) (1 - \alpha_I(\theta, \ptheta | X)) d\ptheta+ \\
  &\sqrt{\eta_0\epsilon}\Omega(\theta)  +  \eta_1 \epsilon \Omega(\theta) + 2\epsilon \Omega(\theta)\\
  & \leq (1 - \rho) \Omega(\theta) + (\sqrt{\eta_0\epsilon} +  \eta_1 \epsilon + 2\epsilon) \Omega(\theta) + L_I, \quad \text{giving}
\end{align*}
%For $\theta$ satisfying $ \| \theta \| >\max(\theta_2, \theta_3, \theta_\epsilon)$, we have %\boqian{need to change afterwards}
%\vspace{-.3in}
\begin{align*}
\mathbb{E}[\lambda_1 | W'| &+ \Omega(\theta')| W, \theta, \vartheta, X] \le \lambda_1(1 - \delta_1)|W| + \lambda_1 t_{end} (1 + \eta_1)\Omega(\theta)\\
&+  (1 - \rho) \Omega(\theta) + (\sqrt{\eta_0} \sqrt{\epsilon} +  \eta_1 \epsilon + 2\epsilon) \Omega(\theta) + L_I\\
& = (1 - \delta_1)\lambda_1 |W| + [1 - (\rho - \lambda_1 t_{end} (1 + \eta_1) - (2 + \eta_1)\epsilon - \sqrt{\eta_0 \epsilon})]\Omega(\theta) + L_I\\
& \assign (1 - \delta_1)\lambda_1 |W| + (1 - \delta_2)\Omega(\theta) + L_I
\end{align*}
For $(\lambda_1,\epsilon)$ small enough, $\delta_2 \in (0,1)$, 
%If we 
%fix $\lambda_1$ small enough, this holds outside some small set $C$.
%Inside that compact set, define 
%$L = \sup_{(W, \theta)} \{ \mathbb{E}[\tilde{\lambda}_1 | W'| + 
%      \Omega(\theta')| W, \theta, \vartheta, X] \}$.
  %if we fix $\lambda_1$
  %There exists  $\tilde{\epsilon} > 0$ and $\tilde{\lambda}_1 >0 $ , such that $\rho >  \tilde{\lambda}_1 t_{end}  (1 + \eta_1) + (2 + \eta_1)\tilde{\epsilon} + \sqrt{\eta_0} \sqrt{\tilde{\epsilon}}$, and denote $\rho_2  \assign \rho - \tilde{\lambda}_1 t_{end}  (1 + \eta_1) + (2 + \eta_1)\tilde{\epsilon} + \sqrt{\eta_0} \sqrt{\tilde{\epsilon}}
  % > 0$. Let $\tilde{\rho} = \min(\rho_2, \delta_1)$. So there exists a small set $C$ such that \begin{align*}
  %\mathbb{E}[\tilde{\lambda}_1 | W'| + \Omega(\theta')| W, \theta, \vartheta, X] &\le (1 - \tilde{\rho}) [\tilde{\lambda}_1 | W| + \Omega(\theta)] +
  %\mathbb{I}_{C} \sup_{(W, \theta)} \{ \mathbb{E}[\tilde{\lambda}_1 | W'| + \Omega(\theta')| W, \theta, \vartheta, X] \}.
  %\end{align*}
      and  $\delta = \min(\delta_1,\delta_2)$ gives the drift condition.
% From equation~\eqref{eq:nu2} and assumption 2, we have for
% $\Omega(\theta) > k\Omega(\vartheta)$
% \begin{align*}
% \int \Omega(\theta')P(d\theta'| W, \theta, \vartheta)  &\leq
% \int \Omega(\theta') p_I(\theta'|\theta,X) d\theta' +\int d\theta'
% \Omega(\theta') \left[ \epsilon_{\theta'}q(\theta'|\theta)+
% \delta_\theta(\theta')\int \epsilon_{\ptheta} q(\ptheta|\theta) d\ptheta \right] \\
% %\epsilon_\ptheta[q(\ptheta|\theta)+\delta_\theta(\ptheta)] \\
% &\leq (1-\rho_I) \Omega(\theta)  + C_I  + \int d\theta'
% \Omega(\theta') \epsilon_{\theta'}q(\theta'|\theta)+
% \Omega(\theta)\int \epsilon_{\ptheta} q(\ptheta|\theta) d\ptheta
% %&\leq (1-\rho_I) \max_sA_s(\theta)  + C_I  + \epsilon_\theta \max_sA_s(\theta)
% % + \epsilon_\theta \max_s A_s(\theta)  + C_q
% \end{align*}
% We can bound the middle integral as follows
% \begin{align*}
%   \int d\theta'\Omega(\theta')  \epsilon_{\theta'} q(\theta'|\theta) &=
%   \int_{B_\alpha} d\theta'\Omega(\theta')  \epsilon_{\theta'} q(\theta'|\theta) +
%   \int_{B_\alpha^c} d\theta'\Omega(\theta')  \epsilon_{\theta'}
%   q(\theta'|\theta)  \\
%   &\le \alpha +
%   \int_{B_\alpha^c} d\theta'\Omega(\theta')  \epsilon_{\theta'} q(\theta'|\theta) \\
%   &\le \alpha + \epsilon_\theta \eta_0 \Omega(\theta)+1
% \end{align*}
%
% From these equations, we have for $\Omega(\theta) > k \Omega(\vartheta)$,
% \begin{align*}
%   \mathbb{E}[\lambda |W'| &+ \lambda_2 \Omega(\vartheta') + \Omega(\theta')|
%   W, \theta, \vartheta, X] \leq
%   \lambda \left[|W|(1 - \delta_1) +  \Omega(\theta) + b\right] + \\
%   & \lambda_2 (1+\eta_0)\Omega(\theta) +
%   \left[(1-\rho_I)  +\epsilon_\theta \eta_0 \right] \Omega(\theta) + C_I
% \end{align*}
% For $\Omega(\theta) < k \Omega(\theta^*)$,
% \begin{align*}
%   \mathbb{E}[\lambda |W'| &+ \lambda_2 \Omega(\vartheta') +
%   \Omega(\theta)| W, \theta, \vartheta, X] \leq
%   \lambda \left[|W|(1 - \delta_1) +  \Omega(\theta)| + b\right] + \\
% & \lambda_2(1+\eta_0) \Omega(\theta) +
%     \frac{(\eta_0 + 1)}{k} \Omega(\vartheta) + C_I \\
%     &= (1-\delta_1)\lambda|W| + \frac{(\eta_0 + 1)}{k\lambda_2} \lambda_2\Omega(\vartheta)+
%     (\lambda + \lambda_2(1+\eta_0))a\Omega(\theta)
% \end{align*}
%     \qed
\end{proof}
