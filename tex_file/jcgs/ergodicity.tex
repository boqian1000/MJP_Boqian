\newpage
\section{Geometric ergodicity}
We derive conditions under which our proposed algorithm
inherits mixing properties of an `ideal' sampler that operates without
computational constraints. The latter proposes a new parameter $\vartheta$
from distribution $q(\vartheta|\theta)$, and accepts with probability 
$\alpha_I(\vartheta,\theta) = 1 \wedge \frac{P(X | \vartheta)q(\theta| \vartheta)p(\vartheta)}
      {P(X | \theta)q(\vartheta|\theta)p(\theta)}$. The resulting ideal
Markov chain has transition probability 
$p_I(\theta'|\theta) = q(\theta'|\theta)\alpha(\theta',\theta) + \left[1-\int d\vartheta
q(\vartheta|\theta)\alpha_I(\vartheta,\theta)\right]\delta_\theta(\theta')$, the first
term corresponding to acceptance, and the second, rejection.

Our main result is Theorem~\ref{thm:geom_erg}, which shows that if the ideal MCMC 
sampler is geometrically ergodic, then so is our tractable auxiliary
variable sampler. Assumption~\ref{asmp:cond_num} is the only non-trivial 
one we have to make, requiring that the rate-matrix $A(\theta)$ not be 
arbitrarily ill-conditioned over all
settings of the parameter $\theta$. This assumption is reasonable from
both theoretical and practical viewpoints.

We first state all our assumptions, before diving into the details
of our proof.
\begin{assumption}
%  The rate-matrix $A(\theta)$ is ergodic for all $\theta$ and satisfies 
 % $\inf_\theta \frac{\mx{\theta}}{\Mx{\theta}} = \mu > 0$.
%  $\forall \theta$ satisfying 
%  $|\theta| > |\hat{\theta}|$, we have $\Mx{\theta} \asymp
%  \mx{\theta}$.
	Suppose the second smallest eigen value of rate-matrix  $A(\theta)$ is $\lambda ^A_2(\theta)$. Exists constants $\mu > 0$ and $C > 0$ such that 
	$\forall \theta > C$,  $ \lambda ^A_2(\theta) \geq \mu \Omega(\theta)$.
  \label{asmp:cond_num}
\end{assumption}
\noindent The ergodicty assumption is natural. The second part is the
strongest assumption needed to prove our result; nevertheless, it holds 
for all the examples we considered. This condition requires the rate 
matrix to be well-conditioned over all parameters of interest: the most 
unstable state cannot have a leaving rate that is larger than that of
the most stable state by an arbitrary factor. This condition controls the
mixing behavior of the MJP (and the embedded Markov chain) for any 
starting state. 
%We note that we only need this
%condition to hold in the tails of $\theta$, i.e.\ we only need
%  $\inf_\theta \frac{\mx{\theta}}{\Mx{\theta}} > 0$ for all $\theta$
%  satisfying $|\theta| > h$ for some $h$. For clarity, we restrict
%  ourselves to $h=0$, extending our proof to the general case is
%  straightforward.
%for any interval $\Delta$, we can always find a $\theta_0$ such that the 
%MJP has mixed at the end of the interval. The earlier assumption ensures
%this holds for all $\theta \ge \theta_0$.

\begin{assumption}
  $\exists$ $\hat{\theta}$ such that $\forall \theta$ satisfying 
  $|\theta| > |\hat{\theta}|$, we have $\Mx{\theta} \ge
  \Mx{\hat{\theta}}$.
  \label{asmp:mono_tail}
\end{assumption}
\noindent This assumption usually holds under suitable reparametrization,
and is not critical: we impose it to avoid book-keeping for 
situations where $\Mx{\theta}$ exhibits oscillations in the tails.

\begin{assumption}
For the ideal sampler with transition probability $p_I(\theta'|\theta)$: \\
i) the set $\SM_M=\{\theta:\Mx{\theta}\le M \}$ is a $1$-small set for 
every
$M$, i.e.\ $\exists$ a probability measure $\phi$ and a constant 
$\kappa_1 > 0$ s.t.\ % $q(\nu | \theta) \alpha_I(\theta, \nu) 
$p_I(\theta'|\theta) \geq \kappa_1 \phi(\theta')$ for $\theta \in \SM_M$, \\
ii) for $M$ large enough, $\exists \rho < 1$ s.\ t.\
$\int \Mx{\nu} p_I(\nu|\theta) d\nu 
\leq (1-\rho_I) \Mx{\theta}+C$, $\forall \theta \not\in \SM_M$.
  \label{asmp:ideal_geom}
\end{assumption}
\noindent Here we specify the properties of the ideal sampler that we
want our proposed sampler to inherit. The two conditions are standard 
small-set and drift conditions necessary for the ideal sampler to satisfy 
geometric ergodicity. The first implies that for $\theta$ in 
$\SM_M$, the ideal sampler forgets its current
location with probability $\kappa_1$. The second condition ensures that
for $\theta$ outside this set, the ideal sampler drifts towards 
$\SM_M$. These two conditions together imply geometric
mixing with rate equal or faster than $\kappa_1$~\cite{meyn2012markov}. 
Observe that we have used $\Mx{\theta}$ as the so-called Lyapunov-Foster 
function to define geometric ergodicity for the ideal sampler and whose
sub-level sets form the small sets. This is the most natural choice, 
though our proof can be tailored to different choices. Similarly, we 
could easily allow $\SM_M$ to be an $n$-small set for any $n\ge 1$ (so 
the ideal sampler needs $n$ steps before it can forget its current 
value in $\SM_M$); we restrict ourselves to the $1$-small case for 
clarity.


\begin{assumption}
%  $\inf_{\nu} \left(\Omega(\theta,\nu) - \Mx{\theta}\right) 
%  \ge m\Mx{\theta} \forall \theta$.
  There exist constants $m_1$ and $m_2$ such that 
  $\inf_\theta \Omega(\theta) = m_1 > 0$ and $\sup_{s,\theta}
  \frac{A_s(\theta)}{\Omega(\theta)} = m_2 < 1$. 
  \label{asmp:low_bnd}
\end{assumption}
\noindent 
Assumption~\ref{asmp:cond_num} allows us to easily ensure this is 
satisfied by setting $\Omega(\theta) = \Mx{\theta} + \mx{\theta}+m_1$. 
Another option is to set $\Omega(\theta) = \kappa\Mx{\theta} + m_1$ some 
$\kappa > 1$.  If $\inf_\theta \Mx{\theta} > 0$ (as is often the case), 
we can just set $\Omega(\theta) = \Mx{\theta} + \mx{\theta}$ so that 
$m_2 = \frac{1}{1+\mu}$. 


\begin{assumption}
$\exists$ $ \xi_1 > \eta_1 > 0$ s.t. 
$\prod P(x_o | s_o, \theta) \in [\eta_1, \xi_1]$.
  \label{asmp:obs_bnd}
\end{assumption}
\noindent This assumption follows~\cite{miasojedow2017}, and holds if 
%the observation process involves no hard constraints over the latent state, and 
$\theta$ does not include parameters of the observation process (or if so,
the likelihood is finite and nonzero for all settings of $\theta$). We can relax this assumption,
though it will introduce technicalities that are tangential to our focus
(which is on complications in parameter inference arising from the 
dynamics, rather than the observation process). Extensions to the more 
general case should be clear from our proof.

\begin{assumption}
Given the proposal density $q(\nu | \theta)$, $\exists \eta_0 > 0$ s.t. $$ \int_\Theta \max_s|A_{ss}(\nu)| q(\nu | \theta)d\nu \leq \eta_0 \max_s|A_{ss}(\theta)|.$$
\end{assumption}
\noindent This mild requirement ensures the proposal distribution doesn't attempt
to explore large $\theta$'s too aggressively.

\begin{assumption}
For any positive $\epsilon > 0$ and $K_0 > 1$, there exists $\theta_2$, such that for $\theta > \theta_2$, $P(\mid \frac{q(\theta | \theta')p(\theta')}{q(\theta | \theta')p(\theta)}\mid \leq K_0 | \theta) \geq 1 - \epsilon$, where $\theta' \sim q(\cdot | \theta)$.
\end{assumption}

\begin{assumption}
For any positive $\epsilon > 0$ and $K_1 > 1$, there exists $\theta_3$, such that for $\theta > \theta_3$, $P(\frac{1}{K_1}\frac{\Omega(\theta')}{\Omega(\theta)} \leq K_1 | \theta) \geq 1 - \epsilon$, where $\theta' \sim q(\cdot | \theta)$.
\end{assumption}
\begin{assumption}
For any positive $\epsilon > 0$ and $0 < K_2 < 1$, there exists $\theta_4$, such that for $\theta > \theta_4$, $P(\theta' > \theta K_2 | \theta) \geq 1 - \epsilon$, where $\theta' \sim q(\cdot | \theta)$.
\end{assumption}

%\begin{assumption}
%For the above $C \subseteq \Theta$, $\exists$ $\bar{\Omega} > 0$ s.t. 
%$\Omega(\theta)  \leq \bar{\Omega}$ for $\forall \theta \in C$.
%\end{assumption}

\begin{theorem}
Under the above assumptions, our auxiliary variable sampler is
geometrically ergodic.  \label{thm:geom_erg} 
\end{theorem}
\begin{proof}
\noindent This theorem follows from two lemmas we will prove, 
lemma~\ref{lem:small_set} showing the existence of a small set (within 
which our sampler forgets its 
current state with some probability $>0$), and lemma~\ref{lem:drift}
showing that our sampler drifts towards this set whenever 
outside. Together, these two results immediately imply geometric 
ergodicity~\citep[Theorems 15.0.1 and Lemma 15.2.8]{meyn2009markov}.
\end{proof}
We regard our sampler as operating on the space $(\theta,\vartheta,W)$, 
with a transition step updating this to $(\theta',\vartheta',W')$. 
We establish the drift condition via the Lyapunov-Foster function
$\ c V(W,\theta,\vartheta) = \left. \lambda_1|W| + \lambda_2 \Omega(\theta) +
\Omega(\vartheta) +L \right. := \c V_W(W) + \c V_{\theta}(\theta) + 
\c V_{\theta^*}(\theta^*)$, for suitable settings of $\lambda_1,
  \lambda_2$ and $L$.
The small set will be a level set of this function, i.e.\ 
$\S M_\alpha := \{(\theta,\theta^*,W): \c V(\theta,\vartheta,W) \le \alpha\}$.
We show that this is a 2-small set (i.e.\ the distribution over states
after 2-steps of our sampler, $P^2(\cdot|\theta,\vartheta,W) \ge \epsilon 
\nu(\cdot)$ for all $(\theta,\vartheta,W) \in \SM_\alpha$. 
The 2-step transition is necessary because of the exchange move we make, 
where on acceptance $\vartheta'=\theta$, while on rejection, $\theta'=\theta$.
We point out that if $\Mx{\theta}$ is bounded, then we can replace
the two $\Omega$ terms by their supremum and absorb them into the 
constant $L$, so that the Lyapunov function only involves $W$.

We start with a proposition bounding the self-transition probabilities
of the embedded Markov chain away from $0$. 
%\begin{proposition}
%The a priori probability the embedded Markov chain makes a self-transition,
%$P(V_{i + 1}=s|V_i=s,\theta,\theta^*)$ is uniformly bounded away
%from $0$ for all $s,\theta,\theta^*$.
%\end{proposition}
%\begin{proof}
%    & \ge 1-\frac{A_{s_0}(\theta)}{\Mx{\theta}} \ge 1 - \mu
% \intertext{For $\Omega(\theta) \ge k\Omega(\theta^*)$}
% P(V_{i + 1}=s|V_i=s,\theta,\theta^*) &= 
%   1 - \frac{A_{s_0}(\theta)}{\Omega(\theta) + \Omega(\theta^*)} \ge
%   1 - \frac{\Omega(\theta)}{m + \Omega(\theta^*)} 
% \intertext{For $\Omega(\theta) < k\Omega(\theta^*)$}
% P(V_{i + 1}=s|V_i=s,\theta,\theta^*) &\ge 
%   1 - \frac{A_{s_0}(\theta)}{\Omega(\theta) + \Omega(\theta)/k} 
%   \ge 1 - \frac{1}{1 + 1/k} 
%\end{align*}
%\end{proof}
\begin{proposition}
The a posteriori probability the embedded Markov chain makes a 
self-transition,
$P(V_i = V_{i + 1} | W, X, \theta, \vartheta) = \delta_1 > 0$,
for %$i = 0, 1, 2, ..., |W|$ and 
all $\theta,\vartheta$. 
\label{prop:self_tr}
\end{proposition}
\begin{proof} 
  We first use $m_2$ from assumption~\ref{asmp:low_bnd} 
  to bound a priori self-transition probabilities:
  \begin{align*}
  P(V_{i + 1}=s|V_i=s,\theta,\vartheta) &= 
    1 - \frac{A_{s}(\theta)}{\Omega(\theta, \vartheta)} 
    \ge 1 - \frac{A_{s}(\theta)}{\Omega(\theta)} \ge 1-m_2.
    \intertext{  We then have}
%\mathbb{E}[\mathbb{I}_{\{V_i = V_{i + 1}\}} | W, X, \theta, \theta^*] 
  P(V_i = V_{i + 1} | W, X, \theta, \vartheta) &= \sum_v P(V_i = V_{i + 1}
  = v | W, X, \theta, \vartheta)
 =\sum_v \frac{P(V_i = V_{i + 1} = v, X | W, \theta, \vartheta)}{P(X | W,
 \theta, \vartheta)} \\
&=\sum_v \frac{P(X | V_i = V_{i + 1} = v, W, \theta, \vartheta)P( V_i =
V_{i + 1} = v|W, \theta, \vartheta)}{P(X | W, \theta, \vartheta)}\\
& \geq \frac{\eta_1}{\xi_1}\sum_v P(V_i = V_{i + 1} = v | W)  
=  \frac{\eta_1}{\xi_1} \sum_v P(V_{i + 1} = v | V_i = v, W)P(V_i = v) \\
& \geq \frac{\eta_1 (1-m_2)}{\xi_1} \doteq \delta_1 > 0
\end{align*}
\end{proof}


\begin{lemma}
  Under our assumptions, for $\forall M,h > 0$, the set $B_{h,M} = 
\left\lbrace (W, \theta, \vartheta) | |W| \leq h, \theta \in \SM_M, \forall \vartheta 
\right\rbrace$ is a small set under our proposed sampler.
\label{lem:small_set}
\end{lemma}
\begin{proof} The $1$-step transition probability of our MCMC algorithm 
  consists of two terms, corresponding to the proposed parameter being 
  accepted and rejected. Discarding the latter, we have %the bound
\begin{align*}
  P(W',\theta',\vartheta'|W,\theta,\vartheta,X)&\geq
  \delta_\theta(\vartheta') q(\theta'|\theta)
\alpha(\theta', \theta, W',X) \sum_{S,T} P(S,T | W, \theta, \vartheta, X) 
P(W'| S, T, \theta', \theta)  
\end{align*}
Here we use the fact that the proposal distribution $q(\nu|\theta)$
and $P(W'|S,T,\theta',\theta,X))$ are independent of  $X$.
We further bound the summation over $(S,T)$ by considering only the term
with $S$ a constant. This corresponds to $|W|$ self-transitions, so that
\begin{align*}
P(S=s_0, T = \emptyset | W, \theta, \vartheta, X) & = 
p_0(s_0|X,W)\prod P(V_{i + 1} = s_0 | V_i = s_0,X,W,\theta,\vartheta) \\ %\prod P(X_{[w_i, w_{i + 1})} | v_i = s_0, \theta)\\
& \geq p_0(s_0|X)\delta_1^{|W|} \numberthis %\eta_1
\end{align*}
Given $T = \emptyset$ and $S = s_0$, $W'$ is a Poisson process with rate 
$r(\theta', \theta, s_0) = \Omega(\theta',\theta) - A_{s_0}(\theta)$.
%$> \epsilon_1 > 0$. 
From the fact that $\Omega(\theta',\theta) = \Omega(\theta') + \Omega(\theta)$,
we apply the superposition theorem to get 
%$\PP(r(\theta, \nu, s_0)) = \PP(\Omega(\nu)) \cup \PP(r(\theta, \nu, s_0) - \Omega(\nu))$.
\begin{align*}
P(W' | S, T = \emptyset, \theta', \theta) & \geq P(W' \from
\PP(\Omega(\theta')))
P(\emptyset \from \PP(\Omega(\theta)-A_{s_0}(\theta) ))\\
  & \geq P(W' \from \PP(\Omega(\theta'))) P(\emptyset \from \PP(\Omega(\theta) ))\\
& \geq P(W' \from \PP(\Omega(\theta'))) \exp(-\Omega(\theta)(t_{end} -
t_{start}))\\
& \geq P(W' \from \PP(\Omega(\theta'))) \exp(-M(t_{end}-t_{start}))
\numberthis %\quad (\text{since }\theta\in B_M)
\end{align*}
Thus we have
\begin{align*}
  \int_T \sum_S P(S,T | W, \theta, \vartheta, X) &P(W'| S, T, \theta',
  \theta)dT \geq \sum_S P(S, T = \emptyset | W, \theta, \vartheta, X) 
  P(W' | S, T=\emptyset,\theta', \theta)\\
               &\geq \delta_1^{|W|} \exp(-M(t_{end}-t_{start})) 
P(W' \from \PP(\Omega(\theta'))) \numberthis
\end{align*}
Finally consider the acceptance rate:
\begin{align*}
\alpha(\theta', \theta, W',X) &= 1 \wedge \frac{P(X | W', \theta', \theta)
q(\theta|\theta')p(\theta')}{P(X | W', \theta, \theta')q(\theta'|\theta)p(\theta)}\\
&= 1 \wedge \frac{P(X|W', \theta', \theta) / P(X|\theta')}{P(X|W', \theta,
\theta') / P(X|\theta)} \frac{P(X | \theta')
q(\theta|\theta')p(\theta')}{P(X | \theta)q(\theta'|\theta)p(\theta)}\\
& \geq 1 \wedge \frac{\eta_1^2}{\xi_1^2} 	\frac{P(X | \theta')
q(\theta|\theta')p(\theta')}{P(X | \theta)q(\theta'|\theta)p(\theta)}\\
& \geq \alpha_I(\theta', \theta,X)\frac{\eta_1^2}{\xi_1^2} \numberthis
\label{eq:acc}
\end{align*}
By assumption \ref{asmp:ideal_geom}, we have the following inequality.
\begin{align*}
  P(W', \theta',\vartheta' | W, \theta, \vartheta) & \geq \frac{\delta_1^{h}
\eta_1^2 \exp(-M(t_{end}-t_{start}))\kappa_1}{\xi_1^2} 
\delta_\theta(\vartheta')P(W' \from \PP(\Omega(\theta'))\phi(\theta') \\
  &:= \rho \delta_\theta(\vartheta')P(W' \from \PP(\Omega(\theta'))\phi(\theta') 
  \intertext{The two-step transition probability is given by}
  P(W'', \theta'',\vartheta'' | W, \theta, \vartheta) & =
  \int P(W'', \theta'',\vartheta'' | W', \theta', \vartheta') 
       P(W', \theta',\vartheta' | W, \theta, \vartheta) 
       dW' d\theta' d\vartheta' \\
       &\ge \int_{\SM_{h,M}} P(W'', \theta'',\vartheta'' | W', \theta', \vartheta') 
       P(W', \theta',\vartheta' | W, \theta, \vartheta) 
       dW' d\theta' d\vartheta' \\
       &\ge \int_{\SM_{h,M}}  \rho \delta_{\theta'}(\vartheta'')P(W'' \from \PP(\Omega(\theta''))\phi(\theta'') \\
         &\qquad \qquad \rho \delta_\theta(\vartheta')P(W' \from \PP(\Omega(\theta'))\phi(\theta') 
       dW' d\theta' d\vartheta' \\
       &\ge \phi(\theta'')P(W'' \from \PP(\Omega(\theta'')) 
       \int_{\SM_{h,M}} \!\!\!\! \delta_{\theta'}(\vartheta'') 
       F_{Poiss(\Omega(\theta'))}(h)\phi(\theta') 
       d\theta'  \\
       & \ge P(W'' \from
       \PP(\Omega(\theta''))\phi(\theta'')\phi(\vartheta'') 
\end{align*}
This is our desired result.
\end{proof}

\noindent Before proving the drift condition, we establish the proposition that
is needed when $\Mx{\theta}$ is unbounded as $\theta$ increases.
This result states that the acceptance probabilities of our proposed
sampler and the ideal sampler can be brought arbitrarily close
outside an appropriate small set, so long as $\Omega(\theta)$ is larger
than $\Omega(\vartheta)$ by a sufficient factor. 

\begin{lemma}
Given an observation observed at $t_{end}$, and $K_0 > 0$, $\theta$ and $\theta'$, with $\Omega(\theta') \leq K_0 \Omega(\theta)$ 
%$\parallel \theta'  - \theta \parallel \leq h$
, for $\forall \epsilon$, $\exists  W_0, \theta_0 > 0$ such that for all $W$ with $|W| > W_0$, and $\theta > \theta_0$ we have $|P(X| W, \theta, \theta') - P(X | \theta)| < \epsilon$
\label{lem:eigenvalue_lemma}
\end{lemma}
\begin{proof}
From our assumption 1, there exists $\mu$ such that $\lambda^A_2(\theta) \geq \mu \Omega(\theta)$. So $\frac{\lambda^A_2(\theta)}{\Omega(\theta) + \Omega(\theta')} \geq \mu / (1 + K_0) $ Since $B(\theta, \theta') = \frac{A(\theta)}{\Omega(\theta, \theta')} + I $, there is a one to one mapping between the eigen values of B and the eigen values of A. $\lambda_B(\theta, \theta') = 1 - \frac{\lambda_A(\theta)}{\Omega(\theta, \theta')}$. So the second largest eigen value of the transition probability matrix $B(\theta, \theta')$ is less or equal to a constant which is less than 1. Note that the constant is only related to $K_0$ and $\mu$. From the assumption 5, we have $\exists \ \xi_1 > \xi_1 > 0$, such that, $ \eta_1 \leq P(X | V=v, \theta) \leq \xi_1$. It leads to the following. Assume the state space has $N$ elements. For any $\epsilon > 0$, there exists $W_0$ for any $W$ with $|W| > W_0$, we have $\parallel P^{(|W|)}(\cdot | W, \theta, \theta') - p_{st}(\cdot | W, \theta, \theta')\parallel_{TV} \leq \frac{\epsilon}{4\xi_1N} $. Where $P^{(t)}(\cdot | W, \theta, \theta' )$
 is the t-step transition probability with respect to the Markov chain with transition matrix $B(\theta, \theta')$ corresponding to the determined time grid $W$, while $p_{st}(\cdot | \theta, \theta')$ is the stationary distribution accordingly.
\begin{align*}
P(X|W , \theta, \theta') &= \sum_V P(X | V, W, \theta, \theta') P(V | W, \theta, \theta')\\
&= \sum_{v} P(X | V_{|W|}=v, \theta) P^{(|W|)}(V_{|W|}=v | W, \theta, \theta')\\
\end{align*}
 We also define the observation stationary distribution as $$P_{st}(X | \theta) = \sum_{v} P(X | V=v, \theta) P_{st}(V=v | \theta, \theta').$$
 From the assumption 5, it is also bounded. $\eta_1 \leq P_{st}(X | \theta) \leq \xi_1$.
\begin{align*}
|P(X|W , \theta, \theta') - P_{st}(X | \theta)| &= | \sum_{v} P(X | V_{|W|}=v, \theta)(P^{(|W|)}(V_{|W|}=v | W, \theta, \theta') -  P_{st}(V=v | \theta, \theta'))|\\
& \leq |\sum_{v} P(X | V_{|W|}=v, \theta)|P^{(|W|)}(V_{|W|}=v | W, \theta, \theta') -  P_{st}(V=v | \theta, \theta')|\\
& \leq \sum_{v} P(X | V_{|W|}=v, \theta)\frac{\epsilon}{4\xi_1N}\\
& \leq \epsilon / 4
\end{align*}
So for $W$ with $|W| > W_0$, we have  $|P(X| W, \theta, \theta') - P_{st}(X | \theta)| < \epsilon / 4$. \\
Similarly, given the $W_0$ which is defined before, we have
\begin{align*}
P(X | \theta) &= \sum_W P(X , W | \theta, \theta')\\
&= \sum_{W s.t. |W| > W_0} P(X | W, \theta, \theta') P(W | \theta, \theta') \\
&+ \sum_{W s.t. |W| \leq W_0} \sum_{v} P(X | V_{|W|} = v, \theta) P^{(|W|)}(V_{|W|} = v | W, \theta, \theta') P(W|\theta, \theta')
%&= \sum_W \sum_{V_{|W|}} P(X | V_{|W|}, \theta) P^{(|W|)}(V_{|W|} | W, \theta, \theta') P(W|\theta, \theta')\\
%&= \sum_{W s.t. |W| \leq W_0'} \sum_{V_{|W|}} P(X | V_{|W|}, \theta) P^{(|W|)}(V_{|W|} | W, \theta, \theta') P(W|\theta, \theta') + \\
%&\sum_{W s.t. |W| > W_0'} \sum_{V_{|W|}} P(X | V_{|W|}, \theta) P^{(|W|)}(V_{|W|} | W, \theta, \theta') P(W|\theta, \theta')
\end{align*}
Consider the first term $\sum_{W s.t. |W| > W_0} P(X | W, \theta, \theta') P(W | \theta, \theta')$.\\
\begin{align*}
&|\sum_{W s.t. |W| > W_0} P(X | W, \theta, \theta') P(W | \theta, \theta')  - P_{st}(X | \theta)P(|W| > W_0 | \theta, \theta')|  \leq  \\
&\sum_{W s.t. |W| > W_0} |P(X | W, \theta, \theta') - P_{st}(X | \theta)|  P(W | \theta, \theta') \leq P(|W| > W_0 | \theta, \theta') \epsilon / 4 
\end{align*}
For the $W_0$, there exists $\theta_0 > 0$ such that for any $\theta > \theta_0$, $P(|W| \leq W_0 | \theta, \theta') < \frac{\epsilon}{4\xi_1} $.
For the second term, we have 
\begin{align*}
&|\sum_{W s.t. |W| \leq W_0} \sum_{v} P(X | V_{|W|} = v, \theta) P^{(|W|)}(V_{|W|} = v | W, \theta, \theta') P(W|\theta, \theta')|  \\
&\leq \xi_1 P(|W| \leq W_0 | \theta, \theta') \leq \epsilon / 4
\end{align*}
Consider the difference between $P(X|\theta)$ and $P_{st}(X|\theta)$ given $\theta > \theta_0$.
\begin{align*}
|P(X|\theta) - P_{st}(X|\theta)| &\leq |\sum_{W s.t. |W| > W_0} P(X | W, \theta, \theta') P(W | \theta, \theta')  - P_{st}(X | \theta)P(|W| > W_0 |\theta, \theta')| \\
&+ |\sum_{W s.t. |W| \leq W_0} \sum_{v} P(X | V_{|W|} = v, \theta) P^{(|W|)}(V_{|W|} = v | W, \theta, \theta') P(W|\theta, \theta')|\\
&+ |P_{st}(X | \theta)P(|W| \leq W_0 | \theta, \theta')|\\
&\leq \epsilon / 4 + \epsilon / 4 + \epsilon / 4 = 3\epsilon/4
\end{align*}
So for $\epsilon > 0$, and $K_0 > 0$, if $|W| > W_0$ and $\theta > \theta_0$ and $\Omega(\theta') \leq K_0 \Omega(\theta)$, we have 
\begin{align*}
|P(X | \theta) - P(X | W, \theta, \theta')| &\leq |P(X | \theta) -P_{st}(X | \theta) | + | P_{st}(X | \theta) -  P(X | W, \theta, \theta')|\\
& \leq \epsilon
\end{align*}

\end{proof}

\begin{proposition}
Write $P(W', \theta' | W, \theta, \vartheta)$ for the transition kernel of our MCMC sampler given the observations. For any positive $\epsilon$, there exist $\theta_1 > 0$ such that for all $\theta \ge \theta_1$, we have
  $P(\{(W', \theta') \in \E_\epsilon \} | W, \theta, \vartheta) \ge 1-\epsilon$.  $| \alpha_I(\theta,\vartheta,X) - \alpha(\theta,\vartheta,W',X)| 
  \le \epsilon | $ for any $(W', \theta') \in E_\epsilon$.
  %$(\theta,\theta^*)$ satisfying
\label{prop:mix0}
\end{proposition}
\begin{proof}
From previous lemma, for any $\epsilon > 0$ and a determined $K_0 > 1$, given $\frac{1}{K_0}  \leq \Omega(\theta) \leq \Omega(\theta') \leq K_0 \Omega(\theta)$, $\exists \theta_0 > 0, W_0 > 0$, such that for any $|W| > W_0$ and $\theta > \theta_0$ and $\theta' > \theta_0$, we have $|\frac{P(X | W, \theta' , \theta)}{P(X | W, \theta , \theta')} - \frac{P(X | \theta')}{P(X | \theta)}| < \epsilon / 2$. 
\begin{align*}
|\alpha(\theta, \theta', W, X) - \alpha_I(\theta, \theta', X)| &= \ \mid 1 \wedge \frac{P(X | W, \theta' , \theta)q(\theta | \theta')p(\theta')}{P(X | W, \theta , \theta')q(\theta' | \theta)p(\theta)} - 1 \wedge \frac{P(X | \theta')q(\theta | \theta')p(\theta')}{P(X | \theta)q(\theta' | \theta)p(\theta)} \mid \\
& \leq \ \mid \frac{P(X | W, \theta' , \theta)}{P(X | W, \theta , \theta')} - \frac{P(X | \theta')}{P(X | \theta)} \mid \frac{q(\theta | \theta')p(\theta')}{q(\theta' | \theta)p(\theta)}\\
\end{align*}
From assumption 7 and assumption 8, we know that the second term can be bounded with high probability. Also, given large $\theta$ and $\theta'$ which is close to $\theta$, the probability of $|W|$ being large is large. \\
Given the current state $(W, \theta)$, from assumption 7, there exists $\theta_1 > 0$, such that for $\theta > \theta_1$, $$P(\frac{q(\theta | \theta')p(\theta')}{q(\theta' | \theta)p(\theta)} \leq 2 | \theta) > 1 - \epsilon / 4.$$\\

Consider the $K_0 $ in previous paragraph. From assumption 8, there exists $\theta_2 > 0$, such that for $\theta > \theta_2$, $$P(\frac{1}{K_0}  \leq \frac{\Omega(\theta')}{\Omega(\theta)} \leq K_0 | \theta) > 1 - \epsilon / 4 .$$\\

Consider the $\theta_0$, and the assumption 9, there exists $\theta_3 > 0$, such that for $\theta > \theta_3$, 
$$P(\theta' > \theta_0 | \theta) > 1 - \epsilon / 4.$$

For the $W_0$, there exists $\theta_4 > 0$, such that for $\theta > \theta_4$, $$P(|W| > W_0 | W, \theta, \theta', \vartheta) > 1- \epsilon / 4.$$

%So given the current state $(W, \theta)$, and $K_0$, $P(\{ \frac{1}{K_0}\leq \frac{\Omega(\theta')}{\Omega(\theta)} \leq K_0 \} \cap \{ \frac{q(\theta | \theta')p(\theta')}{q(\theta' | \theta)p(\theta)} \leq 2\}| \theta ) > 1 - 2\epsilon / 4 $, for any $\theta > \max(\theta_2, \theta_3)$.\\

Set $\theta_5 = \max(\theta_0, \theta_1,\theta_2, \theta_3, \theta_4)$, for $\theta > \theta_5$, we have 
\begin{align*}
\mid \frac{P(X | W, \theta' , \theta)}{P(X | W, \theta , \theta')} - \frac{P(X | \theta')}{P(X | \theta)} \mid \frac{q(\theta | \theta')p(\theta')}{q(\theta' | \theta)p(\theta)} < \epsilon
\end{align*}
with probability greater than $1 - \epsilon$. Also, the set 

%For $K = 1$ and $\epsilon$, there exists $\theta_1$ such that for $\theta > \theta_1$, $\mid \frac{q(\theta | \theta')p(\theta')}{q(\theta' | \theta)p(\theta)}\mid \leq 1$ with probability close to 1. Given $\theta$ and $\theta'$, $P(|W'| > W_0 | \theta, \theta')$ is close to 1.

\end{proof}
\begin{proposition}
  Write $P_{\theta}(W',\theta^*)$ for the conditional distribution 
  over $(W',\theta^*)$ given the current state of the MCMC sampler and the 
  observations, and let
  $\lim_{\theta\rightarrow\infty} \Mx{\theta} = \infty$. Then for any positive 
  $\epsilon$, there exist $B_\theta$ such that for all 
  $\theta \ge B_\theta$, % with $\Omega(\theta) \ge k\Omega(\vartheta)$, 
  we have 
  $P_{\theta}(\{W'\ s.t.\ |\alpha_I(\theta,\vartheta,X) - \alpha(\theta,\vartheta,W',X)| 
  \le \epsilon)\} \ge 1-\epsilon$.%$(\theta,\theta^*)$ satisfying
\label{prop:mix}
\end{proposition}
\begin{proof}
Consider the nearest pair of observations, occuring at times $t$ and 
$t + \Delta$. %let them be separated by a time interval $\Delta$. 
By setting $\theta$ high enough, assumption~\ref{asmp:cond_num} ensures 
that the distribution over waiting times of each state can be concentrated 
arbitrarily close to $0$. Thus, the distribution over states after an 
interval $\Delta$ can be brought arbitrarily close to the equilibrium 
distribution of $A(\theta)$  (call this $p_{\theta}$).

Next, recall that the embedded Markov chain has a transition matrix 
given by $B(\theta^*,\theta) = (I + A(\theta^*)/\Omega(\theta,\theta^*))$. 
For any setting of $\Omega$, this has the same stationary distribution 
$p_{\theta}$: this can be verified by multiplying $B(\theta,\theta^*)$ with 
$p_\theta$. For the embedded Markov chain to be brought close to 
equilibrium over $[t,t+\Delta]$, we need two conditions, 1) the number of 
Poisson events must be large enough (to allow sufficient transitions), and 2) the 
transition matrix $B$ must mix well enough (otherwise even a 
large number of transition opportunities will not forget the initial state). 
We show that for $\theta$ large enough, this holds with high probability.
Assumption~\ref{asmp:mono_tail} will ensure that this continues to hold 
for all larger $\theta$ as well.

Recall first that the proposal distribution $q(\theta^*|\theta)$ is 
centered at the current value $\theta$. For any $\epsilon$, we can find 
an interval $[\theta-h,\theta+h]$ such that for all $\theta$, 
$q(\theta^* \in [\theta-h,\theta+h]|\theta)
\ge 1-\epsilon$. By choosing $\theta$ large 
enough, we can ensure that %$\Omega(\theta) \approx \Omega(\theta^*)$ (i.e.\ 
$\frac{\Omega(\theta)}{ \Omega(\theta^*)} \in [1-\epsilon,1+\epsilon]$) 
with probability greater than $1-\epsilon$.
%The condition $\Omega(\theta) \ge k\Omega(\vartheta)$, 
This, together with assumption~\ref{asmp:cond_num}, ensures that with 
probability greater than $1-\epsilon$, the self-transition probability of $B(\theta,\vartheta)$ 
is bounded away from one, and limits how poorly $B$ can mix.
Second, since $W$ comes from a Poisson process with intensity 
$\Omega(\theta')+ \Omega(\theta) - A_{S(t)}(\theta)$, a large $\theta'$
ensures a large $|W'|$ with high probability: by setting $B_\theta$ large 
enough we can ensure $|W'|$ is large enough with arbitrary probability.

Thus, for large enough $B_\theta$ we can ensure that for all $\theta > 
B_\theta$, the distributions $p(x_{t+\Delta}|s_t)$ and 
$p(x_{t+\Delta}|W,s_t)$ can be brought arbitrarily close with arbitrarily 
high probability.  
By a simple chaining argument, this holds for $p(X|\theta)$ and 
$p(X|\theta,W)$ as well and so too 
  \vinayak{expand}
$\alpha_I(\theta',\theta,X)$ and $\alpha(\theta',\theta,W,X)$
\end{proof}
\begin{lemma}
Write $P(W' | W, \theta, \vartheta, \theta')$ as the transition density with respect to the grids $W'$, there exists $\psi > 1$, such that $P(W' | W, \theta, \vartheta, \theta') \leq \psi ^{|W'|} \tilde{\Omega}(\theta,\theta')^{|W'|}\exp(-\tilde{\Omega}(\theta,\theta')\tau)$, where $\tilde{\Omega}(\theta, \theta') = (\Omega(\theta) + \Omega(\theta')) / \psi$ and $\tau$ is the length of the time interval. 
\end{lemma}
\begin{proof}
As we defined,  there exists $k > 1$, such that $\Omega(\theta) = k \max\{ A_s(\theta)\}$. Recall that given the current state $(W, \theta, \vartheta)$, we first sample $(S, T)$, which is a discrete time Markov chain with rate matrix $B(\theta, \vartheta) = I + \frac{A(\theta)}{\Omega(\theta) + \Omega(\vartheta)}$. Then we sample $W'$ which is a Poisson process with rate $\Omega(\theta) + \Omega(\theta') - A_{S_t}(\theta)$. Integrating out $S, T$, we can get the transition density with respect to the grids $W'$. We have 
\begin{align*}
P(W' | W, \theta, \vartheta, \theta') &= \sum_{S,T} P(S, T | W, \theta, \vartheta) P(W' | S, T, \theta, \theta') .
\end{align*}
Given any $S, T$, the Poisson rate has a lower bound $\frac{k - 1}{k} (\Omega(\theta) + \Omega(\theta'))$. So 
\begin{align*}
P(W' | S, T, \theta, \theta') &= \prod_{i = 0}^{|T|} (\Omega(\theta) + \Omega(\theta') - A_{S_i}(\theta))^{|W_i'|} \exp(-(\Omega(\theta) + \Omega(\theta') - A_{S_i}(\theta))(T_{i + 1} - T_i))\\
& \leq \prod_{i = 0}^{|T|} (\frac{k - 1}{k}(\Omega(\theta) + \Omega(\theta')))^{|W_i'|} \exp(- \frac{k - 1}{k} (\Omega(\theta) + \Omega(\theta'))(T_{i + 1} - T_i))(\frac{k}{k - 1})^{|W'|} \\
&\leq (\frac{k - 1}{k}(\Omega(\theta) + \Omega(\theta')))^{|W'|} \exp(- \frac{k - 1}{k} (\Omega(\theta) + \Omega(\theta'))(T_{|T| + 1} - T_0))(\frac{k}{k - 1})^{|W'|}
\end{align*}
So we can set $\psi = \frac{k}{k - 1}$ and $\tilde{\Omega}(\theta, \theta') = (\Omega(\theta) + \Omega(\theta')) / \psi$, and then sum up $S, T$.
\begin{align*}
P(W' | W, \theta, \vartheta, \theta') &= \sum_{S,T} P(S, T | W, \theta, \vartheta) P(W' | S, T, \theta, \theta') \\
& \leq  \psi ^{|W'|} \tilde{\Omega}(\theta,\theta')^{|W'|}\exp(-\tilde{\Omega}(\theta,\theta')\tau)
\end{align*}
\end{proof}

\begin{lemma}(drift condition) $\exists \delta_2 \in (0, 1), L > 0$ 
  s.t. 
  $\mathbb{E}\left[\lambda_1|W'| + \Omega(\theta')  | W, \theta, \vartheta, X\right] 
  \leq (1 - \delta_2)\left(\lambda_1|W| + \Omega(\theta)   \right) + L$ %where $\lambda = \lceil \frac{(t_{end} - t_{start})k_2(\eta_0 + 1)}{(\eta_1^2 \kappa_1 \mathbb{P}_\phi(C)/\xi_1^2 - \eta_0)} \rceil.$
\label{lem:drift}
\end{lemma}
\begin{proof}
Since $W'=T\cup U'$, we consider $\mathbb{E}[|T| |W,\theta,\vartheta,X]$ 
and $\mathbb{E}[|U'| | W, \theta, \vartheta, X]$ separately.
An upper bound of $\mathbb{E}[|T| | W,\theta,\vartheta]$ can be derived
directly from proposition~\ref{prop:self_tr}:
\begin{align*}
\mathbb{E}[|T| |W,\theta,\vartheta,X] &= \mathbb{E}[\sum_{i = 0}^{|W|-1} 
  \mathbb{I}_{\{ V_{i + 1} \neq V_i \}}| W, \theta, \vartheta, X]\\
&\leq \sum_{i = 0}^{|W| - 1} (1 - \delta_1) = |W|(1 - \delta_1).
\end{align*}
\begin{align*}
\mathbb{E}[|U'| |W, \theta, \vartheta, X] &= \mathbb{E}_{S,T, \nu}\mathbb{E}[|U'| | S, T, W, \theta, \nu, X] = \mathbb{E}_{S,T, \nu}\mathbb{E}[|U'| | S, T, W, \theta, \nu] \\
& \leq \mathbb{E}_{S,T, \nu} \left[(t_{end} - t_{start})\Omega(\theta, \nu)\right] = (t_{end} - t_{start})\int \Omega(\theta, \nu) q(\nu | \theta) d\nu\\
& \leq (t_{end} - t_{start})\left[ \left(  \Omega(\theta) +
\int_\Theta \Omega(\nu) q(\nu | \theta)d\nu \right) \right] \\
& \leq (t_{end} - t_{start}) \left[ (\eta_0 + 1) \Omega(\theta) \right] 
\doteq a \Omega(\theta) + b.
\end{align*}

Next, we note that $\vartheta'$ takes on value $\theta$ with 
probability of acceptance, else it takes the value $\nu$ proposed  from
$q(\nu|\theta)$. We bound the acceptance
probability by $1$, so that
\begin{align}
\mathbb{E}[\Omega(\vartheta')|\theta,\vartheta,W,X)] &\le \Omega(\theta)
+ \int d\nu (1-\alpha(\nu,\theta)) q(\nu|\theta) \Omega(\nu)\nonumber \\
  & \le (1+\eta_0) \Omega(\theta)
\end{align}

Consider the transition probability over $(W',\theta')$:
\begin{align*}
  P(dW', d\theta'&| W, \theta, \vartheta) 
=d\theta' dW' \left[q(\theta' | \theta) 
  \sum_{S,T} P(S, T | W, \theta, \vartheta, X)P(W' | S, T, \theta, \theta')
\alpha(\theta, \theta' | W', X)\right. \\
&\left.+ \int q(\nu | \theta) \sum_{S,T} P(S, T|W,\theta,\vartheta,
    X)P(W' | S, T, \theta, \nu) ( 1 - {\alpha(\theta, \nu | W', X)})d\nu
    \delta_\theta(\theta')\right].
\end{align*}
Integrate out $W'$, then we get the following.
\begin{align*}
  P(d\theta'| W, \theta, \vartheta) =d\theta' \int_{W'}dW'&
  \left[q(\theta' | \theta)
    \sum_{S,T} P(S, T | W, \theta, \vartheta, X)P(W' | S, T, \theta,
  \theta')\alpha(\theta, \theta' | W', X)\right.\\
  &\hspace{-1.3in}\left.  + \int q(\nu | \theta) \sum_{S,T} P(S, T |  W, \theta, \vartheta,
X)P(W' | S, T, \theta, \nu) ( 1 - \alpha(\theta, \nu | W', X))d\nu
\delta_\theta(\theta')\right] \\
&:= d\theta' I_1 + \delta_\theta(\theta')I_2
\end{align*}
{Consider the second term $I_2$, involving an integral over $(\nu,W')$. 
 For any positive $\epsilon$, there exists $\theta_0 > 0$ and $W_0 > 0$, such that for $\theta > \theta_0$, we have $P(\theta' > \theta_0, |W'| > W_0 |\theta, \vartheta, W) > 1 - \epsilon$. Defining $\E_{\epsilon} = \{ (\theta', W') | \theta' > \theta_0, |W'| > W_0\}$, we can divide the area of integration into two complementary parts: $\E_{\epsilon}$, the part of $(\nu,W')$ where 
  $|\alpha(\theta, \nu | X,W') - \alpha_I(\theta, \nu | X)| \le \epsilon$,
and its complement $\E^c_{\epsilon}$. Call these $I_{2,\E_{\epsilon}}$ and 
$I_{2,\E_{\epsilon}^c}$ respectively. 
For $\theta > \theta_0$, the second term has probability less than $\epsilon$, and using the bound 
$1-\alpha \le 1$, we have}
%use $\alpha(\theta, \theta' | W', X)\le 1$ and $q(\theta'|\theta) < \epsilon$ 
%to get}
%%and equation~\eqref{eq:acc} to get}
%  P(d\theta'| &W, \theta, \vartheta) 
%\leq d\theta' \epsilon
% &\left.\left(1 -  \int q(\nu | \theta) \sum_S P(S, T|W, \theta,
% \vartheta, X)P(W'|S, T, \theta, \nu)\alpha_I(\theta, \nu)
% \frac{\eta_1^2}{\xi_1^2}d\nu dW' dT\right) \delta_\theta(\theta')\right]\\
%          & \leq d\nu \left[q(\nu | \theta) + \left(1 -
%          \frac{\eta_1^2}{\xi_1^2} \int_\Theta q(\nu |
%      \theta)\alpha_I(\theta, \nu) d\nu\right)\delta_\theta(\theta')\right]
%      \numberthis \label{eq:nu1}
\begin{align*}
  I_{2,\E_{\epsilon}^c} &\le \int_{\E^c_{\epsilon}} d\nu dW'
q(\nu | \theta) \sum_{S,T} P(S, T |  W, \theta, \vartheta,
X)P(W' | S, T, \theta, \nu) \\ 
& = \int_{\E^c_{\epsilon}} d\nu dW'
q(\nu | \theta) P(W' |  W, \theta, \nu, \vartheta, X) 
\le \epsilon 
\end{align*}
{For the former, we use Proposition~\ref{prop:mix} to get the bound}
%, and bounding
%$\alpha$ by one on the $W$-set with probability $\epsilon$, we have the 
%bound}
\begin{align*}
%\int_{W_1}dW' \int_S q(\nu | \theta) &\int \sum_S P(S, T | W, \theta, \vartheta,
%X)P(W' | S, T, \theta, \nu)dT ( 1 - [\alpha_I(\theta, \nu |
%X,W')])d\nu \\ 
I_{2,\E_{\epsilon}} \le \int_{\E_\epsilon}dW' d\nu q(\nu | \theta) &
  \sum_{S,T} P(S, T | W, \theta, \vartheta, X)P(W' | S, T, \theta, \nu) 
  [ 1 - (\alpha_I(\theta, \nu | X)-\epsilon)] \\
\le \int dW' d\nu q(\nu | \theta) &
  P(W' | W, \theta, \vartheta, \nu, X) 
  [ 1 - (\alpha_I(\theta, \nu | X)-\epsilon)] \\
  \le (1+\epsilon) & - \int  q(\nu | \theta) \alpha_I(\theta, \nu | X) d\nu
%   \intertext{Then}
%      P(d\theta'| &W, \theta, \vartheta)   \le d\theta' \int_{W'}dW'
%      \left[q(\theta' | \theta) \int \sum_S P(S, T
%   | W, \theta, \vartheta, X)P(W' | S, T, \theta, \theta')dT
%   \left[\alpha_I(\theta', \theta | X)+\epsilon_{}\right]\right.\\
%   &\left. + \int q(\nu | \theta) \int \sum_S P(S, T | W, \theta, \vartheta,
%   X)P(W' | S, T, \theta, \nu)dT ( 1 - [\alpha_I(\theta, \nu |
%   X)-\epsilon_\nu])d\nu \delta_\theta(\theta')\right]  \\
%   & \leq d\theta'\left[q(\theta' | \theta)\left[\alpha_I(\theta', \theta |
%   X)+\epsilon_{\theta'}\right] +
%   \left(1 -  \int q(\nu | \theta) [\alpha_I(\nu, \theta|X)-\epsilon_\nu]
%   d\nu \right) \delta_\theta(\theta')\right]\\
%   & \leq d\theta'\left[q(\theta' | \theta) \alpha_I(\theta', \theta | X) +
%   \left(1 -  \int q(\nu | \theta) \alpha_I(\theta, \nu|X) d\nu \right)
%   \delta_\theta(\theta')\right]+\epsilon_{\theta'}q(\theta'|\theta)+
%   \delta_\theta(\theta')\int \epsilon_{\nu} q(\nu|\theta) d\nu \\
%   & = p_I(\theta'|\theta,X) + \epsilon_{\theta'}q(\theta'|\theta)+
%   \delta_\theta(\theta')\int \epsilon_{\nu} q(\nu|\theta) d\nu
%   \numberthis \label{eq:nu2}
\end{align*}

Finally, we have
\begin{align*}
  \int \Omega(\theta') P(d\theta'| W, \theta, \vartheta)  
  &= \int d\theta' \Omega(\theta') I_1 + I_2 \Omega(\theta)
\end{align*}
Again, the first term involves an integral over $(\theta',W')$ which we 
divide into two regions $\tilde{I}_{1,\E_\epsilon}$ and its complement 
$\tilde{I}_{1,\E^c_\epsilon}$. From lemma 8, we have 
$P(W' | W, \theta, \vartheta, \theta') \le \psi ^{|W'|} \tilde{\Omega}(\theta,\theta')^{|W'|}\exp(-\tilde{\Omega}(\theta,\theta')\tau)$, where $\tilde{\Omega} = (\Omega(\theta) + \Omega(\theta')) / \psi$ and $0 < \psi < 1$ and $\tau$ is the length of the time interval.\\
For the latter, we bound the acceptance probability 
by one to get
\begin{align*}
  \tilde{I}_{1,\E^c_\epsilon} &\le  \int_{\E^c_\epsilon}  \Omega(\theta') q(\theta' | \theta)P(W' | W, \theta, \vartheta, \theta')d\theta'dW'\\
  &=  \int_{\theta' \leq \theta_0}  \Omega(\theta') q(\theta' | \theta)P(W' | W, \theta, \vartheta, \theta')d\theta'dW' +  \int_{\theta' > \theta_0, |W'| < W_0}  \Omega(\theta') q(\theta' | \theta)P(W' | W, \theta, \vartheta, \theta')d\theta'dW'\\
  & \leq  \Omega(\theta_0) \int_{\theta' > \theta_0}q(\theta' | \theta) d\theta' + \int_{\theta' > \theta_0} \Omega(\theta') q(\theta' | \theta) [ \sum_{w \le W_0}\frac{\tilde{\Omega}(\theta,\theta')^{w}}{w!}\exp(-\tilde{\Omega}(\theta,\theta')\tau)]d\theta'\\
%  & \leq  \Omega(\theta) \epsilon + \int_{\Theta} \Omega(\theta') q(\theta' | \theta)\epsilon d\theta'\\
%  &= \Omega(\theta) \epsilon (1 + \eta_0)
%  \boqian{?}
  \end{align*}
  For $\epsilon > 0$, there exists $\theta_1 > 0$, such that for $\theta > \theta_1$ $\int_{\theta' > \theta_0}q(\theta' | \theta) d\theta' \le \epsilon$. There also exists $\theta_2 > 0$, such that for $\theta > \theta_2$ and any $\theta'$,  $\sum_{w \le W_0}\frac{\tilde{\Omega}(\theta,\theta')^{w}}{w!}\exp(-\tilde{\Omega}(\theta,\theta')\tau) < \epsilon$. For $\theta > \max(\theta_0, \theta_1, \theta_2)$, we have
\begin{align*}
\tilde{I}_{1,\E^c_\epsilon} &\le \Omega(\theta_0) \int_{\theta' > \theta_0}q(\theta' | \theta) d\theta' + \int_{\theta' > \theta_0} \Omega(\theta') q(\theta' | \theta) [ \sum_{w \le W_0}\frac{\tilde{\Omega}(\theta,\theta')^{w}}{w!}\exp(-\tilde{\Omega}(\theta,\theta')\tau)]d\theta'\\
& \le  \Omega(\theta_0) \epsilon + \int_{\Theta} \Omega(\theta') q(\theta' | \theta)\epsilon d\theta' \\
& \le \Omega(\theta_0) \epsilon + \eta_0 \Omega(\theta)\epsilon \\
& \le (1 + \eta_0) \Omega(\theta)\epsilon
\end{align*}

Then for the first one, we have 
\begin{align*}
\tilde{I}_{1,\E_\epsilon} &= \int_{S_\epsilon} I_1 \Omega(\theta') d\theta'\\
& \leq \int_{\E_\epsilon} \Omega(\theta')q(\theta' | \theta) (\alpha_I(\theta, \theta', X) + \epsilon) d\theta' \\
& \leq \int q(\theta' | \theta) \Omega(\theta')\alpha_I(\theta, \theta', X) d\theta' + \eta_0 \epsilon \Omega(\theta)
\end{align*}

Then for $\theta > \max(\theta_0, \theta_1, \theta_2)\doteq \theta_\epsilon$, we have the following. 
\begin{align*}
  \int \Omega(\theta') P(d\theta'| W, \theta, \vartheta)  
  &= \tilde{I}_{1,\E_\epsilon} + \tilde{I}_{1,\E^c_\epsilon} + I_{2,\E_{\epsilon}} + I_{2,\E_{\epsilon}^c} \\
  & \leq \int q(\theta' | \theta) \Omega(\theta')\alpha_I(\theta, \theta', X) d\theta'  + \int d\nu q(\nu | \theta) (1 - \alpha_I(\theta, \nu | X)) + (3 + 2\eta_0) \Omega(\theta) \epsilon\\
  & \leq (1 - \rho) \Omega(\theta) + (3 + 2\eta_0) \Omega(\theta) \epsilon
\end{align*}
For $\theta > \theta_\epsilon$, we have
\begin{align*}
\mathbb{E}[\lambda_1 | W'| + \Omega(\theta')| W, \theta, \vartheta, X] & \le \lambda_1(1 - \delta_1)|W| + \lambda_1 \tau (1 + \eta_0)\Omega(\theta) +  (1 - \rho) \Omega(\theta) + (3 + 2\eta_0) \Omega(\theta) \epsilon\\
& = (1 - \delta_1)\lambda_1 |W| + [1 - (\rho - \lambda_1 \tau (1 + \eta_0) - (3 + 2\eta_0)\epsilon)]\Omega(\theta) 
\end{align*}
There exists  $\tilde{\epsilon}_0 > 0$ and $\tilde{\lambda}_1 >0 $ , such that $\rho > \tilde{\lambda}_1 \tau (1 + \eta_0) + (3 + 2\eta_0)\tilde{\epsilon}_0$, and set $\rho_2  \doteq \rho - \tilde{\lambda}_1 \tau (1 + \eta_0) - (3 + 2\eta_0)\tilde{\epsilon}_0$. Let $\tilde{\rho} = \min(\rho_2, \delta_1)$ So there exists a small set $C \doteq \{(W, \theta) | |W| \leq h_0, \theta_{\tilde{\epsilon_0}} \}$, where $h_0$ is a positive constant.we have \begin{align*}
\mathbb{E}[\tilde{\lambda}_1 | W'| + \Omega(\theta')| W, \theta, \vartheta, X] &\le (1 - \tilde{\rho}) [\tilde{\lambda}_1 | W| + \Omega(\theta)] + 
\mathbb{I}_{C} \sup_{(W, \theta)} \{ \mathbb{E}[\tilde{\lambda}_1 | W'| + \Omega(\theta')| W, \theta, \vartheta, X] \}.  
\end{align*}
So the drift condition holds.
% From equation~\eqref{eq:nu2} and assumption 2, we have for 
% $\Omega(\theta) > k\Omega(\vartheta)$
% \begin{align*}
% \int \Omega(\theta')P(d\theta'| W, \theta, \vartheta)  &\leq 
% \int \Omega(\theta') p_I(\theta'|\theta,X) d\theta' +\int d\theta'
% \Omega(\theta') \left[ \epsilon_{\theta'}q(\theta'|\theta)+
% \delta_\theta(\theta')\int \epsilon_{\nu} q(\nu|\theta) d\nu \right] \\
% %\epsilon_\nu[q(\nu|\theta)+\delta_\theta(\nu)] \\ 
% &\leq (1-\rho_I) \Omega(\theta)  + C_I  + \int d\theta'
% \Omega(\theta') \epsilon_{\theta'}q(\theta'|\theta)+
% \Omega(\theta)\int \epsilon_{\nu} q(\nu|\theta) d\nu 
% %&\leq (1-\rho_I) \max_sA_s(\theta)  + C_I  + \epsilon_\theta \max_sA_s(\theta)
% % + \epsilon_\theta \max_s A_s(\theta)  + C_q  
% \end{align*}
% We can bound the middle integral as follows
% \begin{align*}
%   \int d\theta'\Omega(\theta')  \epsilon_{\theta'} q(\theta'|\theta) &=  
%   \int_{B_\alpha} d\theta'\Omega(\theta')  \epsilon_{\theta'} q(\theta'|\theta) +  
%   \int_{B_\alpha^c} d\theta'\Omega(\theta')  \epsilon_{\theta'}
%   q(\theta'|\theta)  \\
%   &\le \alpha +  
%   \int_{B_\alpha^c} d\theta'\Omega(\theta')  \epsilon_{\theta'} q(\theta'|\theta) \\
%   &\le \alpha + \epsilon_\theta \eta_0 \Omega(\theta)+1
% \end{align*}

% From these equations, we have for $\Omega(\theta) > k \Omega(\vartheta)$,
% \begin{align*}
%   \mathbb{E}[\lambda |W'| &+ \lambda_2 \Omega(\vartheta') + \Omega(\theta')| 
%   W, \theta, \vartheta, X] \leq 
%   \lambda \left[|W|(1 - \delta_1) +  \Omega(\theta) + b\right] + \\
%   & \lambda_2 (1+\eta_0)\Omega(\theta) +
%   \left[(1-\rho_I)  +\epsilon_\theta \eta_0 \right] \Omega(\theta) + C_I 
% \end{align*}
% For $\Omega(\theta) < k \Omega(\theta^*)$,
% \begin{align*}
%   \mathbb{E}[\lambda |W'| &+ \lambda_2 \Omega(\vartheta') +
%   \Omega(\theta)| W, \theta, \vartheta, X] \leq 
%   \lambda \left[|W|(1 - \delta_1) +  \Omega(\theta)| + b\right] + \\
% & \lambda_2(1+\eta_0) \Omega(\theta) +
%     \frac{(\eta_0 + 1)}{k} \Omega(\vartheta) + C_I \\
%     &= (1-\delta_1)\lambda|W| + \frac{(\eta_0 + 1)}{k\lambda_2} \lambda_2\Omega(\vartheta)+
%     (\lambda + \lambda_2(1+\eta_0))a\Omega(\theta)
% \end{align*}
\end{proof}
