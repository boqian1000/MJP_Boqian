\section{Conclusion}
We have proposed a novel Metropolis-Hastings algorithm for parameter 
inference in Markov jump processes. We use 
uniformization to update the MJP parameters with state-values marginalized 
out, though still conditioning on a random Poisson grid. The 
distribution of this grid depends on the MJP parameters, significantly 
slowing down MCMC mixing. We propose a simple symmetrization scheme to get 
around this dependency. In our experiments, we demonstrate the usefulness 
of this scheme, which outperforms a number of competing baselines.
We also derive conditions under which our sampler inherits geometric 
ergodicity properties of an ideal MCMC sampler.


There are a number of interesting directions for future research.
Our focus was on Metropolis-Hastings algorithms for typical settings,
where the parameters are low dimensional. It is interesting to 
investigate how our ideas extend to schemes like Hamiltonian Monte 
Carlo~\citep{Neal2010} suited for higher-dimensional settings. Another 
direction is to develop and study similar schemes for more complicated 
hierarchical models like mixtures of MJPs or coupled MJPs. While we 
focused only on Markov jump processes, it is also of interest to study 
similar ideas for algorithms for more general processes~\citep{RaoTeh12}. 
It is also important to investigate how similar ideas apply to 
deterministic algorithms like variational Bayes~\citep{OpperVarinf, panzharao17}. From 
a theoretical viewpoint, our proof required the uniformization rate to 
satisfy $\Omega(\theta) \ge k_1 \max_s A_s(\theta) + k_0$ for $k_1 > 1$. 
We believe our result still holds for $k_1 = 1$, and for completeness, 
it would be interesting to prove this.  
