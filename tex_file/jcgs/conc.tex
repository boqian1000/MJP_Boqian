\section{Conclusion}

We have proposed a novel Metropolis-Hastings algorithm for parameter 
inference in Markov jump processes. We use a representation called 
uniformization to update the MJP parameters with state-values marginalized 
out, although still conditioning on a random Poisson grid. The 
distribution of this grid depends on the MJP parameters, significantly 
slowing down MCMC mixing. We propose a novel symmetrization scheme to get 
around this dependency. In our experiments, we demonstrate the usefulness 
of this scheme, which outperforms a number of competing baselines.
We also derive conditions under which our sampler inherits geometric 
ergodicity properties of an ideal MCMC sampler.

RaoTeh2012

There are a number of interesting directions for future research.
Our focus was on Metropolis-Hastings algorithms for typical settings 
where the parameters are low dimensional. It is interesting to 
investigate how our ideas extend to schemes like Hamiltonian Monte 
Carlo~\cite{Neal2010} suited for higher-dimensional settings. Another 
direction is to develop and study similar schemes for more complicated 
hierarchical models like mixtures of MJPs or coupled MJPs. It is also 
important to investigate how similar ideas apply to deterministic 
algorithms like variational Bayes~\cite{panzanrao17}. From a theoretical 
viewpoint, our proof required the uniformization rate to satisfy 
$\Omega(\theta) \ge k_1 \max_s A_s(\theta) + k_0$ for $k_1 > 1$. We 
believe our result still holds for $k_1 = 1$, and for completeness, 
it would be interesting to prove this.  Finally, we are applying 
our ideas to other real-world applications from finance and genetics.
