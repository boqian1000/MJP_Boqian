
\section{Bayesian inference for MJPs}
In practical situations, one observes the MJP trajectory at
a finite set of times, and typically, these observations themselves
are noisy. There are then two challenges than the practitioner faces:
\begin{itemize}
  \item What is the MJP trajectory underlying the observations?
  \item What are the unknown parameters governing the dynamics of the 
    latent MJP?
\end{itemize}

\subsection{Trajectory inference for MJPs}
This problem was addressed in~\cite{RaoTeh13}, and extended to a broader 
class of MJPs (as well as other jump processes like semi-Markov processes
) in~\cite{RaoTeh12}. Both these schemes centere on alternate
approaches to Gillespie's algorithm, where we introduce auxiliary {\em thinned} 
candidate jump times to sample an MJP trajectory. 
We focus on the simpler and more widely used scheme from~\cite{RaoTeh13}, 
which is based on a classical idea called {\em uniformization}~\cite{Jen1953}. 
%while in~\cite{RaoTeh12}, this was extended to a more general dependent thinning approach. 
%We outline
%the latter below: it is more general, and we are not aware of any work
%before~\cite{RaoTeh12} that describes it.

Recall that the diagonal elements of the rate matrix $A_i$ give the 
rate at which the MJP leaves state $i$ for any other state. Importantly,
the parameters are set up so that self-transitions cannot occur. 
Now, introduce an additional parameter $\Omega \ge \max_i A_i$;
\cite{RaoTeh13} suggest setting $\Omega = 2 \max_i A_i$. 
%Assuming the system is
%in state $i$, we sample a {\em candidate} transition-time from an
%exponential, now with rate $\Omega_i$. The system remains in state $i$
%until this time, after which it moves to state $j \neq i$ with probability
%$B_{ij} = A_{ij}/\Omega_i$. The system continues to remain in its current 
%state with probability $1-A_i/\Omega_i$. 
Instead of sequentially sampling a wait-time, and then a new state as
in Gillespie's algorithm, we instead first sample a set of candidate
transition-times over the interval $[0,\cT]$ from a homogeneous Poisson process 
with rate $\Omega$. Call these times $W$; these along with $0$ define a
random grid on $[0,\cT]$.
Define $B = \left(I +\frac{1}{\Omega}A\right)$; observe that this is a
stochastic matric (with positive elements, and rows adding up to $1$).
Assign state-values to the elements in $0 \bigcup W$ according to a discrete-time 
Markov chain with initial distribution $\pi$, and transition matrix $B$;
call these state $V$. Thus $v_0 \sim \pi$, while $p(v_{k+1}=j|v_k=i) = B_{ij}$
for $k \in \{0,\cdots,|W|-1\}$.
Note that $\Omega > \max_i A_i$ results in more
candidate-times than actual MJP transitions; at the same time the transition
matrix $B$ allows self-transitions (unlike A). These two effects cancel
each other out, and trajectories sampled this way for any $\Omega > \max_i A_i$
%These self-transitions correct for the extra candidate transition times
%produced by the higher rate $\Omega_i$, and~\cite{RaoTeh12} show that
%trajectories sampled this way 
have the same distribution as trajectories
sampled by Gillespie's algorithm~\cite{Jen1953,RaoTeh13}.

Introducing the thinned variables allowed~\cite{RaoTeh13} to develop
a novel and efficient MCMC sampler. We outline it below.

\begin{algorithm}[H]
  \caption{Auxiliary variable Gibbs sampler for MJP trajectories~\cite{RaoTeh13} }
   \label{alg:Unif_gibbs}
  \begin{tabular}{l l}
   \textbf{Input:  } & \text{MJP parameters $\theta$; a set of partial and noisy observations $X$}. \\
                     & \text{A  parameter $\Omega > \max_i A_s$}.\\
                      & \text{The previous MJP path $S(t) = (S, T)$}.\\ 
   \textbf{Output:  }& \text{A new MJP trajectory $\tilde{S} (t) = (\tilde{S}, \tilde{T})$}.\\
   \hline
   \end{tabular}
   \begin{algorithmic}[1]
\State \textbf{Given the MJP trajectory $(S,T)$, sample a new set of thinned 
candidate times $U$}: %\cite{RaoTeh12} show that 
These are distributed as an inhomogeneous Poisson process with intensity 
$\Omega-A_{S(t)}$. Since the intensity is piecewise-constant, simulating this 
is straightforward.
\State \textbf{Given the thinned and actual transition times $W = (T \bigcup U)$
from the previous iteration (after discarding state information $S$), 
sample a new trajectory}:
    Conditioned on the skeleton $W$, the set of candidate jump
    times is fixed, and trajectory inference reduces to inference for
    the familiar discrete-time hidden Markov model (HMM) with initial distribution
    $\pi$, and transition matrix $B$. \cite{RaoTeh13} use the forward-filtering
    backward-sampling (FFBS) algorithm: this is an efficient dynamic 
    programming algorithm that makes a forward pass through the
    finite set of candidate times, sequentially updating the 
    distribution over states at each time $w \in W$. 
    %at each time accounting for all observations in the previous interval. 
    Between any two consecutive elements of $W$,
    %separated by an interval $\Delta w$, 
    the system remain in a fixed state, with the likelihood for a state $i$ equal
    to the likelihood under state $i$ of all observations 
    falling in that interval. 
    %and a term $\Omega_i\exp(-\Omega_i\Delta t)$,
    %the probability that the next candidate time occurs after a wait
    %$\Delta t$ under state $i$.
    At the end of the forward pass, we have a distribution over states at the
    end time that accounts for all observation.
    The algorithm then makes a backward pass through the times in $W$, 
    sequentially sampling the state at any time given the state at the 
    following time, and a distribution over states calculated during the
    forward pass. 
\end{algorithmic}
\end{algorithm}

    ~\cite{RaoTeh12} show that the resulting Markov chain targets
    the desired posterior distribution over trajectories, and is 
    ergodic for any choice of $\Omega$ strictly greater than all the
    $A_i$'s. As mentioned earlier, they suggest setting $\Omega = 2\max_i A_i$.

\subsection{Parameter inference for MJPs}
In practice, the MJP parameters themselves are unknown: often,
these are the quantities of primary interest when studying a dynamical
system. A Bayesian approach
places a prior $p(\theta)$ over these unknown variables, and the
resulting posterior distribution $p(\theta|X)$ is approximated
with samples drawn by Gibbs sampling. In particular, for an arbitrary 
initialization of the parameters and the trajectory, one repeats the
following two steps:
\begin{algorithm}[H]
  \caption{Gibbs sampling for parameter inference for MJPs}
   \label{alg:MJP_gibbs}
  \begin{tabular}{l l}
   \textbf{Input:  } & \text{A set of partial and noisy observations $X$}, \\
                      & \text{The previous MJP path $S(t) = (S, T)$, the previous MJP parameters $\theta$}.\\ 
   \textbf{Output:  }& \text{A new MJP trajectory $\tilde{S} (t) = (\tilde{S}, \tilde{T})$, 
                            new MJP parameters $\tilde{\theta}$}.\\
   \hline
   \end{tabular}
   \begin{algorithmic}[1]
  \State  Sample a trajectory from the conditional 
  $p(S_{new}(t)|X,S_{curr}(t),\theta_{curr})$ following 
  algorithm~\ref{alg:Unif_gibbs}.
  \State Sample a new $\theta$ from the conditional 
    $p(\theta_{new}|X,S_{new}(t))$.
   \end{algorithmic}
\end{algorithm}
The last distribution depends on a set of sufficient statistics of the 
MJP trajectory: how
much time is spent in each state, and the number of transitions
between each pair of states. 
%Given these, sample a new $\theta$ from the conditional $p(\theta|X,S(t))$. 
In special circumstances, $\theta$ can be directly sampled from its 
conditional distribution, otherwise, one has to use a Markov kernel like
Metropolis-Hastings or Hamiltonian Monte Carlo to update $\theta$ from the
conditional $p(\theta_{new}|X,S(t),\theta_{curr})$. 

Such a Gibbs sampling approach comes with a well-known limitation:
because of coupling between path and parameters, it can explore parameter 
and path space very sluggishly. In Figure~\ref{fig:conf1} \vinayak{Fix this}
we show
both the distribution of a component of $\theta$ conditioned
only on the observations, and conditioned on both the observations 
as well as a realization of the MJP trajectory: observe how much
more concentrated the latter is compared to the former. The
coupling is strengthened as the trajectory grows longer and longer, and
the Gibbs sampler can mix very poorly for situations with
long observation periods, even if the observations themselves are
sparse and only mildly informative about the parameters.

For the discrete-time case, this problem of parameter-trajectory
coupling can be circumvented by marginalizing out the MJP trajectory 
and directly sampling from the posterior over parameters $p(\theta|X)$.
In its simplest form, this approach involves a Metropolis-Hastings
scheme that proposes a new parameter $\theta$ from some proposal distribution 
$q(\theta_{new}|\theta_{old})$, accepting or rejecting according to the usual
Metropolis-Hastings probability. The latter step requires calculating the 
marginal probability of the observations $p(X|\theta)$, integrating out
the exponential number of possible latent trajectories. Fortunately
this marginal probability is a by-product of the forward-backward
algorithm used to same a new trajectory, and so that no 
additional computational burden is involved. The overall algorithm then is:

\begin{algorithm}[H]
  \caption{Metropolis-Hastings parameter inference for a discrete-time 
Markov chain}
   \label{alg:disc_time_mh}
  \begin{tabular}{l l}
   \textbf{Input:  } & \text{A set of partial and noisy observations $X$};
    a proposal distribution $q(\theta^*|\theta)$. \\
  & \text{The previous Markov chain parameters $\theta$}.\\
  \textbf{Output:  }& \text{A new Markov chain parameter $\theta^*$}.\\
   \hline
   \end{tabular}
   \begin{algorithmic}[1]
  \State Propose a new parameter $\theta^*$ from the proposal distribution
    $q(\theta^*|\theta)$.
  \State Run the forward pass of the forward-backward algorithm to 
    obtain the marginal likelihood of the observations, $p(X|\theta^*)$.
  \State Accept the proposed $\theta^*$ according to the MH probability, 
    $\min(1,\frac{p(X,\theta^*)q(\theta|\theta^*)}{p(X,\theta)q(\theta^*|\theta)})$.
  \State If desired, a new trajectory sample can be obtained by
    completing the backward pass of the forward-backward algorithm for the chosen
    parameter.
\end{algorithmic}
\end{algorithm}

\subsection{A marginal sampler for MJP parameters} 
Constructing such a marginal sampler over the MJP parameters by
integrating out the hidden trajectory is less straightforward:
the set of transition times is unbounded, with individual elements
unconstrained over the observation interval $[0,\cT]$.
%Naively calculating this marginal probability for the continuous-time
%case is not straightforward, as there is no finite set of candidate
%times to make a pass over. 
One approach~\cite{FearnSher2006} is to instead make a sequential 
forward pass through all {\em observations} $X$, using the matrix-exponential
operator to marginalize out the infinite number of possible 
continuous-time trajectories linking two successive times. As
demonstrated in~\cite{RaoTeh13} however, this approach has a number of 
drawbacks: it scales cubically, rather than quadratically with the 
number of states, it cannot exploit structure like sparsity in the 
transition matrix, and can depend in not trivial ways on the exact 
nature of the observation process.
Additionally, the number of expensive matrix exponential calculations scales
with the number of observations rather than the number of transitions the
system makes.

A second approach is particle MCMC~\cite{Andrieu10}. Here, one uses 
particle filtering to obtain an unbiased estimate of the marginal 
probability $p(X|\theta)$; this is then plugged into the 
Metropolis-Hastings acceptance probability. While the resulting MCMC 
sampler targets the correct posterior distribution~\cite{Andrieu09}, 
the resulting scheme resulting scheme does not exploit the structure 
of the MJP, and we show that it can be quite inefficient.

Work in~\cite{RaoTeh13, RaoTeh12} demonstrated the advantage of
introduced the thinned events $U$: this allows exploiting discrete-time 
algorithms like FFBS for efficient parameter inference.
%be brought to playthe thinning-based approach over matrix exponential and particle-MCMC
%approaches for trajectory inference. 
In the next section, we outline a \naive\  first attempt at extending this 
approach to parameter inference.
We describe why this approach is not adequate, and describe our
final algorithm in the following section. 

