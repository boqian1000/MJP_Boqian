%\vspace{-.1in}
\section{\Naive\ parameter inference via Metropolis-Hastings}
%A natural approach to reduce the coupling in the Gibbs sampler is to reduce ?Bayesian fraction of missing information?. 
For discrete-time HMMs, path-parameter coupling can be circumvented by marginalizing out the Markov trajectory, and directly sampling from the marginal posterior $P(\theta|X)$.
In its simplest form, this involves a Metropolis-Hastings (MH) scheme that proposes a new parameter $\vartheta$ from some proposal distribution $q(\vartheta|\theta)$, accepting or rejecting according to the usual MH probability.
The necessary marginal probabilities are computed using the FFBS algorithm.
The Rao-Teh algorithm suggests a simple analog for continuous-time settings: directly update $\theta$, conditioning on the random grid $W$, but marginalizing out the states $V$.
%, following the scheme from Algorithm~\ref{alg:disc_time_mh}.
%The resulting scheme updates $\theta$ conditioned on the random 
%grid, but with the trajectory integrated out 
In particular, given $W$, a parameter $\vartheta$ proposed from $q(\vartheta|\theta)$, and is accepted with probability 
$\min\left(1, 
\frac{P(X|W,\vartheta) P(W|\vartheta)p(\vartheta)q(\theta|\vartheta)} {P(X|W,\theta) P(W|\theta)p(\theta)q(\vartheta|\theta)}\right)$. 

$P(X|W,\theta)$ is the marginal probability of $X$ under a discrete-time HMM with transition matrix $B(\theta)$: this can be computed using the forward pass of the FFBS algorithm. 
The term $P(W|\theta)$ is the probability of $W$ under a rate-$\Omega(\theta)$ Poisson process, 
These, along with the corresponding terms for $\vartheta$ allow the acceptance probability to be computed.
%make a forward pass over $W$, and calculate and $P(X|W,\vartheta)$. % as in algorithm~\ref{alg:disc_time_mh}.
After accepting or rejecting $\vartheta$, the new parameter is used in
a backward pass that samples a new trajectory. 
%Then discard all self-transitions, resample $W$ and repeat. 
Algorithm~\ref{alg:MH_naive}, and figure~\ref{fig:naive_mh} in the appendix detail this.

%\vspace{-.1in}
%\vspace{-.32in}
\begin{algorithm}[H]
   \caption{\Naive\  MH for parameter inference for MJPs }
   \label{alg:MH_naive}
  \begin{tabular}{l l}
   \textbf{Input:  } & \text{Observations $X$}, 
                       \text{the MJP path $S(t) = (S, T)$, the  parameters $\theta$ }and $\pi_0$.\\ 
                     & \text{A  Metropolis-Hasting proposal $q(\cdot | \theta)$}.\\
   \textbf{Output:  }& \text{A new MJP trajectory $S'(t) = (S', T')$, 
                            new MJP parameters $\theta'$}.\\
   \hline
   \end{tabular}
   \begin{algorithmic}[1]
     \State Set $\Omega \assign \Omega(\theta) > \max_s{A_s(\theta)}$ for
     some function $\Omega(\cdot)$ (e.g.\ $\Omega(\theta) = 
      2\max_s A_s(\theta))$.
      \State { Simulate the thinned times $U$ } from a rate-$(\Omega-A_{S(t)})$ Poisson process : 
\begin{align*}
  U \sim \text{PoissProc}(\Omega - A_{S(t)}) 
\end{align*}
      \State 
    Set $W = T \cup U$ and discard $S$, the MJP state information.
    \State 
    \textbf{Forward pass:}
    Set $B(\theta) = I + \frac{A(\theta)}{\Omega(\theta)}$ and
    $\fwd^\theta_0(\cdot) = \pi_0$.
 %   Sequentially update $\fwd^\theta_i(\cdot)$ at time $w_i \in W$ as: 
    $$\textbf{for } i=1\rightarrow |W|\textbf{ do:} \quad \fwd^{\theta}_i(s') = \sum_{s \in \cS} \ell_{i-1}(s) \cdot \fwd^\theta_{i-1}(s)\cdot B_{ss'}(\theta), \quad \forall s' \in \cS.\qquad\qquad\quad $$
      \State Propose $\vartheta \sim q(\cdot| \theta)$.
      For all $w_i \in W$, calculate $\fwd^\vartheta_i(\cdot)$ similar to above.
      \State With probability $\text{acc}(\theta\rightarrow\vartheta)$, set the new parameter $\theta'$ to $\vartheta$, else set it to $\theta$. Here 
      %The acceptance probability for $\vartheta$ is given by 
%         \vspace{-.05in}
          \begin{align}
            \label{eq:ncp_acc}
            \text{acc}(\theta \rightarrow \vartheta) &=  1 \wedge \frac{P(\vartheta|W, X)}{P(\theta|W, X)} \frac{q(\theta|\vartheta)}{q(\vartheta|\theta)}
          =  1 \wedge \frac{P(X| W,\vartheta) P(W | \vartheta)P(\vartheta)}
            {P(X|W, \theta)P(W | \theta)P(\theta)} \frac{q(\theta|\vartheta)}{q(\vartheta|\theta)}.
          \end{align}
%         \vspace{-.1in}
          Here $P(X|W,\theta) = \sum_{s \in \cS} \fwd_{|W|}^\theta(s) $ and $P(W|\theta) = \Omega(\theta)^{|W|}\exp(-\Omega(\theta)t_{end})$, with similar expressions for $\vartheta$. 
          %$P(X|W,\vartheta) = \sum_{s \in \cS} \fwd_{|W|}^\vartheta(s) $. Use these, and the fact that $P(W|\theta)$ is Poisson-distributed to accept or reject the proposed $\vartheta$. Write the new parameter as $\theta'$.
    %as \boqian{$(W,\theta,\vartheta)$}.
    \State %For the new parameter $\theta'$, simulate states $V$. 
    \textbf{Backward pass:}
    Set $v_{|W|} \sim \bck^{\theta'}_{|W|}(\cdot)$, where $\bck^{\theta'}_{|W|}(s) \propto \fwd^{\theta'}_{|W|}(s)\cdot\ell_{|W|}(s) \quad \forall s \in \cS.$ 
    %at time $w_i$ given $v_{i+1}$  at time $w_{i+1}$:
    $$ \textbf{for } i=(|W|-1)\rightarrow 0\textbf{ do:} \quad v_i \sim \bck^{\theta'}_i(\cdot),\ \ \text{where } 
    \bck^{\theta'}_i(s) \propto \fwd^{\theta'}_i(s)\cdot B_{sv_{i+1}}(\theta') \cdot \ell_i(s)  \ \forall s \in \cS.$$
   % Here, $\bck^{\theta'}_{|W|}(\cdot) = \fwd^{\theta'}_{|W|}(\cdot)$.  This completes the FFBS algorithm.
    \State Let $T'$ be the set of times in $W$ when the Markov chain changes state. Define $S'$ as the corresponding set of state values. Return $(S', T', \theta')$.
\end{algorithmic}
\end{algorithm}
The resulting MCMC algorithm updates $\theta$ with the MJP trajectory 
integrated out, and by instantiating less `missing' information, can be expected to mix better. 
This can be formalized by the idea of the Bayesian fraction of missing information~\citep{liu1994fraction, papaspiliopoulos2007general}. 
Alternately, the Gibbs sampler can be viewed as operating on a centered parametrization~\citep{papaspiliopoulos2007general} or sufficient augmentation~\citep{yu2011center} of a joint distribution over parameters, Poisson events and state values, while the MH algorithm above forms a noncentered parametrization or ancillary augmentation.
For a detailed review of the suitability of these two approaches, as well as ways to combine them together, we refer to~\citet{papaspiliopoulos2007general, yu2011center}.

We however note that even with state integrated out, $\theta$ is updated {\em conditioned on $W$}, with the distribution of $W$ depending on $\theta$. 
This manifests in the $P(W|\theta)$ and $P(W|\vartheta)$ terms in the acceptance probability in equation~\eqref{eq:ncp_acc}. 
The fact that the MH-acceptance probability involves a $P(X|\theta)$ term is inevitable, however we found that the $P(W|\theta)$ terms significantly affects acceptance probabilities and mixing. 
Intuitively, under uniformization, for parameter $\theta$, $W$ is a homogeneous Poisson process with rate $\Omega(\theta)$. 
If the proposed parameter $\vartheta$ is such that $\Omega(\vartheta)$ is half $\Omega(\theta)$, then $P(W|\vartheta)$ will have half the mean and standard deviation of the distribution of the number of events in $W$ as $P(W|\theta)$, so that $\vartheta$ is unlikely to be accepted. 
%, resulting in a low acceptance probability.  This will affect mixing.
The next section describes our algorithm that get around this.
