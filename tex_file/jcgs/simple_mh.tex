\section{\Naive\ parameter inference via Metropolis-Hastings}

The key idea of the Rao-Teh algorithm~\cite{RaoTeh13} is create a random 
grid $W$ by combining the extant transition times $T$ with a set of 
thinned candidate transition times $U$, sampled from a rate-$(\Omega-A_{S(t)})$ 
Poisson process. Conditioned on $W$, sampling a new trajectory is a 
standard sampling step for a discrete-time HMM with transition matrix $B$. 
This suggests conditioning on $W$ to update
the MJP parameters as well, following the discrete-time MH-scheme from
algorithm~\ref{alg:disc_time_mh}.
%The resulting scheme updates $\theta$ conditioned on the random 
%grid, but with the trajectory integrated out 
In particular, given $W$, we discard all state information, and propose a 
new parameter $\theta^*$ from $q(\theta^*|\theta)$. 
%now conditioning on the set of times $W$.
To calculate the MH-acceptance probability $\min\left(1,
\frac{p(X|W,\theta^*)p(W|\theta^*)p(\theta^*)q(\theta|\theta^*)}
     {p(X|W,\theta)p(W|\theta)p(\theta)q(\theta^*|\theta)}\right)$, 
we make a forward pass over $W$, and calculate the marginal 
probabilities $p(X|W,\theta)$ and $p(X|W,\theta^*)$. % as in algorithm~\ref{alg:disc_time_mh}.
After accepting or rejecting $\theta^*$, the new $\theta$ can be used in
a backward pass that samples a new trajectory. We then discard all 
self-transitions and repeat; algorithm~\ref{alg:MH_naive}, and 
figure~\ref{fig:naive_mh} in the appendix sketch out this 
scheme.

\begin{algorithm}[H]
   \caption{\Naive\  MH for parameter inference for MJPs }
   \label{alg:MH_naive}
  \begin{tabular}{l l}
   \textbf{Input:  } & \text{A set of partial and noisy observations $X$}. \\
                      & \text{The previous MJP path $S(t) = (S, T)$, the previous MJP parameters $\theta$}.\\ 
                     & \text{A  Metropolis-Hasting proposal $q(\cdot | \theta)$}.\\
   \textbf{Output:  }& \text{A new MJP trajectory $\tilde{S} (t) = (\tilde{S}, \tilde{T})$, 
                            new MJP parameters $\tilde{\theta}$}.\\
   \hline
   \end{tabular}
   \begin{algorithmic}[1]
     \State Set $\Omega \equiv \Omega(\theta) > \max_s{A_s(\theta)}$ for
     some deterministic function $\Omega(\cdot)$ (e.g.\ $\Omega(\theta) = 
      2\max_s A_s(\theta))$.
      \State Sample virtual jumps $U\subset[t_{start}, t_{end}]$ from a 
      nonhomogeneous Poisson process with piecewise-constant rate 
      $R(t) = (\Omega - A_{S(t)})$. 
    Define $W = T \cup U$ and discard all MJP state information.
      \State Propose $\theta^* \sim q(\cdot| \theta)$.
          The acceptance probability is given by 
          \begin{align*}
          \alpha &=  1 \wedge \frac{p(W,\theta^*| y)}{p(W, \theta| y)} \frac{q(\theta|\theta^*)}{q(\theta^*|\theta)}
          =  1 \wedge \frac{p(y| W,\theta^*) p(W | \theta^*)p(\theta^*)}{p(y|W, \theta)p(W | \theta)p(\theta)} \frac{q(\theta|\theta^*)}{q(\theta^*|\theta)}.
          \end{align*}
    \State For both $\theta$ and $\theta^*$, make a forward pass through the 
    elements of $W$, sequentially updating the distribution over states at 
    $w \in W$ given observations upto $w$. 
    For any $\theta$, the Markov transition matrix 
    $B(\theta)$ equals $I + \frac{A(\theta)}{\Omega(\theta)}$ while the initial distribution
      over states is $\pi_0$. The likelihood of state $s$ at step $i$ is 
      $ L_i(s) = p(Y_{[w_i, w_{i + 1})} | S(t) = s , t \in [w_i, w_{i + 1})) = 
      \prod_{o: t_o \in [w_i, w_{i + 1})}p(y_{t_o} | s)$.
    At the end, we have 
    $p(X|W,\theta)$ and $p(X|W,\theta^*)$. Use these, and the fact that 
    $p(W|\theta)$ is Poisson-distributed to accept or reject the
    proposed $\theta$ and $\theta^*$. Write the new state space
    as $(W,\tilde{\theta},\tilde{\theta}^*)$.
    \State For the new parameter $\tilde{\theta}$, make a backward pass through 
    the elements of
    $W$, sequentially assigning a state to each element of $W$. This
    completes the FFBS algorithm.
    \State Let $\tilde{T}$ be the set of times in $W$ when the Markov chain changes state. Define $\tilde{S}$ as the corresponding set of state values. Return $(\tilde{S}, \tilde{T}, \tilde{\theta})$.
\end{algorithmic}
\end{algorithm}
The resulting algorithm updates $\theta$ with the MJP trajectory integrated out, 
and one would expect it to mix more rapidly than simple Gibbs sampling. 
It is important to note however that $\theta$ is updated conditioned on
$W$, and that
%determines not just the MJP trajectory $S(t)$.
%, and with $S(t)$ 
%marginalized out, the observations $X$. 
the distribution of $W$ depends on $\theta$. These are the $p(W|\theta)$ terms
in the acceptance probability; under uniformization, $W$ follows a homogeneous
Poisson process with rate $\Omega(\theta) = 2 \max A(\theta)$. 
%who probability can
%calculated easily during the forward pass.
The fact that the MH-acceptance probability involves a $p(X|\theta)$ term
is inevitable, however in our experiments, we found that the $p(W|\theta)$
terms have a significant effect acceptance probability. 
Any proposal that halves $\max A_s$ (and thus $\Omega$) will halve the
mean and variance of the distribution of the number of events in $W$, 
resulting in an extremely low acceptance probability.
In the next section, we describe a way around this issue.

