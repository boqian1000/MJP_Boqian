%\subsection{Correctness of the proposed algorithm }
%\label{sec:verify2}
\begin{proposition}
  The sampler described in Algorithm~\ref{alg:MH_improved} has the posterior distribution $P(\theta,s_0,S,T|X)$ as its stationary distribution.
\end{proposition}
\begin{proof}
  Consider a realization $(\theta,s_0,S,T)$ from the posterior distribution $P(\theta, s_0, S, T|X)$. An iteration of the algorithm first simulates $\vartheta$ from $q(\vartheta|\theta)$. By construction, the marginal distribution over all but the last variable in the set $(\theta, s_0, S, T, \vartheta)$ is still the posterior.

  The algorithm next simulates $U$ from a Poisson process with rate $\Omega(\theta,\vartheta) - A_{S(\cdot)}(\theta)$. Write $W = T \cup U$.
  The random grid $W$ consists of the actual and thinned candidate transition times, and is distributed according to a rate-$\Omega(\theta, \vartheta)$ Poisson process (Proposition 2 in~\cite{RaoTeh13}). 
  Thus, the triplet $(W,\theta,\vartheta)$ has probability proportional to $P(\theta)q(\vartheta|\theta)\text{PoissProc}(W|\Omega(\theta,\vartheta)) P(X|W,\theta,\vartheta)$.
  Next, the algorithm proposes swapping $\theta$ and $\vartheta$ with $W$ fixed (a deterministic proposal), and accepts with MH-acceptance probability 
  $$\texttt{acc} = 
  1 \wedge \frac{P(\vartheta)q(\theta|\vartheta)
P(X|W,\vartheta,\theta)}{P(\theta)q(\vartheta|\theta)
P(X|W,\theta,\vartheta)} =
1 \wedge \frac{P(\vartheta)q(\theta|\vartheta)\text{PoissProc}(W|\Omega(\vartheta,\theta)) P(X|W,\vartheta,\theta)}{P(\theta)q(\vartheta|\theta)\text{PoissProc}(W|\Omega(\theta,\vartheta))
P(X|W,\theta,\vartheta)},$$
where we exploit the symmetry of $\Omega(\cdot,\cdot)$.
%in its two arguments.
%$P(W|\theta,\vartheta)$ is just a Poisson process with rate $\Omega(\theta)+ \Omega(\vartheta)$, so that $P(W|\theta,\vartheta) = P(W|\theta,\vartheta)$, which can be ignored when calculating $\texttt{acc}$. 
%$P(X|W,\theta,\vartheta)$ and $P(X|W,\vartheta,\theta)$ are obtained
%after a forward pass over $W$ using discrete-time transition matrices
%$B(\theta,\vartheta) = \left(I + \frac{A(\theta)}{\Omega(\theta)+\Omega(\vartheta)}\right)$ 
%and $B(\vartheta,\theta) = \left(I + \frac{A(\vartheta)}{\Omega(\theta)+\Omega(\vartheta)}\right)$. 
Write the new parameters as $(\theta', \vartheta')$. 

This MH step has stationary distribution over $(W,\theta',\vartheta')$ proportional to
$P(\theta')q(\vartheta'|\theta)$ $\text{PoissProc}(W|\Omega(\theta',\vartheta'))
P(X|W,\theta',\vartheta')$, so that the triplet $(W,\theta', \vartheta')$ has the same distribution as $(W,\theta, \vartheta)$.
The algorithm uses $B(\theta',\vartheta')$ to make a backward pass through $W$, simulating state values on $W$ from the conditional of a Markov chain with transition matrix $B(\theta',\vartheta')$ given observations $X$. 
Dropping the self-transition times results in $(\theta', s'_0, S', T', \vartheta')$. 
From uniformization (see also Lemma 1 in~\cite{RaoTeh13}), the trajectory $(s'_0, S', T')$ is distributed according to the conditional of a rate-$A(\theta')$ MJP given observations $X$.
Finally, dropping $\vartheta'$ 
%effectively marginalizing this parameter.  As we saw at the start of the proof, this  
results in $(\theta',s'_0,S',T')$ from the posterior given $X$, proving stationarity.
% \begin{align*}
%  p(y, W, S, T, \theta, \theta^*) &= p(\theta) q(\theta^* | \theta) P(S,T| \theta, \theta^*) P(W| S, T, \theta, \theta^*)P(y | S, T, \theta, \theta^*)\\
%  &=p(\theta) q(\theta^* | \theta) P(S,T| \theta) P(W| S, T, \theta, \theta^*)P(y | S, T).
% \end{align*}
% The marginal distribution of $(y, S, T, \theta, \theta^*)$ and $(y, S, T, \theta)$ as follows.\\
% \begin{align*}
%  p(y, S, T, \theta, \theta^*) &= p(\theta) q(\theta^* | \theta) P(S,T| \theta, \theta^*)P(y | S, T, \theta, \theta^*)\\
%  &=P(y, S, T, \theta) q(\theta^* | \theta).
% \end{align*}
% \begin{align*}
%  p(y, S, T, \theta) &= p(\theta)P(S,T| \theta)P(y | S, T, \theta).
% \end{align*}
% So the conditional distribution over $\theta^*$ given $(y, S, T, \theta)$ is $q(\theta^* | \theta)$. And the conditional distribution over W given $(y, S, T, \theta, \theta^*)$  is $P(W | S, T, \theta, \theta^*)$, which is actually the distribution of Non Homogeneous Poisson Process with piecewise constant rate $h(\theta) + h(\theta^*) - A_{S(t)}(\theta)$.\\
% Thus the Step 1 + Step 2 is actually equivalent to sampling from the conditional distribution $P(\theta^* , W| S, T, \theta, y)$.\\
% The Step 3 + Step 4 satisfy the detailed balance condition. The reason is as follows.
% \begin{align*}
% &P((W, S, T, (\theta, \theta^*)) \rightarrow (W, S^*, T^*, (\theta^*, \theta))) P(S,T, (\theta, \theta^*) | W, y)\\
% &= (1 \wedge \frac{P((\theta^*,\theta) | W, y)}{P((\theta,\theta^*) | W, y)})P(S^*, T^* | W, (\theta^*, \theta), y)P(S, T | W, (\theta, \theta^*), y)P((\theta, \theta^*) | W, y)\\
% &= P((W, S^*, T^*, (\theta^*, \theta)) \rightarrow (W, S, T, (\theta, \theta^*))) P(S^*,T^*, (\theta^*, \theta) | W, y)
% \end{align*} 
% Therefore the stationary distribution of this MCMC sampler is $P(W, S, T, (\theta, \theta^*) | y)$. Thus the stationary distribution of $(S, T, \theta)$ is the corresponding marginal distribution $P(S, T, \theta | y)$.  
%\qed
\end{proof}

\section{Related work}\label{sec:comments}
 %the set of transition times is unbounded, with individual elements
 %unconstrained over the observation interval $[0,\cT]$.
 %Naively calculating this marginal probability for the continuous-time
 %case is not straightforward, as there is no finite set of candidate
 %times to make a pass over. 

Our paper modifies the algorithm from~\citet{RaoTeh13} to include parameter inference.
That algorithm requires a uniformization rate $\Omega(\theta) > \max_s A_s(\theta)$, and empirical results from that paper suggest $\Omega(\theta) = 2\max_s A_s(\theta)$.
The uniformization rate $\Omega(\theta,\vartheta)$ in our algorithm includes a proposed parameter $\vartheta$, must be symmetric in both arguments and must be greater than both $\max_s A_s(\theta)$ and $\max_s A_s(\vartheta)$. 
A natural and simple setting is $\Omega(\theta,\vartheta) = \max_s A_s(\theta) + \max_s A_s(\vartheta)$. 
When $\theta$ is known, our algorithm has $\vartheta$ equal to $\theta$ (i.e.\ the `proposed' $\vartheta$ equals $\theta$), and our uniformization rate reduces to $2\max_s A_s(\theta)$. 
This provides a principled motivation for the particular choice of $\Omega$ in~\citet{RaoTeh13}.

Of course, we can consider other choices, such as $\Omega(\theta,\vartheta) = \kappa(\max A_i(\theta) + \max A_i(\vartheta))$ for $\kappa > 1$.  
These result in more thinned events, and so more computation, with faster MCMC mixing. 
We study the effect of $\kappa$ in our experiments, but find the smallest setting of $\kappa=1$ performs best.
It is also possible to have non-additive settings for $\Omega(\theta,\vartheta)$, for example, $\Omega(\theta,\vartheta) = \kappa \max( \max_i A_i(\theta), \max A_i(\vartheta))$ for some $\kappa > 1$. We investigate this too.

A key idea in our paper, as well as~\cite{RaoTeh13}, is to impute the random grid of candidate transition times $W$ every MCMC iteration. 
Conditioned on $W$, the MJP trajectory follows an HMM with transition matrix $B$. 
By running the FFBS algorithm over $W$, we can marginalize out the states associated with $W$, and calculate the marginal $P(X|W,\theta)$. 
Another approach to parameter inference that integrates out state values follows~\citet{FearnSher2006}. 
 This algorithm makes a sequential forward pass through all {\em observations} $X$ (rather than $W$). 
 Unlike with $W$ fixed, one cannot a priori bound the number of transitions between two successive observations, so that~\citet{FearnSher2006} have to use matrix exponentials of $A$ (rather than just $B$) to calculate transition probabilities.
 The resulting algorithm is cubic, rather than quadratic in the number of states, and the number of expensive matrix exponentiations needed scales with the number of observations, rather than the number of transitions.
 Further, matrix exponentiation results in a dense matrix, so that~\cite{FearnSher2006} cannot exploit sparsity in the transition matrix.
 In our framework, $B=I+\frac{1}{\Omega}A$ inherits sparsity present in $A$. Thus if $A$ is tri-diagonal, our algorithm is {\em linear} in the number of states.

 A second approach to marginalizing out state information is particle MCMC~\citep{Andrieu10}. 
 This algorithm, described in section~\ref{sec:pmcmc} in the supplementary material, uses particle filtering to get an unbiased estimate of $P(X|\theta)$. 
 Plugging this estimate into the MH acceptance probability results in an MCMC sampler that targets the correct posterior, however the resulting scheme does not exploit the Markovian structure of the MJP the way FFBS can. 
 In particular, observations that are informative of the MJP state can result in marginal probability estimates that have large variance, resulting in slow mixing. 
 By contrast, given $W$, FFBS can compute the marginal probability $P(X|W,\theta)$ {\em exactly}. 

The basic idea of marginalizing out information to accelerate MCMC convergence is formalized by the idea of the Bayesian fraction of missing information~\citep{liu1994fraction}. 
In this context, papers such as~\citet{papaspiliopoulos2007general,yu2011center} have studied MCMC algorithms for hierarchical latent variable models. 
%Uniformization forms such a representation for MJPs: first sample $\theta$, then sample the latent $W$, and use this to sample MJP state values. 
%The \naive\ algorithm~\ref{alg:MH_naive} is a direct application of ideas presented in those papers.
%An interesting direction is to see how these frameworks can shed light on, and improve our symmetrized MH algorithm~\ref{alg:MH_improved}. 
%Viewed in this light, our contribution is a rewriting of uniformization that includes the auxiliary parameter $\vartheta$. Our swap operator forms a particular Markov kernel that exploits this reparametrization for fast mixing. 
%Two interesting directions are to see how such symmetrization ideas apply to other problems considered in those works, and how ideas from those works can shed more light on, and improve our algorithm.
The Gibbs sampler of algorithm~\ref{alg:MJP_gibbs} can be viewed as operating on a centered parametrization~\citep{papaspiliopoulos2007general} or sufficient augmentation~\citep{yu2011center} of a hierarchical model involving the parameter $\theta$, the latent variables $(v_0, V, W)$ and the observations $X$. 
These papers then suggest noncentered parametrizations or ancillary augmentations, which in our context correspond to simulating $\theta$, $W$, and an {\em independent} set of $(|W|+1)$ i.i.d.\ uniform random variables $Q$. 
Through a sequence of inverse-cdf transforms, the state values $(v_0,V)$ are then written as a deterministic function of $Q$ and $\theta$: $(v_0,V) = f_\theta(Q)$, after which the observations $X$ are produced. 
Now, proposing a new parameter $\vartheta$ automatically proposes a new set of state variables $(v_0',V') = f_\vartheta(Q)$, so that problem of path-parameter coupling is avoided.
A similar idea could also be used to avoid couplng between $\theta$ and the Poisson process $W$.
However now, updating $Q$ given $\theta$ and $(v_0, V, W)$ raises significant challenges to mixing.
By contrast, our approach marginalizes out the variables $(v_0,V)$ (or $Q$), and will mix significantly faster.
Nevertheless, results from the literature on NCPs can suggest further improvements to our approach, and give guidance about conditions under which 
approaches like ours outperform centered parametrisations like algorithm~\ref{alg:MJP_gibbs}, or when a mixture of centered and non-centered
updates could be useful~\citep{yu2011center}.
%Poisson events $W$, and the state values $(v_0,V)$. The MH algorithm reverses the order in which the path and parameter are updated, and is closely related to 
%For a detailed review of the suitability of these two approaches, as well as ways to combine them together, we refer to~\citet{papaspiliopoulos2007general, yu2011center}.


%
 %
Our approach of first simulating $\vartheta$, and then simulating $W$ from a Poisson process whose rate is symmetric in $\theta$ and $\vartheta$ is related to~\citet{Neal04Drag}. In that work, to simulate from an `energy' model $P(x,y) \propto \exp(-E(x,y))$, the author proposes a new parameter $x^*$, and then updates $y$ via intermediate transitions to be symmetric in $x$ and $x^*$, before proposing to swap $x$ and $x^*$. Our approach exploits the specific structure of the Poisson and Markov jump processes to do this directly, avoiding the need for any tempered transitions. 

Our algorithm is also related to work on MCMC for doubly-intractable distributions.  Algorithms like~\cite{Moller2006,murray2006,Andrieu09} all attempt to evaluate an intractable likelihood under a proposed parameter $\vartheta$ by introducing auxiliary variables, typically sampled independently under the proposed parameters. 
For MJPs, this would involve proposing $\vartheta$, generating a new grid $W^*$, and then using $P(X|W,\theta)$ and $P(X|W^*,\vartheta)$ in the MH acceptance step. 
This is more involved (with two sets of grids), and introduces additional variance that reduces acceptance rates. %if the new parameter $\vartheta$ is incompatible with the old grid $U$ or vice versa. 
While~\cite{murray2006} suggest annealing schemes to try to address this issue, we exploit the uniformization structure to provide a cleaner solution: generate a single set of auxiliary variables that depends symmetrically on both the new and old parameters. 
%It is interesting to see whether a similar idea can be used in other applications as well.
