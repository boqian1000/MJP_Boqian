%\subsection{Correctness of the proposed algorithm }
%\label{sec:verify2}
\begin{proposition}
  The sampler described in Algorithm~\ref{alg:MH_improved} has the posterior
  distribution $p(\theta,S(t)|X)$ as its stationary distribution.
\end{proposition}
\begin{proof}
  Suppose that at the start of the algorithm, we have a pair $(\theta,S(t))$ from
  the posterior distribution $p(\theta,S(t)|X)$. Introducing $\vartheta$
  from $q(\vartheta|\theta)$ results in a triplet whose marginal over the first
  two variables is still $p(\theta,S(t)|X)$.

  Sampling $U$ from a Poisson process with rate $\Omega(\theta) +
  \Omega(\vartheta) - A_{S(t)}(\theta)$, results in a random grid $W = T \cup U$
  that is distributed according to a rate $\Omega(\theta) + \Omega(\vartheta)$
  Poisson process (Proposition 2 in~\cite{RaoTeh13}). Discarding all state 
  information results in a triplet $(W,\theta,\vartheta)$ with probability
  proportional to $p(\theta)q(\vartheta|\theta)p(W|\theta,\vartheta)
  p(X|W,\theta,\vartheta)$.

Next we propose swapping $\theta$ and $\vartheta$. Since this
is a deterministic proposal, the MH-acceptance probability is given by
$$\alpha = 1 \wedge \frac{p(\vartheta)q(\theta|\vartheta)p(W|\vartheta,\theta)
p(X|W,\vartheta,\theta)}{p(\theta)q(\vartheta|\theta)p(W|\theta,\vartheta)
p(X|W,\theta,\vartheta)}$$
The term $p(W|\theta,\vartheta)$ is just a Poisson process with rate $\Omega(\theta)+
\Omega(\vartheta)$, so that $p(W|\theta,\vartheta) = p(W|\theta,\vartheta)$. 
$p(X|W,\theta,\vartheta)$ and $p(X|W,\vartheta,\theta)$ are obtained
after a forward pass over $W$ using discrete-time transition matrices
$B(\theta,\vartheta) = \left(I + \frac{A(\theta)}{\Omega(\theta)+\Omega(\vartheta)}\right)$ 
and $B(\vartheta,\theta) = \left(I + \frac{A(\vartheta)}{\Omega(\theta)+\Omega(\vartheta)}\right)$. 

Calling the parameters 
after the accept step $(\theta', \vartheta')$, we have that
$(\theta', \vartheta',W)$ has the same distribution as
$(\theta, \vartheta,W)$.
Finally, following Lemma 1 in~\cite{RaoTeh13}, using the matrix 
$B(\theta,\vartheta)$ to make a backward pass through $W$,
and discarding the self-transitions results in a trajectory $(S'(t)$
distributed according to $A(\theta')$. Discarding the auxiliary parameter
$\vartheta'$ results is a pair $(\theta',S'(t))$ from
the posterior.
% \begin{align*}
%  p(y, W, S, T, \theta, \theta^*) &= p(\theta) q(\theta^* | \theta) P(S,T| \theta, \theta^*) P(W| S, T, \theta, \theta^*)P(y | S, T, \theta, \theta^*)\\
%  &=p(\theta) q(\theta^* | \theta) P(S,T| \theta) P(W| S, T, \theta, \theta^*)P(y | S, T).
% \end{align*}
% The marginal distribution of $(y, S, T, \theta, \theta^*)$ and $(y, S, T, \theta)$ as follows.\\
% \begin{align*}
%  p(y, S, T, \theta, \theta^*) &= p(\theta) q(\theta^* | \theta) P(S,T| \theta, \theta^*)P(y | S, T, \theta, \theta^*)\\
%  &=P(y, S, T, \theta) q(\theta^* | \theta).
% \end{align*}
% \begin{align*}
%  p(y, S, T, \theta) &= p(\theta)P(S,T| \theta)P(y | S, T, \theta).
% \end{align*}
% So the conditional distribution over $\theta^*$ given $(y, S, T, \theta)$ is $q(\theta^* | \theta)$. And the conditional distribution over W given $(y, S, T, \theta, \theta^*)$  is $P(W | S, T, \theta, \theta^*)$, which is actually the distribution of Non Homogeneous Poisson Process with piecewise constant rate $h(\theta) + h(\theta^*) - A_{S(t)}(\theta)$.\\
% Thus the Step 1 + Step 2 is actually equivalent to sampling from the conditional distribution $P(\theta^* , W| S, T, \theta, y)$.\\
% The Step 3 + Step 4 satisfy the detailed balance condition. The reason is as follows.
% \begin{align*}
% &P((W, S, T, (\theta, \theta^*)) \rightarrow (W, S^*, T^*, (\theta^*, \theta))) P(S,T, (\theta, \theta^*) | W, y)\\
% &= (1 \wedge \frac{P((\theta^*,\theta) | W, y)}{P((\theta,\theta^*) | W, y)})P(S^*, T^* | W, (\theta^*, \theta), y)P(S, T | W, (\theta, \theta^*), y)P((\theta, \theta^*) | W, y)\\
% &= P((W, S^*, T^*, (\theta^*, \theta)) \rightarrow (W, S, T, (\theta, \theta^*))) P(S^*,T^*, (\theta^*, \theta) | W, y)
% \end{align*} 
% Therefore the stationary distribution of this MCMC sampler is $P(W, S, T, (\theta, \theta^*) | y)$. Thus the stationary distribution of $(S, T, \theta)$ is the corresponding marginal distribution $P(S, T, \theta | y)$.  
\end{proof}

\subsection{Comments}\label{sec:comments}

The uniformization scheme of~\cite{RaoTeh13} works for any underlying Poisson
process whose rate $\Omega$ is greater than $\max_i A_i$. The strict inequality
ensures that the conditional probability of sampling one or more thinned events 
$U$ is positive for every trajectory $S(t)$ (recall 
$U \sim \text{PoissonProc}(\Omega-A_{S(t)})$). Empirical results from~\cite{RaoTeh13}
suggest setting $\Omega = 2 \max_i A_i$.

Implicit in our new scheme is a uniformizing Poisson process with rate
$\Omega(\theta,\vartheta) = \Omega(\theta) + \Omega(\vartheta)$. For our 
scheme to be valid, $\Omega(\theta,\vartheta)$ must be greater than both
$\max_i A_i(\theta)$ and $\max_i A_i(\vartheta)$. The smallest and simplest such
choice is $\Omega(\theta,\vartheta) = \max A_i(\theta) + \max A_i(\vartheta)$.
For a fixed $\theta$, this reduces to $\Omega = 2\max A_i$, providing
an intuitive motivation for the approach in~\cite{RaoTeh13}.
Larger alternatives include 
$\Omega(\theta,\vartheta) = \kappa(\max A_i(\theta) + \max A_i(\vartheta))$
for $\kappa > 1$.  These result in more thinned events, and therefore more 
computation, with the benefit of faster MCMC mixing. We study the effect of 
$\kappa$ in our experiments.

It is also possible to have non-additive settings for $\Omega(\theta,\vartheta)$.
A simple option is to set it equal to 
$\kappa \max( \max_i A_i(\theta), \max A_i(\vartheta))$ for some choice of $\kappa
> 1$. We investigate this option as well.

Our proposed algorithm is related to work on MCMC inference for doubly-intractable distributions.
Algorithm like~\cite{Moller2006,murray2006,Andrieu09} all attempt to evaluate an intractable
likelihood under a proposed parameter $\theta^*$ by introducing auxiliary variables, however there
the auxiliary variable is sampled independently under the proposed parameters. For MJP, this would
involve proposing a new parameter $\theta^*$, generating a new uniformizing grid $U^*$, and then
accepting or rejecting. This can complicate computations (with two sets of time points), and also
reduce acceptance rates if
 the new parameter $\theta^*$ is incompatible with the old grid $U$ or
vice versa. While~\cite{murray2006} suggest annealing schemes to try to address this issue, we exploit
the structure of the Poisson process and provide a cleaner solution: generate a single set of
auxiliary variables that depends symmetrically on both the new and old parameters. It is interesting
to see whether a similar idea can be used in other applications as well.
