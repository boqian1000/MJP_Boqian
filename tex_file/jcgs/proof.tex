\section{Geometrical ergodicity of the MH algorithm}~
As shown before, virtual jumps are sampled after uniformization procedure. Let's consider the redundant representation of the trajectory. $V = (v_0, v_1, v_2,..., v_n)$, $W = (w_1, w_2, w_3,..., w_n)$, where $v_i$ can be the same as $v_{i + 1}$. Denote $X$ as $(W, V, \theta)$. Also assume that noisy observations $Y = (Y_1, Y_2, ..., Y_k)$ are observed at some determined times $(t_1^{obs}, t_2^{obs}, ..., t_k^{obs})$. We will show that the chain of $X's$ is geometrically ergodic. Let $P$ be the transition kernel of the Markov chain $X_n$ generated by Algorithm 1. Let $\Pi(X)$ be the target distribution, which is the posterior distribution $P(X | Y)$. First, consider the following posterior distribution.
\begin{align*}
P(V | W, Y, \theta) &\propto  P(W, V | \theta) P(Y | W, V) p_0(\theta)\\
&= \nu_0(v_0)g_0(v_0; \theta)\prod_{i = 1}^n P(v_{i - 1}, v_i; \theta)g_i(v_i; \theta) p_0(\theta)\\
\end{align*}
\begin{align*}
g_i(v; \theta) &= \Omega(\theta) \exp(-(w_{i + 1} - w_i)\Omega(\theta)) \prod_{j: w_i < t_j^{obs} < w_{i + 1}} L_j(Y_j| v)\\
g_n(v; \theta) &= \exp(-(t_{end} - w_n)\Omega(\theta)) \prod_{j: w_n < t_j^{obs} < t_{end}} L_j(Y_j| v)\\
\end{align*}
Given the jumping times $W$, the state on the jumping times is a discrete time markov chain with $B(\theta)$ as transition matrix.
\begin{align*}
B(\theta) = I + \frac{A(\theta)}{\Omega(\theta)}\\
\end{align*}
The following is $n_0$ step transition probability.
\begin{align*}
B_{i - n_0 : i}(v_{i - n_0}, s_i = v; \theta) = \sum_{v_{i - n_0 + 1}, v_{i - n_0 + 2},...,v_{i - 1}} (\prod_{l = i - n_0 + 1}^{i - 1} B(v_{l - 1}, v_l; \theta))B(v_{i - 1}, v; \theta)\\
B_{i - n_0 : i}^{g}(v_{i - n_0}, s_i = v; \theta) = \sum_{v_{i - n_0 + 1}, v_{i - n_0 + 2},...,v_{i - 1}} (\prod_{l = i - n_0 + 1}^{i - 1} B(v_{l - 1}, v_l; \theta)g_l(v_l; \theta))B(v_{i - 1}, v; \theta)\\
\end{align*}
\begin{theorem}~
  Denote the state space as $S$ and the parameter space as $\Theta$, and assume that:\\
1. Exist an irreducible rate matrix $A^{min}$ such that $A(s, s';\theta) \geq A^{min}(s, s')$, for $\forall s \in S$ and $s' \in S$, where $s \neq s'$.\\
2. For $\forall \theta$, $\forall s$,  and the determined $\Omega(\theta)$, there exits an $\eta$ such that $\frac{A(s; \theta)}{\Omega(\theta)} \leq 1 - \eta$, where $A(s; \theta) = \sum_{s' \neq s} A(s, s';\theta)$.\\
3. $\exists \Omega^{max} < +\infty$, such that, $\forall \theta$, $\Omega(\theta) \leq \Omega^{max}$.\\
4. $\exists \kappa > 0$ such that the proposal distribution in MH step $q(\theta' | \theta) \geq \kappa q_0(\theta_0)$, where $q_0(\theta_0)$ is the prior distribution of $\theta$.\\
\indent Then the chain $X_n$ generated by Algorithm 1 is geometrically ergodic, i.e. there exist an constant $\gamma$ and a function $M(X)$ such that $L(Y | X) > 0$ and every m, $$\Vert P^m(X, \cdot) - \Pi(\cdot) \Vert_{tv} \leq \gamma^m M(X).$$
\end{theorem} 
\begin{lemma}~
   For a fixed $n_0$ , and $\forall$ $i$  such that $ n_0 \geq i \leq n - 1$, assume that\\
1. $\exists$  $ \xi > 0$, $B_{i - n_0: i}(v_{i - n_0}, v; \theta) \geq \xi$, $\forall$ $v_{i - n_0}, v \in S$.\\
2. $\exists$  $\eta > 0$, $B(v,v ; \theta) \geq \eta$, $\forall$, $v \in S$.\\
3. $\exists$  $g_l^{min} > 0$, $g_l^{max}$, such that $g_l^{min} \leq g_l(v; \theta) \leq g_l^{max}$, for $\forall$ $v \in S$, and $\forall$ $l \in [i - n_0 + 1, i]$\\ 
Then $\mathbb{P}(V_i = v | V_{i + 1} = v, W, \theta, Y) \geq \delta_i$ and $\delta_i = \xi\eta \prod_{l = i - n_0 + 1}^i \frac{g_l^{min}}{g_l^{max}}$.
\end{lemma}
\begin{proof}
  We condition on $V_{i - n_0} = v_{i - n_0}$ and we can get the following. 
\begin{align*}
\mathbb{P}(V_i = v | v_{i + 1} = v, V_{i - n_0} = v_{i - n_0} W, \theta, Y) &= \frac{\mathbb{P}(V_i = v | v_{i - 1} = v, V_{i - n_0}| W \theta, Y)}{\mathbb{P}(v_{i + 1} = v, V_{i - n_0}| W \theta, Y)} \\
&= \frac{B_{i - n_0: i}^g(v_{i - n_0}, v; \theta)g_i(v;\theta)B(v,v;\theta)}{\sum_{v'}B_{i - n_0: i}^g(v_{i - n_0}, v'; \theta)g_i(v';\theta)B(v',v;\theta)} \\
&\geq \frac{\prod_{l = i - n_0 + 1}^i g_l^{min}}{\prod_{l = i - n_0 + 1}^i g_l^{max}} \frac{B_{i - n_0: i}(v_{i - n_0}, v; \theta)B(v, v; \theta)}{\sum_{v'}B_{i - n_0: i}(v_{i - n_0}, v'; \theta)B(v', v; \theta)}\\
&\geq \xi \eta \prod_{l = i - n_0 + 1}^i \frac{g_l^{min}}{g_l^{max}}
\end{align*}
\end{proof}
\begin{lemma}~
 If the assumptions of Lemma 2 hold for $\forall$ $i \in [n_0, n - 1]$, then $\mathbb{E}[|J| | W, \theta, Y] \leq |W|  + 1- \sum_{i = n_0}^{|W| - 1} \delta_i$, where $J = \{i \in [1, n]: V_i \neq V_{i - 1}\} \cup \{ 0 \}$ for $\forall \theta \in \Theta$.
\end{lemma}
\begin{proof}
We notice that $|W| = n$. Apply lemma 2 to every $i \in [n_0, n - 1]$. We can get $\mathbb{P}(V_i = v| V_{i + 1}, W, \theta, Y) \geq \delta_i.$ Then $\mathbb{E} \mathbb{I}_{\{V_i \neq V_{i + 1} \}} = \mathbb{E}[\mathbb{P}(V_i \neq V_{i + 1}| V_{i + 1}, W, \theta, Y) | W, \theta, Y] \leq 1 - \delta_i$. Then we apply the naive upper bound $1$ for every $i < n_0$. Then $$\mathbb{E}[|J| | W, \theta, Y] = \mathbb{E}[1 + \sum_{i = 0}^{n - 1}\mathbb{I}_{\{V_i \neq V_{i + 1}\}}| W, \theta, Y] \leq n + 1 - \sum_{i = n_0} ^ {n - 1} \delta_i.$$
\end{proof}
\begin{proposition}
  (Drift Condition)  Under the assumptions of theorem 1, there exit   $0 < \delta < 1$ and $c < +\infty$ such that in one step of algorithm 1, $\mathbb{E} [|J(X')| |X] \leq (1 - \delta) |J(X)| + c$.
\end{proposition}
\begin{proof}
Assume the current state is $X = (V, W, \theta)$, where $V = (v_0, v_1, v_2,..., v_n)$, $W = (w_1, w_2, w_3,..., w_n)$. First we will show there exists a $n_0$, such that the assumptions of lemma 2 hold for this $n_0$. Let $B^{min} = I + \frac{A^{min}}{r^{max}}$. Because $A^{min}$ is irreducible, $B^{min}$ is irreducible too. So there exists $n_0 \in \mathbb{Z}^+$ and $\xi > 0$ such that $(B^{min})^{n_0}(v, v') \geq \xi$ for $\forall v, v' \in S$. So condition 1 of lemma 2 holds. Assumption 2 of Theorem 1 leads to condition 2 of lemma 2. \\
In the first step of algorithm 1, virtual jumps $U$ are sampled. And we get $W'$. $|U|$ is poisson distributed with rate $\int_{t_{start}}^{t_{end}}(\Omega(\theta) - A(X_t))dt \leq \Omega^{max}(t_{end} - t_{start})$. So $\mathbb{E}[|W'| | X] \leq |J(X)| + \Omega^{max}(t_{end} - t_{start})$.\\For condition 3 of lemma 2, the upper bound is uniform for all $l$. $g_l^{max} = \Omega^{max}$. For the lower bound, if $g_l(v; 
\theta)$ includes likelihood factors, then we simply use $0$ as lower bound. For the remaining, we have a uniform lower bound $g_l^{min} = \Omega^{min} \exp(\Omega^{max}(t_{end} - t_{start}))$, where $\Omega^{min} = \min_{v}\{ \sum_{v' \neq v} A(v, v')\}$. There are at most $k$  $g_l(v; \theta)$ which contains likelihood factors. If $g_l^{min} = 0$, then let $\delta_i = 0$ for $i \in [l, l + n_0 - 1]$. We also let $\delta_i = 0$ for $ i < n_0$. So there are at most $(k + 1) n_0$ indices $i$ such that $\delta_i = 0$. For at least $n - (k + 1)n_0$ indices such that $\delta_i = \xi \eta (\frac{g^{min}}{g^{max}})^{n_0} \triangleq \delta$ which is free of $\theta$.\\ 
Then apply lemma 3 using the $\delta_i$ we define earlier. $\mathbb{E}[|J'| W', \theta', Y] \leq |W'| + 1 - (|W'| - (k + 1) n_0) \delta \leq (1 - \delta) |W'| + ((k + 1)n_0\delta + 1)$\\
$$\mathbb{E}[J(X') | X] = \mathbb{E}[\mathbb{E}[J(X')| W'] | X] \leq (1- \delta)(|J(X)| + \Omega^{max}(t_{end} - t_{start})) + (k + 1)n_0\delta + 1.$$
\end{proof}
\begin{proposition}
  (Small set Condition)  The set $\{ X: |J(X)| \leq h \}$ is 1-small for every $h > 0$. I.e. there exits a probability measure $\Psi$ and a constant $\beta > 0$ such that $P(X, dX') \geq \beta \Psi(dX')$, where $|J(X)| \leq h$.
\end{proposition}
\begin{proof}
First choose a $V^\dagger = (v_1^\dagger, v_2^\dagger,..., v_k^\dagger)$ such that $\prod_{j = 1}^k L_j(Y_j|v_j^\dagger) \triangleq L^\dagger > 0$. Then define $V^\ast = (v_0^ \ast, v_1^\ast, ...,v_m^\ast)$ such that $V^\dagger$ is a sub-sequence of $V^\ast$, where $v_i^\ast \neq v_{i + 1}$. Also, embed $t^{obs} = (t_1^{obs}, t_2^{obs},..., t_k^{obs})$ in a longer time sequence $W^\ast = (w_1^\ast, w_2^\ast,...,w_m^\ast)$.\\
$$\nu_0(v_0^\ast) \prod_{i = 1} ^ {m - 1}B(v_i^\ast,v_{i + 1}^\ast; \theta) \geq \nu_0(v_0^\ast) \prod_{i = 1} ^ {m - 1} \frac{A^{min}(v_i^\ast,v_{i + 1}^\ast)}{\Omega^{max}} \triangleq \beta_2^\ast.$$
Define the regeneration measure $\Psi(dX')$ as follows.\\
$$W_i' \sim Unif(w_{i - 1}^\ast, w_i^\ast)$$ 
$$V_i' = v_i$$
$$\theta' \sim p_0(\cdot)$$
The random vector $(w_1', w_2',...,w_m')$ has the uniform distribution on the set $\tau = \{ (w_1, w_2,...,w_m)|w_{i - 1}^\ast \leq w_i\leq w_i^\ast\}$. There is another way in which algorithm 1 can be executed. $\Omega(\theta) - B(v; \theta) \geq \eta \Omega(\theta) \geq \eta \Omega^{min} \triangleq \epsilon$. In step 1, we independently sample two Poisson processes $U_0$ and $U_1$ with rates $\epsilon$ and $\Omega(\theta) - B(V_t; \theta)$. Let $U = U_0 \bigcup U_1$. Then $W' = J(X) \bigcup U$. $$\mathbb{P}(U_0 \in \tau) \triangleq \beta_0 > 0.$$
$$\mathbb{P}(U_1 = \varnothing) \geq \exp(-\Omega^{max}(t_{end} - t_{start})) \triangleq \beta_1 > 0.$$
In step 2 of algorithm 1, $\theta$ is updated under Metropolis Hasting scheme. First, $\theta'$ is proposed via the proposal kernel $q(\theta'| \theta)$, then $\theta'$ is accepted with probability $\alpha = 1 \wedge \frac{q(\theta | \theta')P(\theta'| W', Y)}{q(\theta' | \theta)P(\theta| W', Y)} \triangleq 1 \wedge \bar{\alpha}$. Assume $P(Y | V, W) \in [L_{min}, L_{max}]$
\begin{align*}
P(\theta, W', Y) &= \int p(V', \theta, W', Y) dV'\\
&= \int p_0(\theta)P(W, V | \theta)P(Y | V', W') dV'\\
&\in [L_{min}p_0(\theta)P(W'| \theta), L_{max}p_0(\theta)P(W'| \theta)]
\end{align*} 
We notice that $P(W' | \theta) = \Omega(\theta)^{|W'|}(t_{end} - t_{start})^{|W'|}\exp(-\Omega(\theta)(t_{end} - t_{start}))$
\begin{align*}
\bar{\alpha} &\geq \frac{q(\theta | \theta' )}{q(\theta' | \theta)} \frac{L_{min}}{L_{max}}\frac{P(W' | \theta')}{P(W'|\theta)}\frac{p_0(\theta)}{p_0(\theta)}\\
&\geq \frac{q(\theta| \theta')}{q(\theta' | \theta)}\frac{L_{min}}{L_{max}}(\frac{\Omega^{min}}{\Omega^{max}})^{|W'|}\exp(-\Omega^{max}(t_{end} - t_{start}))\frac{p_0(\theta')}{p_0(\theta)}\\
&\geq \kappa\frac{L_{min}}{L_{max}}(\frac{\Omega^{min}}{\Omega^{max}})^{|W'|}\exp(-\Omega^{max}(t_{end} - t_{start}))\frac{p_0(\theta')}{q(\theta' | \theta)}\\
&\triangleq \kappa C(|W'|)\frac{p_0(\theta')}{q(\theta' | \theta)}
\end{align*}
So for $\forall \theta' \in \Theta$, and given $W'$,
 \begin{align*}
&q(\theta' | \theta) \hat{\alpha} d\theta' \geq \kappa C(|W'|) p_0(\theta')d\theta'\\
&q(\theta' | \theta)  d\theta' \geq \kappa p_0(\theta')d\theta'\\
\end{align*}
So for $\forall \theta' \in \Theta$,  $\exists D(|W'|) = 1 \wedge C(|W'|) < +\infty$ such that $q(\theta' | \theta)\alpha d\theta' \geq D(|W'|)p_0(\theta')d\theta'$. We notice that $D(x)$ is decreasing with respect to $x$.\\
In step 2, FFBS is applied to sample a new trajectory given the jump times. We can use rejection sampling to do the same sampling $ V$ from $P(\cdot| W', \theta', Y)$ as follows.\\
(i) Simulate $V'$ with transition matrix $B(\cdot,\cdot;\theta')$ and initial distribution $\mu_0$.\\
(ii) Accept it with probability $\alpha= \frac{\mu_0(v_0')\prod_{i = 0}^{|W'| - 1}B(v_i', v_{i + 1}'; \theta)\prod_{i =0}^{|W'|}g_i(v_i'; \theta)}{\mu_0(v_0')\prod_{i = 0}^{|W'| - 1}B(v_i', v_{i + 1}';\theta) (\Omega(\theta))^{|W'| + 1}L_{max}} \geq \frac{(\Omega^{min})^{|W'| + 1}}{(\Omega^{max})^{|W'| + 1}L_{max}}$.\\
Suppose the current state is $X = (W, V, \theta)$ with $|J(X)| \leq h$ and the next state is $X' = (W', V', \theta)$. We define the following events\\
$(E_1)$: In step 1, we get $W' = J(X) \bigcup U_0$, and $U_0 \in \tau$. So $\mathbb{P}(E_1) \geq \beta_0 \beta_1 > 0$.\\
$(E_2)$: In step 2, we propose $\theta'$ and accept it. $\mathbb{P}(E_2) \geq D(m + h)p_0(\theta')d\theta'\triangleq \beta_2 p_0(\theta')d\theta'$, where $\beta_2 > 0$. \\
$(E_3)$: In step 3, all the jump times in $J(X)$ become virtual jump times. $\mathbb{P}(E_3) \geq \beta_2^\ast \eta^{|J(X)|} \geq \beta_2^\ast \eta^h \triangleq \beta_3 > 0$\\
$(E_4)$: given $E1$ -- $E3$, $V'$ is accepted. $\mathbb{P}(E_4) \geq (\frac{\Omega^{min}}{\Omega^{max}})^{m + h}\exp(-\Omega^{max}(t_{end} - t_{start})) \frac{L^\dagger}{L_{max}} \triangleq \beta_4$.\\
If $E_1$ -- $E_4$ happen, then a new trajectory $(W', V')$ is generated. It is independent with $X$. $p(W', V') = \Psi |_{(W,V)}(W', V')$.
So, $P(X, dX') \geq \beta_0\beta_1\beta_2\beta_3\beta_4 \Psi(dX')$.
\end{proof}
Theorem 1 directly follows from Propositions 4 and 5, according to Roberts and Rosenthal (2004).

