\documentclass[11pt]{article}
\usepackage{float}
\usepackage{fullpage}
\usepackage{natbib}
\usepackage{bbm}

\newtheorem{defn}{Definition} 
\newtheorem{prop}[defn]{Proposition} 

\newcommand{\rev}[2]{\textbf{Comment #1: }\emph{#2}}
\newcommand{\resp}{\textbf{Response: }}

\title{Response to review {\texttt{review.pdf}} of `Efficient parameter inference for Markov jump processes'}
\author{}
\date{}
\begin{document}
\maketitle

Thank you for your thoughtful and detailed review. Below, we include a detailed response to your comments. We have revised the manuscript accordingly. 

~\\~\\
\rev{(Page 1)}{I think the connection to the literature on non-centred parametrisations (NCPs) must be made clear in the present manuscript.}\\ 
\resp 
We are sincerely thankful to you for directing us to these papers. We agree that these address the same general problem of parameter sampling in latent variable models. 
We have included a thorough discussion of the ideas there, and how our work relates to these in the section on related work, as well as when we introduce the MH samplers.
However, we believe that you are mistaken that our algorithm is a simple instance of these. 
Instead, Algorithm 2 in section 4.3 of your review corresponds to our ``naive" algorithm that we describe in section 4 of our paper. 
This by itself forms a straightforward combination of ideas from Rao and Teh, 2013 and parameter inference for HMMs, and we agree that this has a fairly straightforward relationship to NCP sampling methods. 
Our main contribution though is the symmetrized MH samplers, which we believe is a novel and non-obvious contribution. This is what performs best in our experiments, and is also what our theory on geometric ergodicity addresses. We clarify this below.

~\\
\rev{(Page 4)}{Note that (3) defines a NCP because the parameters $\theta$ and the latent Poisson process $\tilde{x}$ are independent a-priori.}\\
\resp{This is actually not true: the Poisson rate $\lambda$ (in our notation, we call this $\Omega$, though we will stick with your notation) must be larger that the largest rate at which events happen in the MJP: 
  $\lambda(\theta) \ge \max A_s(\theta)$. 
  This introduces dependence between the parameters and the Poisson process, which is also why we explicitly include the parameter $\theta$ in the Poisson rate, writing it as $\lambda(\theta)$ (or $\Omega(\theta)$ in our notation), e.g.\ when we set 
$\lambda(\theta) = 2 * \max A_s(\theta)$.}

This dependence between $\lambda$ and $\theta$ is crucial, and is why the naive algorithm (and algorithm 2 described in section 4 of the manuscript) are not adequate. At the end of section 4 and start of section 5 of both the original and new manuscript, we discuss this issue.
The symmetrized MH sampler that is our main contribution seeks to solve this.

~\\
\rev{(Page 1)}{The proposed parameter-estimation method turns out to be a particular non- centred algorithm which is a version of the non-centred algorithms for Poisson point process models proposed in Papaspiliopoulos et al. (2007); Roberts et al.  (2004)."}\\
\rev {(Page 5)}{Note that Algorithm 2 given above coincides with Algorithm 5 proposed in the reviewed paper.}\\
\resp
As we stated, Algorithm 2 in the review corresponds to Algorithm 4 in the original submission, and Algorithm 3 in the revision. Specifically, the algorithm in the review does not have a deterministic `swap'-proposal, and the new parameter there is proposed {\em after} simulating the Poisson events, not before. It is not aware of, and thus does not attempt to solve the coupling between the parameter and the Poisson grid.

This is because it assumes there is no dependence between the parameters $\theta$ and the Poisson process (or at least, that this dependence is not significant). However, as our experiments show, this dependence is strongly affects MCMC performance, and that the  naive algorithm (and algorithm 2 from the review) are significantly worse than our symmetrized MH sampler (and often even Gibbs).
%In particular, the Poisson process itself contains significant information about the current parameter $\theta$, so that the `obvious' MCMC algorithm is inadequate.

Our symmetrized MH algorithm  realizes that the uniformization scheme works for any $\lambda(\theta) \ge \max_s A_s$, but now chooses an upperbound $\lambda(\theta, \vartheta)$ that depends on both old and new parameter, and is symmetric in the new and old parameters. 
To do this, we reverse the `natural' order of MCMC steps outlined in the algorithm in the review, simulating the parameter $\vartheta$ first to set the uniformization rate, and \emph{then} simulating the Poisson process. 
Now $\lambda(\theta,\vartheta)$ is the same if $\theta$ is the old parameter and $\vartheta$ the new one, or vice versa. It is however still \emph{not} independent of the parameters in general.
Our MH proposal is then to swap the two parameters. 
Our experiments conclusively demonstrate the importance of this approach. Our revised manuscript also includes new plots showing that without this, the dependence of the Poisson process and parameter results in a drop in acceptance probability. Finally, our geometric ergodicity results concentrates on this algorithm, not the obvious naive algorithm.

Our contribution can also be viewed as an auxiliary variable representation for MJPs that includes the old and the new parameters as latent variables before the Poisson processes is generated. 
It might be possible to express our final algorithm as a `partial' NCP algorithm built on this representation (but as we state, the Poisson process is not independent of the parameters). 
%as the CP-NCP is a very broad framework. 
In this light, our main contribution is a novel latent variable representation to facilitate efficient parameter inference (involving a proposed paramter $\vartheta$), as well as a particular Markov transition kernel that exploits the structure of the problem (simulate a Poisson and swap parameters). We believe the rewriting the paper from this viewpoint would require significant additional notation, and would confuse readers unfamiliar with NCP. However, we do point this out in our paper in the section on related work.

%The main algorithm that we propose involves a bit more thought however. We tried to express this as a special instance of an NCP algorithm, but this was not obvious: we believe that even if this were possible, it would be not be obvious without exploiting the particular structure of the problem. We describe this below.

\subsection*{Other comments}
\rev{1}{The last paragraph of Section 5.1 sounds rather vague.} \\ 
\rev{2}{It seems a bit awkward that Section 5 contains only one subsection, Sub- section 5.1, and also a lot of text before this first subsection.}\\ 
\resp We have rewritten and extended this subsection, and have assigned it to its own section.

~\\ 
\rev{4}{the notation S(t) for the MJP seems strange to me} \\
\resp We have replaced this to be either $\{S(t), t \in [0,t_{end})\}$ or $(s_0, S, T)$, where $s_0$ is the initial state, $T$ is the set of jump times, and $S$ is the set of associated states.

~\\
\rev{7}{P. 2, l. 37: describing the work as “elegant” is perhaps not objective enough}
\resp{We have removed this.}

~\\
\rev{3}{make sure that references in the text such as “Figure X”, “Section X”, “Assumption X” and “Algorithm X” are consistently capitalised} \\
\rev{5}{P. 2, l. 35: samples path -> sampling the path} ~\\
\rev{6}{P. 2, l. 35: given path -> given the path} ~\\
\rev{8}{P. 2, l. 54: N -states -> N states}
~\\
\rev{9}{P. 4, l. 50: high-level -> high level}
~\\
\rev{10}{P. 17, l. 17: missing space after “Figure 5”}
~\\
\rev{11}{P. 17, l. 44: use \texttt{dotsc} instead of \texttt{cdots} here}
~\\
\rev{12}{P. 18, l. 54: use \texttt{dotsc} instead of \texttt{cdots} here}
~\\
\rev{13}{Bibliography: there are some typos in the bibliography:}\\ 
\resp{Thank you for pointing these out, we have fixed these}

\end{document}
